- en: Databases in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will leverage databases in our scripts so that we can accomplish
    meaningful tasks when working with large quantities of data. Using a simple example,
    we will demonstrate the capabilities and benefits of using a database backend
    in our Python scripts. We will store file metadata that has been recursively indexed
    from a given root directory into a database and then query it to generate reports.
    Although this may seem like a simple feat, the purpose of this chapter is to showcase
    the ways we can interact with a database in Python by creating an active file
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will delve into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic design and implementation of SQLite3 databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with these databases in Python using built-in and third-party modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to recursively iterate through directories in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding filesystem metadata and the methods for accessing it using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafting CSV and HTML reports for easy review by our end user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter was developed and tested using Python 2.7.15 and Python
    3.7.1. The `file_lister.py` script was developed to work with Python 3.7.1\. The
    `file_lister_peewee.py` script was developed and tested using both Python 2.7.15
    and Python 3.7.1.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Databases provide an efficient means of storing large amounts of data in a
    structured manner. There are many types of databases, commonly broken into two
    categories: **SQL** or **NoSQL**. **SQL** (short for **Structured Query Language**)
    is designed to be a simple language that allows users to manipulate large datasets
    that are stored in a database. This includes common databases, such as MySQL,
    SQLite, and PostgreSQL. NoSQL databases are also useful and generally use JSON
    or XML to store data of varying structures, both of which were discussed as common
    serialized data types in the previous chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Using SQLite3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SQLite3 is the latest version of SQLite and is one of the most common databases
    found in application development. This database, unlike others, is stored as a
    single file and does not require a server instance to be running or installed.
    For this reason, it is widely used due to its portability and is found in many
    applications for mobile devices, desktop applications, and web services. SQLite3 uses
    a slightly modified SQL syntax, though of the many SQL variations that exist,
    it is one of its simpler implementations. Naturally, there are some limitations
    to this lightweight database. These limitations include a restriction of one writer
    being connected to the database at a time, 140 TB of storage, and that it is not
    client-server based. Because our application will not execute multiple write statements
    simultaneously, uses less than 140 TB of storage, and does not require a client-server
    setup for distribution, we will be using SQLite for our example in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before developing our code, let's take a look at the basic SQL statements we
    will be using. This will help us understand how we can interact with databases
    even without Python. In SQL, commands are commonly written in uppercase, although
    they are case-insensitive. For this exercise, we will use uppercase to improve
    legibility. All SQL statements must end in a semicolon to execute, as it denotes
    the end of a statement.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to follow along, install a SQLite management tool, such as
    the command-line tool sqlite3\. This tool can be downloaded from [https://www.sqlite.org/download.html](https://www.sqlite.org/download.html).
    The output shown in this section has been generated with the sqlite3 command-line
    tool, though the statements that have been given will generate the same database
    in most other sqlite3 graphical applications. When in doubt, use the official
    sqlite3 command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will create a table, a fundamental component of any database.
    If we compare a database to an Excel workbook, a table is tantamount to a worksheet.
    Tables contain named columns, as well as rows of data that are mapped to these
    columns. Just like how an Excel workbook may contain multiple worksheets, so too
    can a database contain multiple tables. To create a table, we will use the `CREATE
    TABLE` command, specifying the table name and then wrapping, in parentheses, the
    column names and their data types as a comma-separated list. Finally, we end the
    SQL statement with a semicolon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the `CREATE TABLE` statement, we specify the `id` and `name` columns
    in the `custodians` table. The `id` field is an integer and primary key. This
    designation of `INTEGER PRIMARY KEY` in SQLite3 will create an automatic index
    that sequentially increments for each added row, therefore creating an index of
    unique row identifiers. The `name` column has the data type of `TEXT`, which allows
    any character to be stored as a text string. SQLite supports five data types,
    two of which we''ve already introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '`INTEGER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TEXT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REAL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BLOB`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NULL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `REAL` data type allows floating point numbers (for example, decimals).
    The **BLOB** (short for **Binary Large OBject**) data type preserves any input
    data exactly as is, without casting it as a certain type. The `NULL` data type
    simply stores an empty value.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the table, we can begin to add data to it. As we can see in
    the following code block, we can use the `INSERT INTO` command to insert data
    into the table. The syntax following this command specifies the table name, the
    columns to insert the data into, followed by the `VALUES` command specifying the
    values to be inserted. The columns and data must be wrapped in parentheses, as
    shown in the following code. Using the `null` statement as a value, the auto-incrementing
    feature of SQLite will step in and fill in this value with the next available
    unique integer. Remember that this auto-incrementing is only true because we designated
    it as `INTEGER PRIMARY KEY`. As a general rule, only one column in a table should
    have this designation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve inserted two custodians, `Chell` and `GLaDOS`, and we let SQLite assign
    IDs to each of them. After the data has been inserted, we can select and view
    this information using the `SELECT` command. The basic syntax involves invoking
    the `SELECT` command, followed by the columns to select (or an asterisk `*` to
    designate all columns) and the `FROM` statement, indicating the table name following
    a trailing semicolon. As we can see in the following code, `SELECT` will print
    out a pipe (`|`) separated list of the values stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to showing only the desired columns from our table, we can also
    filter data on one or more conditions. The `WHERE` statement allows us to filter
    results and return only responsive items. For the purpose of the script in this
    chapter, we will stick to a simple `where` statement and only use the equals operator
    to return responsive values. When executed, the `SELECT-WHERE` statement returns
    only the custodian information where the `id` value is `1`. In addition, note
    that the order of the columns reflects the order in which they were specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are more operations and statements available to interact with SQLite3
    databases, although the preceding operations highlight all that we require for
    our scripts. We invite you to explore additional operations in the SQLite3 documentation,
    which can be found at [https://sqlite.org](https://sqlite.org).
  prefs: []
  type: TYPE_NORMAL
- en: Designing our script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first iteration of our script focuses on performing the task at hand with
    a standard module, sqlite3, in a more manual fashion. This entails writing out
    each SQL statement and executing them as if you were working with the database
    itself. Although this is not a very Pythonic manner of handling a database, it
    demonstrates the methods that are used to interact with a database with Python.
    Our second iteration employs two third-party libraries: `peewee` and `jinja2`.'
  prefs: []
  type: TYPE_NORMAL
- en: Peewee is an **object-relational mapper** (**ORM**), which is a term that's
    used to describe a software suite that uses objects to handle database operations.
    In short, this ORM allows the developer to call functions and define classes in
    Python that are interpreted as database commands. This layer of abstraction helps
    to standardize database calls and allows for multiple database backends to be
    easily interchanged. Peewee is a light ORM, as it is a single Python file that
    supports PostgreSQL, MySQL, and SQLite3 database connections. If we needed to
    switch our second script from SQLite3 to PostgreSQL, it would only require that
    we modify a few lines of code; our first script would require more attention to
    handle this same conversion. This being said, our first version does not require
    any dependencies beyond the standard Python installation for SQLite3 support, an
    attractive feature for tools that are designed to be portable and flexible while
    in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Our `file_lister.py` script is a per-custodian metadata collection and reporting
    script. This is important in incident response or the discovery phase of an investigation,
    as it stores information about active files on a system or in a specified directory
    by custodian name. A custodian assignment system allows for multiple machines,
    directory paths, or network shares to be indexed and categorized by a single custodian
    name, regardless of whether the custodian is a user, machine, or device. To implement
    this system, we need to prompt the user for the custodian name, the path of the
    database to use, and the input or output information.
  prefs: []
  type: TYPE_NORMAL
- en: By allowing the examiner to add multiple custodians or paths into the same database,
    they can append to the files that have been found for a single custodian or add
    in as many custodians as they please. This is helpful in collections as the investigator
    can preserve as few or as many paths as they need, as we all know how unexpected
    devices show up once we are in the field. In addition, we can use the same script
    to create file listing reports, regardless of the number of collected files or
    custodians, as long as the custodian has at least one collected file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our design state, we don''t only take into account our script but also the
    database and the relational model we will use. In our case, we are handling two
    separate items: custodians and files. These both make for good tables, as they
    are separate entries that share a common relation. In our scenario, a file has
    a custodian and a custodian may have one or more files; therefore, we will want
    to create a foreign key, relating files to a specific custodian. A foreign key
    is a reference to a primary key in another table. The primary key and the foreign
    key references are usually a unique value or an index that links the data together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents the relational model for our database. We
    have two tables, custodians and files, and a one-to-many relationship between
    them. As defined earlier, this one-to-many relationship will allow us to assign
    many files to a single custodian. Using this relationship, we can ensure that
    our script will properly assign information in a structured and easy-to-manage
    manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9dd4f71-f0f4-4dc7-b9f8-40931f44a576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this relational model, for example, we could have a custodian named JPriest
    who owns files located in a folder named `APB/`. Under this root folder, there
    are 40,000 files spread among 300 subdirectories, and we need to assign each of
    those 40,000 files to JPriest. Because custodian names may be long or complex,
    we want to assign JPriest an identifier, such as the integer 5, and write that
    to each row of the data being stored in the `Files` table. By doing this, we accomplish
    three things:'
  prefs: []
  type: TYPE_NORMAL
- en: We are saving space as we are storing only one character (`5`) instead of seven
    (JPriest) in each of the 40,000 rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are maintaining a link between the JPriest user and their files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we ever needed to rename JPriest, we could change one row in our `Custodians`
    table and therefore update the custodian's name for all associated rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually manipulating databases with Python – file_lister.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a note, this script will be designed to work only in Python 3 and was tested
    with Python 3.7.1\. If you'd like the Python 2 version of the code after working
    through this section, please see [https://github.com/PacktPublishing/Learning-Python-for-Forensics](https://github.com/PacktPublishing/Learning-Python-for-Forensics) for
    the prior iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first iteration of the script, we use several standard libraries to
    complete all of the functionality required for the full operation. Like we did
    in prior scripts, we are implementing `argparse`, `csv`, and `logging` for their
    usual purposes, which include argument handling, writing CSV reports, and logging
    program execution. For logging, we define our log handler, `logger`, on line 43\.
    We have imported the `sqlite3` module to handle all database operations. Unlike
    our next iteration, we will only support SQLite databases through this script.
    The `os` module allows us to recursively step through files in a directory and
    any subdirectories. Finally, the `sys` module allows us to gather logging information
    about the system, and the `datetime` module is used to format timestamps as we
    encounter them on the system. This script does not require any third-party libraries.
    We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Following our import statements, we have our `main()` function, which takes
    the following user inputs: custodian name, target input directory or output file,
    and a path to the database to use. The `main()` function handles some high-level
    operations, such as adding and managing custodians, error handling, and logging.
    It first initializes the database and tables, and then checks whether the custodian
    is in the database. If it is not, that custodian is added to the database. The
    function allows us to handle the two possible run options: to recursively ingest
    the base directory, capturing all subobjects and their metadata, and to read the
    captured information from the database into a report using our writer functions.'
  prefs: []
  type: TYPE_NORMAL
- en: The `init_db()` function, which is called by `main()`, creates the database
    and default tables if they do not exist. The `get_or_add_custodian()` function,
    in a similar manner, checks to see whether a custodian exists. If it does, it
    returns the ID of the custodian, otherwise it creates the custodian table. To
    ensure that the custodian is in the database, the `get_or_add_custodian()` function
    is run again after a new entry is added.
  prefs: []
  type: TYPE_NORMAL
- en: After the database has been created and the custodian table exists, the code
    checks whether the source is an input directory. If so, it calls `ingest_directory()`
    to iterate through the specified directory and scan all subdirectories to collect
    file-related metadata. Captured metadata is stored in the `Files` table of the
    database with a foreign key to the `Custodians` table to tie each custodian to
    their file(s). During the collection of metadata, we call the `format_timestamp()`
    function to cast our collected timestamps into a standard string format.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the source is an output file, the `write_output()` function is called, passing
    the open database cursor, output file path, and custodian name as arguments. The
    script then determines whether the custodian has any responsive results in the
    `Files` table and passes them to the `write_html()` or `write_csv()` function,
    based on the output file path''s extension. If the extension is `.html`, then
    the `write_html()` function is called to create an HTML table using Bootstrap
    CSS, which displays all of the responsive results for the custodian. Otherwise,
    if the extension is `.csv`, then the `write_csv()` function is called to write
    the data to a comma-delimited file. If neither of the extensions is supplied in
    the output file path, then a report is not generated and an error is raised that
    the file type could not be interpreted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the required arguments and the setup for this script. On
    lines 321 through 339, we build out the `argparse` command-line interface with
    the required positional arguments `CUSTODIAN` and `DB_PATH`, and the optional
    arguments `--input`, `--output`, and `-l`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On lines 341 through 347, we check that either the `--input` or `--output`
    argument was supplied by the user. We create a variable, `arg_source`, which is
    a tuple containing the mode of operation and the corresponding path specified
    by the argument. If neither of the mode arguments were supplied, an `ArgumentError`
    is raised and prompts the user for an input or output. This ensures that the user
    provides the required arguments when there are one or more options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On lines 349 through 368, we can see the log configuration that we used in
    previous chapters and check for the `-l` argument, making a path to the log if
    necessary. We also log the script version and the operating system information
    on lines 366 through 368:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the logging squared away, we can create a dictionary, which defines the
    arguments passed into the `main()` function using kwargs. Kwargs, or keyword arguments,
    provide a means of passing arguments as dictionary key-value pairs, where the
    keys match the parameter name and are assigned a corresponding value. To pass
    a dictionary to a function or class as kwargs instead of a value, we must specify
    two asterisks preceding the dictionary name, as seen on line 373\. If we did not
    use kwargs, we would have needed to pass the `args.custodian`, `arg_source`, and
    `args.db_path` arguments as individual positional arguments. There is more advanced
    functionality with kwargs, and examples of this can be found at [https://docs.python.org/3.7/faq/programming.html](https://docs.python.org/3.7/faq/programming.html#how-can-i-pass-optional-or-keyword-parameters-from-one-function-to-another).
    We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following flowchart to understand how each function is linked
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07b28043-e7ad-4163-8fa5-5b1778dbea1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Building the main() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `main()` function is broken up into two phases: database initialization
    and input/output (I/O) processing. Database initialization, inclusive of the docstring,
    occurs on lines 46 through 57, where we define and document the inputs for the
    function. Note that the input variables match the keys of the `args_dict` that
    is passed as a keyword argument to the function. If `args_dict` did not have those
    exact keys defined, we would receive a `TypeError` when calling the function.
    See the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 57, we call the `init_db()` function, passing the path to the database
    and assigning the returned database connection to the `conn` variable. The database
    connection object is handled by the `sqlite3` Python library. We use this object
    to communicate with the database by translating all calls from Python into SQL.
    With the connection object, we can call the cursor object. A cursor is an object
    that is used to send and receive data through the connection; we will define it
    in the functions where we want to interact with the database, since we want to
    keep cursors limited in scope, whereas we can share the database connection between
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After additional logging, we call `get_or_add_custodian()`, passing the connection
    object and custodian name to the function. By passing the open connection, we
    allow the function to interact with the database and define its own cursor. If
    the `custodian_id` is found, we move forward and skip the `while` loop on line
    61; otherwise, we rerun the `get_or_add_custodian()` function until we have added
    the custodian and retrieved a custodian ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have a custodian ID to work with, we need to determine whether the
    source is specified as input or output. If on line 64 the source is an `input`,
    then we run the `ingest_directory()` function, which iterates through the provided
    root directory and gathers associated metadata about any subfiles. Once complete,
    we commit (save) our changes to the database and log its completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If the source is an `output`, the `write_output()` function is called to handle
    writing the output in the specified format. If the source type cannot be determined,
    we raise an `argparse.ArgumentError` error, stating that the arguments cannot
    be interpreted. After running the desired mode, we end the function by closing
    our database connections and log completion of the script, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Initializing the database with the init_db() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `init_db()` function is called on line 87 of the `main()` function to perform
    the basic tasks of creating the database and the initial structure within it.
    First, we need to check whether the database already exists, and if it does, connect
    to it and return the connection object. Regardless of whether a file exists or
    not, we can use the `sqlite3` library''s `connect()` method to open or create
    a file as a database. This connection is used to allow communication between Python
    objects and the database. We also specifically use a cursor object, assigned as
    `cur` on line 94, to keep track of the position we are at among executed statements.
    This cursor is required to interact with our database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If the database does not exist, then we must create a new database, connect
    to it, and initialize the tables. As mentioned in the SQL section of this chapter,
    we must create these tables by using the `CREATE TABLE` statement, followed by
    the column names and their data types. In the `Custodians` table, we need to create
    an auto-incrementing `id` column to provide an identifier for the `name` column,
    which will hold the custodian's names.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we must first build our query in the `sql` variable on line 96\.
    After assignment, we pass this variable to the `cur.execute()` method, which executes
    our SQL statement through the cursor object. At this point, the cursor talks to
    the connection object from before, which then communicates with the database.
    Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 99, we create another SQL query using `PRAGMA`, which allows us to
    modify the database''s configuration. By default, in SQLite3, foreign keys are
    disabled, preventing us from referencing data from one table in another. Using
    the `PRAGMA` statement, we can enable this feature for our database by setting
    `foreign_keys` to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We repeat the table creation process for the `Files` table, adding many more
    fields to account for the file metadata. On lines 100 through 105, we write out
    the list of field names and their associated data types. We are able to wrap this
    string across multiple lines by using triple quotes and have Python interpret
    it as a single string value. As we've already seen, we need columns to store an
    ID (in a similar fashion to the `Custodians` table), the filename, file path,
    extension, size, modified time, created time, accessed time, mode, and inode number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mode` attribute specifies the permissions of the file and is based on
    the UNIX permissions standard, whereas the `inode` attribute is the unique number
    that identifies filesystem objects in UNIX-based systems. Both of these elements
    are further described in the *Understanding the ingest_directory() function* section,
    where they are extracted from the files. After creating the two tables and defining
    their structures, we execute the final SQL statement on line 106 and return the
    connection object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Checking for custodians with the get_or_add_custodian() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, the database is initialized and ready for further interaction.
    The `get_or_add_custodian()` function is called to check for the existence of
    the custodian and to pass along the ID if it is found. If the custodian does not
    exist, the function will add the custodian to the `Custodians` table. On line
    120, we call the `get_custodian()` function to check and see whether the custodian
    exists. On line 122, we use a conditional to check whether `id` is not empty,
    and if so, assign the ID of the custodian to the `cust_id` variable. The SQLite
    library returns tuples for backward compatibility, the first element of which
    will be our ID of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If the custodian is not found, we insert it into the table for future use. In
    lines 125-126, we craft a SQL statement to insert the custodian into the `Custodians`
    table. Note the `null` string in the `VALUES` section; this is interpreted by
    SQLite as a `NoneType` object. SQLite converts `NoneType` objects in our primary
    key field to an auto-incrementing integer. Following the `null` value is our custodian
    string. SQLite requires that string values be wrapped in quotes, similar to Python.
  prefs: []
  type: TYPE_NORMAL
- en: We must use double quotes to wrap our query that contains single quotes. This
    prevents any issues with a string breaking due to an error with the quotes. If
    you see a syntax error in this section of the code, be sure to check the quotes
    used on lines 125-126.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we execute this statement and return the empty `cust_id` variable
    so that the `main()` function will have to check for the custodian in the database
    again and rerun this function. The next pass should detect our inserted value
    and allow the `main()` function to proceed. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Although we could call the `get_custodian()` function here (or grab the ID after
    the insert) for validation purposes, we have the `main()` function check for the
    custodian again. Feel free to implement one of these alternative solutions and
    see in what ways it impacts the performance and stability of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving custodians with the get_custodian() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_custodian()` function is called to retrieve the custodian ID from
    the SQLite database. Using a simple `SELECT` statement, we select the `id` column
    from the `Custodian` table, where we match the name provided by the user to the
    `name` column. We use the string `format()` method to insert the custodian name
    into the SQL statement. Note that we still have to wrap the inserted string in
    single quotes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing this statement, we use the `fetchone()` method on line 144
    to return a single result from the statement. This is the first time our script
    requests data out of the database. To acquire data, we use any of the `fetchone()`,
    `fetchmany()`, or `fetchall()` functions to gather data from the executed statement.
    These three methods are only available to the cursor object. The `fetchone()`
    method is the better option here as we anticipate a single custodian to be returned
    by this statement. This custodian ID is captured and returned in the `data` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the ingest_directory() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ingest_directory()` function handles the input mode for our script and
    recursively captures the metadata of files from a user-supplied root directory.
    On line 158, we set up our database cursor before a `count` variable, which will
    keep count of the number of files stored in the `Files` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important part of this function is the `for` loop on line 160\. This
    loop uses the `os.walk()` method to break apart a provided directory path into
    an iterative array that we can step through. There are three components of the
    `os.walk()` method. They are generally named `root`, `folders`, and `files`. The
    `root` value is a string that represents the path of the base directory we are
    currently walking during the specific loop iteration. As we traverse through subfolders,
    they will be appended to the root value. The `folders` and `files` variables provide
    lists of folder and filenames within the current root, respectively. Although
    these variables may be renamed as you see fit, this is a good naming convention
    to prevent overwriting Python statements, such as `file` or `dir`, which are already
    used in Python. In this instance, though, we do not need the `folders` list from
    `os.walk()`, so we will name it as a single underscore (`_`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is a common practice for assigning a value to a variable that is unused
    in the code. For this reason, only use a single underscore to represent unused
    data. Where possible, try to redesign your code to not return unwanted values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the loop, we begin iterating over the `files` list to access information
    about each file. On line 162, we create a file-specific dictionary, `meta_data`,
    to store the collected information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: On line 163, we use a try-except clause to catch any exceptions. We know we
    said not to do that, but hear us out first. This catch-all is in place so that
    any error within a discovered file does not cause the script to crash and stop.
    Instead, the filename and error will be written to the log before skipping that
    file and continuing execution. This can help an examiner quickly locate and troubleshoot
    specific files. This is important as some errors may occur on Windows systems
    due to filesystem flags and naming conventions that cause errors in Python. Different
    errors will then occur on macOS and Linux/UNIX systems, making it hard to predict
    all of the instances where the script will crash. This is an excellent example
    of why logging is important, as we can review errors that have been generated
    by our script.
  prefs: []
  type: TYPE_NORMAL
- en: Within the try-except clause, we store the different properties of the file's
    metadata to keys. To begin, we record the filename and full path on lines 163
    and 164\. Note how the dictionary keys share the name with the columns they belong
    to in the `Files` table. This format will make our lives easier later in the script.
    The file path is stored using the `os.path.join()` method, which combines separate
    paths into a single one using the operating system's specific path separator.
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 167, we gather the file extension by using the `os.path.splitext()`
    method to split the extension after the last `.` in the filename. Since this function
    on line 167 creates a list, we select the last element to ensure that we store
    the extension. In some situations, the file may not have an extension (for example,
    a `.DS_Store` file), in which case the last value in the returned list is an empty
    string. Be aware that this script does not check file signatures to confirm that
    the file type matches the extension; the process of checking file signatures can
    be automated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the os.stat() method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On line 170, we use `os.stat()` to collect our metadata for the file. This method
    reaches out to the system's `stat` library to gather information about the supplied
    file. By default, this method returns an object with all of the available data
    gathered about each file. Because this information varies between platforms, we
    have selected only the most cross-platform properties for our script, as defined
    in the `os` library documentation; more information can be found at [https://docs.python.org/3/library/os.html#os.stat_result](https://docs.python.org/3/library/os.html#os.stat_result).
    This list includes creation time, modified time, accessed time, file mode, file
    size, inode number, and mode. SQLite will accept the data types in string format,
    though we will store them in the script with the correct data types in case we
    need to modify them or use special characteristics of the specific types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file mode is best displayed as an octal integer, so we must use the Python
    `oct()` function to convert it into a readable state, as shown on line 171:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The file mode is a three-digit integer representing the read, write, and execute
    permissions of a file object. The permissions are defined in the following table
    and use the numbers 0-7 to determine the permissions that are assigned. Each digit
    represents permissions for the file''s owner, the group the file is assigned to,
    and all other users. The number 777, for example, allows full permissions to anyone,
    and 600 means that only the owner can read and write to the file. Beyond each
    individual digit, octal representation allows us to assign additional permissions
    for a file by adding digits. For example, the value 763 grants the owner full
    permissions (700), read and write permissions to the group (040 + 020), and write
    and execute permissions to everyone else (002 + 001). You will probably never
    see 763 as a permission set, though it makes for a fun example here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Permission** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 700 | Full file owner permissions |'
  prefs: []
  type: TYPE_TB
- en: '| 400 | An owner has read permission |'
  prefs: []
  type: TYPE_TB
- en: '| 200 | An owner has write permission |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | An owner has execute permission |'
  prefs: []
  type: TYPE_TB
- en: '| 070 | Full group permissions |'
  prefs: []
  type: TYPE_TB
- en: '| 040 | A group has read permission |'
  prefs: []
  type: TYPE_TB
- en: '| 020 | A group has write permission |'
  prefs: []
  type: TYPE_TB
- en: '| 010 | A group has execute permission |'
  prefs: []
  type: TYPE_TB
- en: '| 007 | Full permissions for others (not in the group or the owner) |'
  prefs: []
  type: TYPE_TB
- en: '| 004 | Others have read permission |'
  prefs: []
  type: TYPE_TB
- en: '| 002 | Others have write permission |'
  prefs: []
  type: TYPE_TB
- en: '| 001 | Others have execute permission |'
  prefs: []
  type: TYPE_TB
- en: 'The following table shows additional file type information, which is provided
    by Python''s `os.stat()` method. The three-hashes in the table indicate where
    the file permissions we just discussed are located within the number. The first
    two rows of the following table are self-explanatory, and symbolic links represent
    references to other locations in a filesystem. For example, in the following table,
    the value 100777 represents a regular file, with full permissions for the owner,
    groups, and anyone else. Although it may take time to get accustomed to this,
    this system is very useful for identifying the permissions of files and who has
    access to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 040### | Directory |'
  prefs: []
  type: TYPE_TB
- en: '| 100### | Regular file |'
  prefs: []
  type: TYPE_TB
- en: '| 120### | Symbolic link |'
  prefs: []
  type: TYPE_TB
- en: 'The `inode` value, a unique identifier of filesystem objects, is the next value
    we will capture on line 172\. Although this is a feature that''s only found in
    Linux/UNIX/macOS-based systems, Python converts the record number for NTFS into
    the same object for uniformity. On line 173, we assign the file size, which is
    represented by the number of allocated bytes as an integer. On lines 174 through
    179, we assign the accessed, modified, and created timestamps to the dictionary,
    in that order. Each timestamp is converted from a float into a string using our
    `format_timestamps()` function. We have now collected the necessary data to complete
    a row in our `Files` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The exception mentioned earlier in this section is defined on line 180 and
    logs any errors that are encountered while collecting metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, outside of our try-except clause, we add the `custodian_id` to our
    `meta_data` dictionary so that we can store it alongside our record. We can now
    construct our SQL statement for inserting the new file metadata record. As we
    saw previously, we will construct an insert statement on line 186 and add placeholders
    for the column and value names. Using the `.format()` method, we will insert our
    `meta_data` key and value data. On line 187, we join the `meta_data` keys into
    a string where each key is separated by double quotes and a comma. On line 188,
    we join a comma-separated list of commas, inserting one question mark per value
    as a placeholder for our `execute()` call. An example of the generated string
    in the `sql` variable is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to then provide a list of our values, as seen within the try
    block on lines 189-190, to the SQLite3 Python library to craft the correct insert
    statement for the database. We need to convert our dictionary values into a tuple
    for support with SQLite3, as shown in the call on line 190:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can close our except clause and provide error handling and logging
    for SQLite3 library errors on lines 191 through 197\. After our error handling,
    we increment our file processing count by 1 and move to the next file, which can
    be found in either of our two for loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our innermost for loop completes, we use the `commit()` method to save
    the new records in our database. We also run the `commit()` method again once
    our outer for loop finishes, before logging that the directory ingestion is complete
    and providing the user with a count of files handled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Developing the format_timestamp() helper function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This comparatively small function interprets integer timestamps as human-readable
    strings. Because the Python `os.stat()` module returns the time as a count of
    seconds since the epoch, 1/1/1970, we need to use the `datetime` library to perform
    this transformation. Using the `datetime.datetime.fromtimestamp()` function, we
    can parse the float to a `datetime` object, which we name `ts_datetime` on line
    211\. With the date as a `datetime` object, we can now use the `strftime()` method
    to format the date using our desired format, `YYYY-MM-DD HH:MM:SS`, on line 212\.
    With the string ready to be inserted into the database, we return the value to
    the calling function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Short utility functions like this are useful to incorporate into larger scripts.
    One advantage is that if we wanted to update our date format, we only have to
    change it in one location, versus finding every use of `strftime()`. This smaller
    function also increases the readability of our code. The `ingest_directory()`
    function is already pretty sizable, and adding this logic three times over could
    become confusing to the next person to review the code. These functions are useful
    in string formatting or common conversions, though as you are designing your own
    script, consider what utility functions you can create to make your life easier.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the write_output() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the output destination is specified by the user, the `write_output()` function
    is called. Once invoked, we select the custodian ID from the database using the
    `get_custodian()` function, which is called on line 225\. If found, we need to
    build a new query to determine the number of files associated with the custodian
    using the `COUNT()` SQL function. If the custodian is not found, an error is logged
    to alert the user that the custodian was unresponsive, as we can see on lines
    234 through 237:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If the custodian is found and the number of stored files is greater than zero,
    we check what type of report to generate. The conditional statements starting
    on line 239 check the size of `count` and the extension of the source. If `count`
    is not greater than zero or does not contain a value, then an error is logged
    on line 240\. Otherwise, we check for the CSV file extension on line 241 and the HTML
    file extension on line 243, calling the respective function if we find a match.
    If the source does not end in either of those file extensions, then an error is
    logged, stating that the file type could not be determined. Finally, if the code
    reaches the else statement on line 247, we log the fact that an unknown error
    occurred. We can see all of this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Designing the write_csv() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the file extension is CSV, we can start iterating through the entries stored
    in the Files table. The SQL statement on line 261 uses the `WHERE` statement to
    identify only files related to the specific custodian. The `cur.description` value
    that''s returned is a tuple of tuples, with eight elements in each of the nested
    tuples, representing our column names. The first value in each tuple is the column
    name, whereas the remaining seven are empty strings that are left in place for
    backward compatibility purposes. Using list comprehension on line 265, we iterate
    through these tuples and build the list of column names by selecting only the
    first element from each item in the returned tuples. This one-line statement allows
    us to condense a simple for loop into a single statement that generates the desired
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: A list comprehension is a succinct method for generating a list with a single-line
    for loop. These are generally used to filter the content of a list or provide
    some form of transformation. On line 265, we are using it to perform a structural
    transformation, extracting only the first item from each element of the `cur.description`
    list and storing it as columns. This is because the Python SQLite bindings return
    the column names as a nested tuple where the first element of each subtuple is
    the column's name.
  prefs: []
  type: TYPE_NORMAL
- en: With the column names prepared, we log that the CSV report is being written
    and open the output file in `wb` mode on line 267\. We then initialize a writer
    by calling the `csv.writer()` method on line 268 and passing the file object.
    After this file is opened, we write the column rows by calling on the `csv_writer`
    object to `writerow()`, which writes a single row.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we will loop through the results by iterating over the cursor,
    where it will return a row for each iteration of the loop until exiting when no
    more rows are responsive to the original query. For each row that''s returned,
    we need to call the `writerow()` method again, as shown on line 272\. We then
    flush the new data to the file on line 273 to ensure that the data is written
    to disk. Finally, we log that the report is complete and stored at the user-specified
    location. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Composing the write_html() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the user specifies an HTML report, the `write_html()` function is called
    to read data from the database, generate the HTML tags for our data, and, using
    Bootstrap styling, create a table with our file metadata. Because this is HTML,
    we can customize it to create a professional-looking report that can be converted
    into a PDF or viewed by anyone with a web browser. If additional HTML elements
    prove to be useful in your version of the report, they can easily be added to
    the following strings and customized with logos, highlighting by extension, responsive
    tables, graphs, and much more, which is possible if you use various web styles
    and scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Since this book is focused on the design of Python scripts, we won't be diving
    into detail about HTML, CSS, or other web design languages. Where we use these
    features, we will describe the basics of why they are used and how to implement
    them, though we recommend using related resources (such as [http://www.w3schools.com](http://www.w3schools.com))
    to learn more about those topics if they are of interest to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function begins similarly to `write_csv()`: we select the files that belong
    to the custodian in a SQL statement on line 287\. Once executed, we again gather
    our `cols` using list comprehension on line 291\. With our column names, we define
    the `table_header` HTML string using the `join()` function on our list and separating
    each value with `<th></th>` tags on line 292\. For all except the first and last
    element, this will enclose each element in a `<th>{{ element }}</th>` tag. Now,
    we need to close the first and last element tags to ensure that they form the
    proper table header. For the beginning of the string, we append the `<tr><th>`
    tags to define the table row `<tr>` for the entire row, and the table header `<th>`
    for the first entry. Likewise, we close the table header and table row tags at
    the end of the string on line 293, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: On line 297, we open our HTML file in `w` mode as the `html_file` variable.
    With the file open, we begin to build our HTML code, starting with the `<html><body>`
    tags that are used to initialize HTML documents on line 298\. Next, we connect
    to the custom style sheet that's hosted online to provide the Bootstrap styles
    for our table. We do this by using the `<link>` tag, with the type and the source
    of the style sheet, which is located at [https://www.bootstrapcdn.com/](https://www.bootstrapcdn.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's define the header of our HTML report so that we can ensure it contains
    the custodian ID and name. We will do this by using the `<h1></h1>` or heading
    1 tags. For our table, we use the table tags on line 302 and the Bootstrap styles
    (`table`, `table-hover`, and `table-striped`) we would like to implement.
  prefs: []
  type: TYPE_NORMAL
- en: For additional information on Bootstrap, visit [http://getbootstrap.com](http://getbootstrap.com).
    While this script uses Bootstrap CSS version 3.3.5, explore the more recent updates
    to Bootstrap and see if you can implement the newer features in your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this header information in the HTML string, we can write it to the file,
    first writing the HTML header and style sheet information on line 304, followed
    by the column names for our table on line 305, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s iterate over the records in the database and write them to the
    table as individual rows. We begin by joining each element in the table data tags
    (`<td></td>`) that specify the table cell content. We use list comprehension before
    joining the data on line 308 and converting it to the string value that the `join()`
    method requires:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 310, we add a new line character (`\n`) followed by a `<tr>` table
    row tag and the initial `<td>` tag to open the table data for the first element.
    The newline character reduces the loading time in some HTML viewers, as it breaks
    the data into multiple lines. We also have to close the last table data tag and
    the entire table row, as seen at the end of line 310\. The row data is written
    to the file on line 311\. Finally, within the loop for the table rows, we `.flush()`
    the content to the file. With the table data built, we can close the table, body,
    and the HTML tags on line 313\. Once outside of the `for` loop, we log the report''s
    status and location on line 315:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Running the script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this iteration, we have highlighted the process that's required for reading
    all of the file metadata of a directory recursively, storing it into a database,
    extracting it out of the database, and generating reports from the data. This
    iteration uses basic libraries to handle the necessary SQL and HTML operations
    in a fairly manual fashion. The next iteration focuses on using Python objects
    to perform this same functionality. Both iterations are final versions of the
    scripts and are fully functional. The separate iterations demonstrate different
    methods to accomplish the same task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run our script, we need to first supply it with the name of the custodian,
    the location of the database to create or read from, and the desired mode. In
    the first example, we specify the input mode and pass the root directory to index.
    In the second example, we create a CSV report with the output mode and supply
    an appropriate file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/140d7386-f8f6-47a6-bbc6-8a426c4b6f17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the preceding script can be viewed in the following screenshot.
    Here, we have simply created a generic CSV report containing the captured metadata
    of the indexed files for this chapter''s custodian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2065652c-8091-4818-aada-4df3ad1c2940.png)'
  prefs: []
  type: TYPE_IMG
- en: Automating databases further – file_lister_peewee.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this iteration, we will use third-party Python modules to automate our SQL
    and HTML setup further. This will introduce extra overhead; however, our script
    will be simpler to implement and more streamlined, which will allow us to easily
    develop further functionality. Developing with an eye toward the future helps
    prevent us from rewriting the entire script for every minor feature request.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have imported the majority of the standard libraries required in the prior
    version and added the third-party `unicodecsv` module (version 0.14.1). This module
    wraps around the built-in `csv` module and automatically provides Unicode support
    for the CSV output. To keep things familiar, we can even name it `csv` by using
    the `import...as...` statement on line 8. As mentioned previously in this chapter, `peewee` (version
    2.8.0) and `jinja2` (version 2.8) are the two libraries that can handle our SQLite
    and HTML operations. As these last three imports are third-party libraries, they
    will need to be installed on the user''s machine for our code to run properly
    and can be done so with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the import statements and license, we define our common script metadata
    and logging handler. On line 46, we add the `database_proxy` object, which is
    used to create the Peewee base model for the `Custodian` and `Files` class tables.
    We also add the `get_template()` function, which builds a template HTML table
    using `jinja2`. The other functions largely resemble their counterparts in the
    previous iteration, with minor adjustments here and there. However, we have removed
    the `get_custodian()` function as Peewee has that functionality builtin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The code block under the `if __name__ == ''__main__''` conditional that defines
    command-line arguments and sets up logging is identical to the prior iteration.
    We will not repeat these implementation details here as we can simply copy and
    paste the section from the previous iteration, saving a few trees. While that
    section has remained unchanged, the overall flow of our script has seen minor
    modifications, as shown in the following flow diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af060217-ae83-40fb-85b9-57a1bf674405.png)'
  prefs: []
  type: TYPE_IMG
- en: Peewee setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Peewee, the object relational manager library that was described at the beginning
    of this chapter, is excellent at database management in Python. It uses Python
    classes to define settings for the database, including table configurations, the location
    of the database, and how to handle different Python data types. On line 46, we
    must first create an anonymous database connection using the Peewee `Proxy()`
    class, which allows us to redirect the information into the previously specified
    format. This variable must be declared before any Peewee operations, as per its
    documentation ([http://docs.peewee-orm.com/en/3.6.0/](http://docs.peewee-orm.com/en/3.6.0/)).
  prefs: []
  type: TYPE_NORMAL
- en: Following the initialization of the proxy, we define our first Python class
    used in this book, thus creating a `BaseModel` class that defines the database
    to use. As part of the Peewee specification, we must link the `database_proxy`
    to the `database` variable within the `Meta` class of our `BaseModel` object.
  prefs: []
  type: TYPE_NORMAL
- en: While this required configuration may not make the most sense at the moment,
    continue through the rest of this chapter and revisit this section after completing
    and running the script, as the purpose of these modules will become clearer. Additionally,
    the aforementioned documentation does an excellent job at demonstrating the features
    and the usage of Peewee.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must include the base model, as defined on lines 48 through 50, as the minimum
    setup for Peewee to create the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create the `Custodians` table that's defined on line 60\. This table
    inherits the `BaseModel` properties and therefore has the `BaseModel` class within
    its parentheses. This is usually used to define arguments that are needed for
    a function, but with classes, it can also allow us to assign a parent class to
    inherit data from. In this script, the `BaseModel` class is the child of `peewee.Model`
    and the parent to the `Custodians` and (soon to be discussed) `Files` tables.
    Keep in mind that Peewee describes tables as class models and that the library
    will be creating a table named `Custodians` for us; more on this in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'After initialization, we add a text field, `name`, to the `Custodians` table
    on line 61\. The `unique=True` keyword creates an auto-incrementing index column
    in addition to our `name` column. This table configuration will be used later
    to create the table, and then insert data into it and retrieve information out
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The `Files` table has many more fields and several new data types. As we already
    know, SQLite only manages the text, integers, none, and BLOB data types, and so
    a few of these types may look out of place. Using the `DateTimeField` as an example,
    Peewee can take any Python `date` or `datetime` object. Peewee will automatically
    store it as a text value in the database and can even preserve its original time
    zone. When the data is called out of the table, Peewee attempts to convert this
    value back into a `datetime` object or into a formatted string. Although the date
    is still stored as a text value in the database, Peewee transforms the data in
    transit to provide better support and functionality in Python. Although we could
    replicate this functionality manually, like we did in our prior script, this is
    one of the many useful features that are bundled into Peewee.
  prefs: []
  type: TYPE_NORMAL
- en: 'On lines 56 through 66, we create typed columns, which reflect primary and
    foreign keys, text, timestamps, and integers. The `PrimaryKeyField` specifies
    unique and primary key attributes and is assigned to the `id` column. The `ForeignKeyField`
    has the `Custodians` class as the argument, as Peewee uses this to relate it back
    to the index in the `Custodians` class we defined. Following the two special key
    fields are a series of fields that we described earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This completes the entire setup for the database we created previously using
    a SQL query in the first script. Although it is lengthier in comparison, it does
    prevent us from having to write our own SQL queries and, when working with larger
    databases, it is even more essential. For example, a larger script with many modules
    would greatly benefit from using Peewee to define and handle database connections.
    Not only would it provide uniformity across the modules, it also allows cross-compatibility
    with different database backends. Later in this chapter, we will showcase how
    to change the database type between PostgreSQL, MySQL, and SQLite. Although the
    Peewee setup is verbose, it adds many features and saves us from having to develop
    our own functions to handle database transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Jinja2 setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's discuss the configuration of the other new module. Jinja2 allows
    us to create powerful text templates using a Pythonic syntax for text expansion
    and logic evaluation. Templates also allow us to develop a reusable block of text
    versus needing to build our table rows and columns line by line within our Python
    script's `for` loops. Although the prior script takes a simplistic approach by
    forming an HTML file from strings, this template is more robust, dynamic, and
    most importantly, more sustainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function defines one variable, `html_string`, which holds our Jinja2 template.
    This string captures all of the HTML tags and data to be processed by Jinja2\.
    Although we place this information in a single variable, we could also place the
    text in a file to avoid the extra line count in our code. On lines 76 and 77,
    we can see identical information to the previous iteration''s `write_html()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'On lines 78 through 80, we open the `<body>` and `<h1>` header tags, followed
    by a string containing two instances of a Python object wrapped in spaced double
    curly braces (`{{ ... }}`). Jinja2 looks for a provided dictionary key or object
    name that matches the name of the string inside of the spaced braces. In the case
    of lines 79 and 80, the `custodian` variable is an object with `id` and `name`
    attributes. Using the same syntax as in Python, we can call the object''s attribute
    and insert them into the HTML when the template is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `<table>` tag, on line 81, specifies the Bootstrap CSS classes we use to
    style our table. On line 82, we open the table row `<tr>` tag, followed by a newline
    `\n` character and a new template operator. The curly braces surrounding percentage
    symbols (`{% ... %}`) indicate to Jinja2 that the template contains an operation,
    such as a loop, that it needs to evaluate. In our case, on line 83 we start a
    for loop, similar in syntax to Python's for loop, though missing the closing colon.
    Skipping ahead to line 85, we use the same syntax to surround the `endfor` statement,
    notifying Jinja2 that the loop is complete. We must do this because the HTML is
    not tab or space sensitive and cannot automatically determine the boundary of
    a loop like Python's indented code.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good practice to include spaces between the Jinja2 template syntax and
    the value we would like Jinja2 to insert into the configured placeholders. For
    example, `{{ Document_Title }}` reads a lot easier than `{{Document_Title}}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 84, we then wrap the newly defined header variable in the table header
    `<th>` tags. After the loop completes, we close the table row `<tr>` tag on line
    86\. Through this loop, we have generated a table row, `<tr>`, containing a list
    of the table headers, `<th>`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we open a new loop to iterate over each reported column, creating a new
    table row `<tr>` and wrapping each element in a table data `<td>` tag. Because
    each column of the database is an attribute of the Peewee-returned row object,
    we can specify the column name using the following format: `entry.column_name`.
    Through this simple for loop, we build a table in an easy-to-read and extensible
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `{% endfor %}` statement, we can complete this HTML template by closing
    the open HTML tags and closing the multiline string with three double quotes. With
    the `html_string` built, we call the Jinja2 templating engine to interpret the
    built string. To do so, we call and return the output of the `jinja2.Template()` function
    on line 103\. This allows us to use this template whenever we need to generate
    an HTML report. We could have also supplied Jinja2 with an HTML file using the
    same markup as the template to load. This is especially helpful when building
    more complex or multi-page HTML content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Updating the main() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is almost identical to the `main()` function we saw in the previous
    iteration, albeit with a few exceptions. To begin, on line 117 we do not need
    to catch a returned value from `init_db()` as `peewee` handles that for us after
    initialization. We have also removed the `while` loop when calling `get_or_add_custodian`,
    as the logic of the function has been supplemented by Peewee, rendering the sanity
    check unnecessary. We assign the returned custodian table to a variable named
    `custodian_model` since Peewee refers to each table as a model.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the `Custodians` and `Files` classes are models in Peewee that
    represent the `Custodians` and `Files` tables in SQLite. In Peewee terms, a set
    of data returned from one model is referred to as a model instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data returned on line 120 is identical in nature to what was returned by
    the `SELECT` statements in the previous instance of the script, though it is a
    model instance that''s handled by Peewee:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The third modification involves modifying how we handle the different modes
    for our script. Now, we only need to provide the `target` and the `custodian_model` variables
    since we can access the database via the `peewee` model classes that we have already
    built. This behavior will be illustrated within each function to demonstrate how
    to insert and access data in the tables. The remainder of the function remains
    the same from our prior iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Adjusting the init_db() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `init_db()` function is where we define the database type (for example,
    PostgreSQL, MySQL, or SQLite). Although we are using SQLite in this example, we
    could use another database type to call a separate `peewee` function on line 144,
    such as `PostgresqlDatabase()` or `MySQLDatabase()`. On line 144, we must pass
    the path to the file we want Peewee to write the database to. If we prefer to
    only have the database temporarily, we could pass the special string `:memory:` to
    have Peewee host the SQLite database in memory. There are two downsides to the
    memory option: one is that the database is not persistent after the script exits,
    and the second is the database''s contents must fit in memory, which may not be
    possible on older machines or with large databases. With our use case, we must
    write the database to disk as we may wish to rerun the script against the same
    database to create additional preservations or reports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: After creating our database object, we have to initialize the `database_proxy`
    we created on line 46 and update it to reference the newly created SQLite database.
    This proxy connection tells Peewee how to route the data from the models into
    our SQLite instance.
  prefs: []
  type: TYPE_NORMAL
- en: We had to create this proxy earlier to allow us to specify the model data before
    we initiate the database connection. The use of this proxy also allowed us to
    ask the user where they'd like to store the database, and through a proxy, we
    can create a placeholder that we can later assign to the SQLite (or other) database
    handler.
  prefs: []
  type: TYPE_NORMAL
- en: More information about proxy usage is available in the Peewee documentation
    at [http://docs.peewee-orm.com/en/3.6.0/peewee/database.html?highlight=proxy#dynamically-defining-a-database](http://docs.peewee-orm.com/en/3.6.0/peewee/database.html?highlight=proxy#dynamically-defining-a-database).
  prefs: []
  type: TYPE_NORMAL
- en: Once connected to the proxy, we can create the necessary tables, thus calling
    the `create_tables()` method on our Peewee database object. As you can see, we
    had to create a list of the models first so that when we called `create_tables()`,
    we could reference the tables (and their schemas) to create.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `safe=True` argument is required here as we want to ignore the table if
    it exists in the database so that we do not overwrite or lose data. If we were
    to expand the functionality of the tool or needed another table, we would need
    to remember to add it to the list on line 146 so that the table would be created.
    As mentioned in the `main()` function, we do not need to return any connection
    or cursor object here, as the data flows through the `peewee` model classes we
    defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Modifying the get_or_add_custodian() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function is much simpler than the prior iteration. All we must do is call
    the `get_or_create()` method on our `Custodians` model and pass the field identifier,
    `name`, and the value it should respond to, `custodian`. With this call, we will
    have an instance from the model and a Boolean value of whether the row was created
    or not. Using this `created` Boolean value, we can add a logging statement to
    alert the user that a custodian was either added to the database or that an existing
    custodian was retrieved. On line 164, we return the model instance to the calling
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Improving the ingest_directory() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While one of the more complex functions in this script, it is almost identical
    to the prior iteration, as the method to gather this information has not varied.
    The new additions here include the initialization on line 177 of a list we will
    use to collect the dictionaries of file metadata and the assignment of the passed
    `custodian_model` instance instead of an integer value for the custodian. We also
    generate the `ddate` value, set to a default timestamp, to insert into `peewee`
    in the case that the script is unable to retrieve a date value and needs to store
    a partial record. The default timestamp values will be set to the minimum value
    for Python's `datetime` library to ensure that date encoding and decoding are
    still functional.
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 207, we append the `meta_data` dictionary to the `file_data` list.
    What''s missing, however, is the code to build a complex SQL insert statement
    and a list of column names and their values. Instead, we iterate over the `file_data`
    list and write the data in a more efficient manner, as described in a moment;
    for now, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: On line 209, we start to insert file metadata into the database. Because we
    may have several thousands of lines of data in our list, we need to batch the
    inserts to the database to prevent any resource exhaustion issues. The loop on
    209 uses the `range` function, starting at `0` and continuing through the length
    of the `file_data` list in increments of `50`. This means that `x` will be an
    increment of `50` until we reach the last element, where it will catch all remaining
    items.
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing this, on line 210, we can insert data into `Files` using the `.insert_many()`
    method. Within the insert, we access entries from `x` through `x+50` to insert
    `50` elements of the list at a time. This method is a change of philosophy from
    the previous iteration where we inserted each line as it was gathered. Here, we
    are inserting, batches of rows at the same time using a simplified statement to
    perform the `INSERT` actions. Finally, on line 211, we need to execute each task
    that we have performed to commit the entries to the database. At the end of the
    function, we log the count of the files that have been inserted, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to adjust the unit of 50 rows to execute as an insert. Tweaking this
    number on your system may produce improved performance, although this sweet spot
    tends to vary depending on the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to look into inserting records once our `file_data` list gets
    to a certain length to help with memory management. For example, if the `file_data`
    list exceeds 500 records, pause the collection, insert the whole list (that is,
    50 records at a time), clear the list, and then resume the metadata collection.
    On larger collections, you should notice a significant reduction in memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at the format_timestamp() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function serves the same purpose as the prior iteration, but returns a
    `datetime` object instead, since Peewee uses this object to write the data to
    the cell for `datetime` values. As we saw in the previous iteration, by using
    the `fromtimestamp()` method, we can convert the integer date value into a `datetime`
    object with ease. We can return the `datetime` object as is because Peewee handles
    the rest of the string formatting and conversion for us. This is shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Converting the write_output() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this function, we can see how to query a `peewee` model instance. On line
    235, we need to select a count of files where the custodian is equal to the custodian''s
    `id`. We first call `select()` on the model to signify we wish to select data,
    followed by the `where()` method to specify the column name, `Files.custodian`,
    and the value, `custodian_model.id`, to evaluate. This is followed by the `count()`
    method to provide an integer of the number of responsive results. Note that the
    `count` variable is an integer, not a tuple, like it was in the previous iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 240, we follow the same logic from the prior iteration to check and
    see whether some lines were responsive, followed by statements to validate the
    output extension to engage the correct writer or provide the user''s accurate
    error information. Note that, this time, we pass along the custodian model instance
    versus an `id` or name on lines 243 and 247, as Peewee performs operations best
    on existing model instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Simplifying the write_csv() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `write_csv()` function uses a new method from the `peewee` library, allowing
    us to retrieve data from the database as dictionaries. Using the familiar `Files.select().where()`
    statement, we append the `dicts()` method to convert the result into Python dictionaries.
    This dictionary format is an excellent input for our reports, as the built-in
    CSV module has a class named `DictWriter`. As its name suggests, this class allows
    us to pass a dictionary of information to be written as a row of data in a CSV
    file. Now that we have our query staged, we can log to the user that we are starting
    to write the CSV report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define our column names for our CSV writer and open the user-specified
    output file using the `with...as...` statement. To initialize the `csv.DictWriter`
    class, we pass the open file object and column headers that correspond to the
    table''s column names (and therefore the dictionary key names). After initialization,
    we call the `writeheader()` method and write the table''s header at the top of
    the spreadsheet. Finally, to write the row content, we open a `for` loop on our
    query object to iterate over the rows and write them to the file with the `.writerow()`
    method. Using the `enumerate` method, we can provide the user with a status update
    every 10,000 rows to let them know that our code is hard at work for larger file
    reports. After writing those status updates (and rows, of course), we add some
    additional log messages for the user and exit the function. Although we are calling
    the `csv` library, remember that it is actually our `unicodecsv` import. This
    means that we will encounter less encoding errors while generating our output
    versus using the standard `csv` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Condensing the write_html() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will need the `get_template()` function we designed earlier to generate
    our HTML report. On line 291, we call this pre-built Jinja2 template object and
    store it in the `template` variable. When referencing the template, we need to
    provide a dictionary with three keys: `table_headers`, `file_listing`, and `custodian`.
    These three keys are required as they are what we chose as placeholders in our
    template. On line 292, we build out the table headers as a list of strings, formatted
    in the order we wish to display them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterwards, we create our `file_data` list for the `file_listing` key on line
    296 by using a similar `select` statement that''s found in the CSV function. This
    list allows us to access the attributes individually within the template, as specified
    earlier. We could have placed this logic within the template file as well, but
    we thought it best to place malleable logic in a function versus a template. Take
    a look at lines 296 and 297:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'With all three of these elements gathered, we create a dictionary with the
    keys to match the data in our template on line 299\. After a log statement, we
    open the source using a `with...as...` statement. To write the template data,
    we call the `render()` method on our `template` object, passing our already built
    dictionary as a `kwarg` on line 307\. The `render()` method evaluates the statements
    and logic found in the template and places the provided data in the correct location
    to form an HTML report. This method also returns the raw HTML as a string, so
    we have encapsulated it in a `write()` call to immediately write the data to the
    file. Once written, we log the path to the source, as well as its successful completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Running our new and improved script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This iteration highlights the use of additional Python third-party libraries
    to handle many of the operations we previously performed in a more manual manner.
    In this instance, we used Peewee and Jinja2 to further automate database management
    and HTML reporting. These two libraries are popular methods for handling this
    type of data and are either bundled into or have ports for other Python suites,
    such as Flask and Django.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, this iteration closely resembles the first to demonstrate the differences
    in the two methods in a clearer manner. One of the goals of this book is to introduce
    as many methods for performing a task in Python as possible. The purpose of this
    chapter is not to create a better iteration, but to showcase different methods
    to accomplish the same tasks and add new skills to our toolbox. This is the last
    chapter where we will be creating multiple iterations of a script; the chapters
    going forward are focused on more expansive singular scripts as we begin to expand
    on our forensic coding capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the way in which we execute our script has not changed. We still need
    to specify a custodian, a path to a database, and the type of mode. You may notice
    that this script is considerably slower than our previous script. Sometimes, when
    using automated solutions, our code can suffer due to additional overhead or the
    inefficient implementation of the module. Here, we've lost some efficiency by
    moving away from a more bare-bones and manual process. However, this script is
    more maintainable and does not require the developer to have in-depth knowledge
    of SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this iteration, we opted to generate our Bootstrap-based HTML report. What
    this report lacks in analytical capacity, it gains in portability and simplicity.
    This is a professional looking page, thanks to Bootstrap, and can be searched
    for specific files of interest or printed out for those that prefer the paper-and-pen
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/704f62c0-0c15-4525-84bf-fa7f0564587b.png)'
  prefs: []
  type: TYPE_IMG
- en: Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As always, we challenge you to add new features to this script and extend it
    using the knowledge and available resources that you have. For this chapter, we
    first challenge you to hash the indexed files using MD5 or SHA1 and store that
    information in the database. You can use the built-in `hashlib` library to handle
    hashing operations; more on this and other hashing techniques in [Chapter 7](91206072-f125-4a9e-83fe-8de632624d0e.xhtml),
    *Fuzzy Hashing*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, consider adding user-specified filters for particular file extensions
    for the collection. These features can be implemented without major renovation
    to the code, though you may find it easiest and more beneficial to your understanding
    to start from scratch and build the script with one or more of these new features
    in mind.
  prefs: []
  type: TYPE_NORMAL
- en: One more extension that we can add to our code is parsing the file's modes into
    separate columns for ease of querying in our database and reports. While the number
    we store is compact and the format is generally understood, splitting out the
    value into separate columns can help non-technical reviewers understand these
    file's properties and allow easier queries against the database in case we want
    to identify all files with a specified permission set. We could either perform
    this operation in our collection module or keep our current database schema and
    interpret the modes while generating our reports.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on the use of databases in script development. We explored
    how to use and manipulate a SQLite database in Python to store and retrieve information
    about file listings. We discussed when and how a database is a correct solution
    to store this information, as it has a fixed data structure and could be a large
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we discussed multiple methods of interacting with databases, a
    manual process to show how databases work at a lower level, and a more Pythonic
    example where a third-party module handles these low-level interactions for us.
    We also explored a new type of report, using HTML to create a different output
    that can be viewed without additional software, and manipulating it to add new
    styles and functionality as we see fit. Overall, this section builds on the underlying
    goal of demonstrating different ways we can use Python and supporting libraries
    to solve forensic challenges. The code for this project can be downloaded from
    GitHub or Packt, as described in the *Preface*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to parse binary data and registry hives
    using third-party libraries. Learning how to parse binary data will become a fundamental
    skill for the forensic developer and will be performed by many of the libraries
    that are featured throughout the remainder of this book.
  prefs: []
  type: TYPE_NORMAL
