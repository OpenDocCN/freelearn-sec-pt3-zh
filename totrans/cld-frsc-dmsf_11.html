<html><head></head><body>
<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2"><h1 class="chapter-number" id="_idParaDest-140"><a id="_idTextAnchor151" class="pcalibre1 pcalibre2 pcalibre"/>8</h1>

<h3 id="_idParaDest-141" class="calibre6"><a id="_idTextAnchor152" class="pcalibre1 pcalibre2 pcalibre"/>The Digital Forensics and Incident Response Process</h3>
<p class="calibre3">So far, we have mostly looked at cloud-native tools for investigators to review logs and perform analysis. In the subsequent chapters, we will be looking at some of the third-party tools that complement cloud-native tools – tools that can aid in collecting and analyzing forensic artifacts, marrying cloud-native and third-party toolsets every investigator should be familiar with before embarking upon a cloud forensic case. Specifically, this chapter will revisit the basics of digital forensics and the incident response process. We will also identify some core concepts and introduce tools we have typically used in cloud <span>forensic cases.</span></p>
<p class="calibre3">In this chapter, we will learn about <span>the following:</span></p>

<ul class="calibre12">
<li class="calibre13">The basics of the incident <span>response process</span></li>
<li class="calibre13">Commonly used tools and techniques for host and <span>memory forensics</span></li>
<li class="calibre13">Options to conduct <span>live forensics</span></li>
<li class="calibre13"><span>Network forensics</span></li>
<li class="calibre13">A refresher on <span>malware analysis</span></li>
<li class="calibre13">Traditional forensics versus <span>cloud forensics</span></li>
</ul>
<p class="calibre3">This chapter assumes you are familiar with most of these topics; this is just a refresher on commonly accepted incident response techniques and tools utilized <span>in cases.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-142" class="calibre5"><a id="_idTextAnchor153" class="pcalibre1 pcalibre2 pcalibre"/>The basics of the incident response process</h1>
<p class="calibre3">The incident response process is an <a id="_idIndexMarker535" class="pcalibre1 pcalibre2 pcalibre"/>overarching process that allows investigators to approach incident response cases in a structured manner. The seven stages within the incident response process enable investigators to understand the actions required to satisfy the conditions in each stage. The following diagram outlines the critical steps of the incident <span>response process:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer133"><img alt="Figure 8.1 – The incident response process" src="../images/00165.jpeg" class="calibre135"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.1 – The incident response process</p>
<p class="calibre3">Here are the <span>key stages:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Preparation</strong>: This is the pre-incident stage, where organizations work with the incident response teams to <a id="_idIndexMarker536" class="pcalibre1 pcalibre2 pcalibre"/>document and plan activities in the event of an incident. Typically, organizations will establish their objectives for handling an incident and how to address critical cybersecurity issues <a id="_idIndexMarker537" class="pcalibre1 pcalibre2 pcalibre"/>arising from incidents in the form of an <strong class="bold">incident </strong><span><strong class="bold">response plan</strong></span><span>.</span>
<p class="calibre3">The incident response plan will typically include the roles and responsibilities of various actions during an incident, key stakeholders who are notified, and so on. The incident response plan will also include organizations’ objectives for handling multiple incidents. Once the plans have been documented, organizations conduct periodic exercises to train, test, and update their <span>plans regularly.</span></p>
</li>
<li class="calibre13"><strong class="bold">Detection</strong>: This phase<a id="_idIndexMarker538" class="pcalibre1 pcalibre2 pcalibre"/> refers explicitly to the process where security teams are armed with tools to identify an incident. Typically, this will be<a id="_idIndexMarker539" class="pcalibre1 pcalibre2 pcalibre"/> the organization’s <strong class="bold">security incident and event monitoring</strong> (<strong class="bold">SIEM</strong>) tool or<a id="_idIndexMarker540" class="pcalibre1 pcalibre2 pcalibre"/> their <strong class="bold">endpoint detection and response</strong> (<strong class="bold">EDR</strong>) tool. These tools are monitored by a dedicated group of teams who provide an early warning about an incident occurring. Organizations set up routine security monitoring and alerting to identify breaches or incidents immediately. This is also the stage where security teams must triage the incident as soon <a id="_idIndexMarker541" class="pcalibre1 pcalibre2 pcalibre"/>as they are notified. <strong class="bold">Triage</strong> is a <a id="_idIndexMarker542" class="pcalibre1 pcalibre2 pcalibre"/>concept where <strong class="bold">Digital Forensic and Incident Response</strong> (<strong class="bold">DFIR</strong>) teams evaluate the breach, the scope of the breach, and the impact as a result of this breach and decide if further investigation is <a id="_idIndexMarker543" class="pcalibre1 pcalibre2 pcalibre"/>necessary. This is the<a id="_idIndexMarker544" class="pcalibre1 pcalibre2 pcalibre"/> step where the security team identifies the breach as <span>an </span><span><strong class="bold">incident</strong></span><span>.</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Security teams need to scope the incident correctly, identify how many systems are impacted, how big or small the spread of the breach is, and so on; otherwise, key elements can be missing <span>from investigations.</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Containment</strong>: Right after the security teams call an incident and regroup various security teams, the <a id="_idIndexMarker545" class="pcalibre1 pcalibre2 pcalibre"/>typical first step is to contain the incident. Imagine an example of a burst pipeline with water gushing out; your first action would be to stop the leak before you can fix it. In similar teams in a cyber incident situation, the first action is, in this case, always about stopping the leak or a breach, then determining what has happened, how this incident occurred, and so on. Security teams can deploy specific security tools to contain the incident or use what is already deployed within an organization’s <span>IT environment.</span>
<p class="calibre3">Examples include disconnecting the host from the network, network quarantining the host using an EDR tool, and shutting certain services down (not allowing access to users). This stage is also where security teams must preserve any evidence before they are eliminated. We learned about evidence preservation in <a href="part0020_split_000.html#_idTextAnchor027" class="pcalibre1 pcalibre2 pcalibre"><span><em class="italic">Chapter 2</em></span></a> and its importance in investigations and potential legal actions. In most situations, organizations that do not have the skills or capabilities to preserve evidence usually rely on third-party professionals who offer <span>DFIR services.</span></p>
</li>
<li class="calibre13"><strong class="bold">Eradication and recovery</strong>: In our view, the eradication and recovery and investigation and analysis phases occur in parallel, provided necessary forensic evidence is preserved. In the<a id="_idIndexMarker546" class="pcalibre1 pcalibre2 pcalibre"/> eradication and recovery stage, incident response teams work with other security teams within the organizations or authorized third parties to remediate the incident. This can include configurational changes, patching, or a complete system rebuild from backup or scratch. Once remediation is completed, these systems are restored to <span>normal operations.</span></li>
<li class="calibre13"><strong class="bold">Investigation and analysis</strong>: Once you have stopped the incident from escalating further, investigation teams take over the case. Investigation teams are responsible for analyzing what happened; this includes a review of logs and forensic artifacts and understanding how the incident occurred, what data was impacted, the extent <a id="_idIndexMarker547" class="pcalibre1 pcalibre2 pcalibre"/>of the damage, and so on. This stage also provides input to the detection, eradication, and recovery stages to detect any associated incidents and apply additional remediations or security hardening that are required for recovery activities. Often, this is the stage where investigation teams identify the incident’s root cause. In incident response<a id="_idIndexMarker548" class="pcalibre1 pcalibre2 pcalibre"/> jargon, we call it <strong class="bold">patient zero</strong>. The investigation team will leverage threat intelligence to identify the threat actor, their motives, and <a id="_idIndexMarker549" class="pcalibre1 pcalibre2 pcalibre"/>potential <strong class="bold">indicators of compromise</strong> (<strong class="bold">IoCs</strong>) that may locate any additional <span>investigative avenues.</span></li>
<li class="calibre13"><strong class="bold">Reporting</strong>: Once the investigation has been wrapped up, key facts have been determined, and the root cause has been identified, the investigation team moves to the reporting<a id="_idIndexMarker550" class="pcalibre1 pcalibre2 pcalibre"/> stage. Ideally, the incident report will include all the aspects of the incidents, artifacts reviewed, a timeline of the incident’s occurrence, actions performed, and so on. Once the report has been developed, it is circulated to various relevant stakeholders. Depending on the nature and severity of the incident, the incident reports may only be shared with a limited group, including, in some cases, a breach coach (external legal counsel) to support and prepare the organizations for any potential litigation risks<a id="_idIndexMarker551" class="pcalibre1 pcalibre2 pcalibre"/> resulting from <span>the incident.</span></li>
<li class="calibre13"><strong class="bold">Post-incident reviews</strong>: Once everything has been wrapped up and the organization has moved back to the business-as-usual phase, organizations and security teams typically conduct a debrief, or <a id="_idIndexMarker552" class="pcalibre1 pcalibre2 pcalibre"/>what is known as a post-incident review. Security teams<a id="_idIndexMarker553" class="pcalibre1 pcalibre2 pcalibre"/> review documentation and investigative notes that were prepared during and after the incident and identify procedural gaps on how to improve in detecting or mitigating this incident from reoccurring. Security teams update their incident response plan using the knowledge they’ve gained from handling <span>this incident.</span></li>
</ul>
<p class="calibre3">Having established the fundamental aspects of the incident response process, let’s jump into the practical tools and techniques that are utilized in digital forensics investigations. These tools play a critical role in uncovering digital evidence, analyzing volatile memory, dissecting filesystems, and piecing together timelines <span>of events.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-143" class="calibre5"><a id="_idTextAnchor154" class="pcalibre1 pcalibre2 pcalibre"/>Tools and techniques for digital forensic investigations</h1>
<p class="calibre3">One of the challenges in any incident investigation is acquiring artifacts quickly and in a forensically sound manner. In some cases, investigators may collect artifacts to investigate the incident further and identify the root cause. Moving swiftly to collect artifacts and evidence is crucial to investigations. This section will explore some valuable host and memory artifacts <span>for investigations.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-144" class="calibre10"><a id="_idTextAnchor155" class="pcalibre1 pcalibre2 pcalibre"/>Prerequisites</h2>
<p class="calibre3">Before investigators can begin <a id="_idIndexMarker554" class="pcalibre1 pcalibre2 pcalibre"/>collecting their logs from the cloud console, we can utilize some of the prerequisites that were explored in previous chapters (enabling logs, audit trails, and so on), along with the ones listed here. These prerequisites will help investigators conduct their incident response activities much <span>more smoothly:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Instance protection</strong>: Some CSPs will allow instance protection once an incident is declared. For example, in AWS, you can configure your instance to prevent it from termination. This will ensure you are not terminating or deleting the instance once it shuts down and that all the artifacts and associated volumes are preserved for investigation. Investigators can even apply tags as a visual marker for security administrators not to change or update <span>their configuration.</span></li>
<li class="calibre13"><strong class="bold">Decommission</strong>: Deregister and decommission the instance from auto-scaling groups <span>where possible.</span></li>
<li class="calibre13"><strong class="bold">Isolate instance</strong>: Wherever possible, and if necessary, investigators should update the host firewall (the CSP’s firewall) to limit traffic to external IPs and ports. Investigators can <a id="_idIndexMarker555" class="pcalibre1 pcalibre2 pcalibre"/>access the instance from the cloud console and not connect from <span>the internet.</span></li>
<li class="calibre13"><strong class="bold">Inventory-associated volumes</strong>: When an incident is declared, and investigators have identified the instance, inventory the volumes attached to the host as snapshots may be required for all the <span>connected volumes.</span></li>
<li class="calibre13"><strong class="bold">Recreate in an isolated lab</strong>: If the situation warrants, based on the investigative leads, then investigators should consider making a copy or snapshot of the instance and then using that to re-launch in a dedicated and isolated forensics lab hosted on the cloud to capture the <span>necessary artifacts.</span></li>
</ul>
</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-145" class="calibre10"><a id="_idTextAnchor156" class="pcalibre1 pcalibre2 pcalibre"/>Cloud host forensics</h2>
<p class="calibre3">Host forensics or digital forensics involves collecting, processing, analyzing, and preserving digital host-based evidence. It requires discovering artifacts, traces, and information not generally<a id="_idIndexMarker556" class="pcalibre1 pcalibre2 pcalibre"/> available to security tools and often hidden from general users. Host forensics plays a crucial role in cybersecurity and law enforcement investigations, providing insights into some of the<a id="_idIndexMarker557" class="pcalibre1 pcalibre2 pcalibre"/> actions taken by attackers or users, a timeline of activities, and so on. It is helpful in cases of intellectual property theft, data breaches, malware infections, and insider threats, and it is commonly used in <span>ransomware cases.</span></p>
<p class="calibre3">Recognizing that this book is intended for DFIR professionals, we will look at various forensic elements that investigators can perform at a high level. We will also spend time identifying key artifact sources that can be useful in the event of investigations. Collecting artifacts from cloud instances is similar; the artifacts and event logs are consistent across the<a id="_idIndexMarker558" class="pcalibre1 pcalibre2 pcalibre"/> deployment model. So, there is no difference between a physical host deployed in a data center versus a host running on <span>the cloud.</span></p>

<h3 class="calibre11">Key artifact sources</h3>
<p class="calibre3">We want to learn about key <a id="_idIndexMarker559" class="pcalibre1 pcalibre2 pcalibre"/>sources of artifacts that can sometimes be referred to for investigation. These sources often provide evidence that pinpoints what activity a user or threat actor (subject of the investigation) could have done in a system. By querying these artifact repositories, you will learn about threat actors and the vast depth of information operating systems generally retain when a user operates a <span>computing system.</span></p>
<p class="calibre3">However, note that with every<a id="_idIndexMarker560" class="pcalibre1 pcalibre2 pcalibre"/> iteration of Windows, Linux, or any other operating system, the information that’s collected by the artifact or the artifact repository itself could change or may no longer be available. It is essential to note the version of the operating system, including major and minor versions, to prepare investigators on what artifacts would be available for a <span>given version.</span></p>
<p class="calibre3">In the next subsection, we’ll look at two prominent and commonly used operating systems that are deployed for servers and IT infrastructural environments. You will also see similar deployments in the cloud (irrespective of the <span>cloud provider).</span></p>

<h4 class="calibre136">Windows operating system</h4>
<p class="calibre3">One of the most popular operating systems is Windows; it collects tons of artifacts that can be very helpful for<a id="_idIndexMarker561" class="pcalibre1 pcalibre2 pcalibre"/> investigations. The following is a list of artifact repositories that Windows typically records. However, as indicated earlier, depending on the operating system’s major and minor versions, some artifacts may be available in a different folder path or no longer available. Note that the following logs are not an exhaustive list, and third-party tools may collect additional records and artifacts that can be crucial <span>for investigation:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Core </strong><span><strong class="bold">Windows logs</strong></span><span>:</span><ul class="calibre17"><li class="calibre13"><strong class="source-inline">C:\Windows\System32\Tasks\*</strong>: Contains XML files associated with Task Scheduler and contains information such as task definitions, task author, trigger criteria, and any task customizations. These are automated processes that trigger under specific conditions or at certain times. Threat actors use Task Scheduler to hide and evade detection while maintaining persistence. You may also see files associated with Task Scheduler created under <strong class="source-inline">C:\Windows\Tasks\*</strong> for some <span>legacy systems.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\Prefetch\*</strong>: Prefetch is a Windows feature that allows the application to <a id="_idIndexMarker562" class="pcalibre1 pcalibre2 pcalibre"/>optimize loading times by preloading data files and libraries that it frequently uses. It provides forensic evidence or proof that an application was executed, including if any code or script was injected within an application (for example, code execution via PowerShell). When an application is executed for the first time, it creates a prefetch file that documents all the files and libraries that can be pre-loaded into memory from the disk before execution. Prefetch includes information such as executable name, execution times, <span>and count.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\winevt\Logs\*</strong>: Windows Event Logs are the most critical log sources every investigator will want to look into. Various categories of logs have dedicated log files that record associated events. Some of the important ones to look for are <span>as follows:</span><ul class="calibre17"><li class="calibre13"><strong class="source-inline">Security.evtx</strong>: Login/logouts, destination hostname and IP address, alternate username, logon session ID, and <span>logon type.</span></li><li class="calibre13"><strong class="source-inline">System.evtx</strong>: Windows system startup/shutdown time, service installs, driver failure/installations, hardware changes, and any <span>system-related activities.</span></li><li class="calibre13"><strong class="source-inline">Windows PowerShell.evtx</strong>: PowerShell script executions and cmdlet invocation, often corroborated with <strong class="source-inline">Microsoft-Windows-WinRM%4Operational.evtx</strong> for information on remote session authentication and session information and <strong class="source-inline">Microsoft-Windows-PowerShell%4Operational.evtx</strong> for script <span>block logs.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-PowerShell%4Operational.evtx</strong>: Records details of PowerShell executions, including script blocks, module loads, and <span>script policies.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-TerminalServices-RDPClient%4Operational.evtx</strong>: <strong class="bold">Remote Desktop Connections</strong> (<strong class="bold">RDP</strong>) activity and destination<a id="_idIndexMarker563" class="pcalibre1 pcalibre2 pcalibre"/> RDP connections including IP address and <a id="_idIndexMarker564" class="pcalibre1 pcalibre2 pcalibre"/>hostname (evidence of lateral movement initiation). It can be corroborated <span>with </span><span><strong class="source-inline">Security.evtx</strong></span><span>.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-TerminalServices-RemoteConnectionManager%4Operational.evtx</strong>: Evidence of the RDP source IP address and username (evidence of where lateral movement occurred). This can be corroborated with <strong class="source-inline">Microsoft-Windows-Terminal Services-LocalSessionManager%4Operational.evtx</strong>, <strong class="source-inline">Microsoft-Windows-RemoteDesktopServices-RdpCoreTS%4Operational.evtx</strong>, <span>and </span><span><strong class="source-inline">Security.evtx</strong></span><span>.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-TaskScheduler%4Operational.evtx</strong>: Information about scheduled task execution, creation, and registration and corroborated with <strong class="source-inline">Security.evtx</strong> for evidence of administrator <span>privilege usage.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-WMI-Activity%4Operational.evtx</strong>: <strong class="bold">Windows Management Instrumentation</strong> (<strong class="bold">WMI</strong>)-related<a id="_idIndexMarker565" class="pcalibre1 pcalibre2 pcalibre"/> activities, including WMI queries, WMI method invocations, and WMI <span>provider operations.</span></li><li class="calibre13"><strong class="source-inline">Microsoft-Windows-WinRM%4Operational.evtx</strong>: <strong class="bold">Windows Remote Management</strong> (<strong class="bold">WinRM</strong>) is used for remote administrations and provides<a id="_idIndexMarker566" class="pcalibre1 pcalibre2 pcalibre"/> information about remote sessions <span>and authentications.</span></li></ul></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\Logfiles\W3SVC1\*</strong>: Contains information about <strong class="bold">Internet Information Services</strong> (<strong class="bold">IIS</strong>) activities, including incoming HTTP <a id="_idIndexMarker567" class="pcalibre1 pcalibre2 pcalibre"/>requests and responses, requested URLs, user agent strings, response codes, error codes, and descriptions. The log files also include <a id="_idIndexMarker568" class="pcalibre1 pcalibre2 pcalibre"/>server-side errors, application crashes, or issues. Investigators can use this information to perform traffic analysis, such as the number of requests and <span>data transferred.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\appcompat\Programs\Amcache.hve</strong>: <strong class="bold">Application Compatibility</strong> (<strong class="bold">AppCompat</strong>) is a cache or database that stores information about application <a id="_idIndexMarker569" class="pcalibre1 pcalibre2 pcalibre"/>compatibility. AppCompat is used to track application executions, including the full file path, last modification of an executable, and so on. In older Windows systems, this cache also records when the application was last executed. <strong class="source-inline">Amcache.hve</strong> records application installations and executions. It includes full application metadata and the SHA1 hash of the <span>executable file.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\config\SAM</strong>: The Windows <strong class="bold">Security Accounts Manager</strong> (<strong class="bold">SAM</strong>) database that is<a id="_idIndexMarker570" class="pcalibre1 pcalibre2 pcalibre"/> responsible for storing local user account information and their unique <strong class="bold">security identifier</strong> (<strong class="bold">SID</strong>), password<a id="_idIndexMarker571" class="pcalibre1 pcalibre2 pcalibre"/> hashes, and account policies, such as password strength, account lockout policy, and so on. It also plays a crucial role in Window’s authentication process. Irrespective of the Windows operating system version, Windows will automatically maintain the relevant <strong class="source-inline">SAM</strong> database and any local accounts created within the host machine. Access to the <strong class="source-inline">SAM</strong> database requires elevated privileges, and threat actors often try to attack the <strong class="source-inline">SAM</strong> database to compromise credentials and use it to perform other forms <span>of attack.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\config\SOFTWARE</strong>: This is one of the critical registry hives that collects information about the system state, installed software, and various other configurational elements of Windows. Investigators can use the <strong class="source-inline">SOFTWARE</strong> registry hive to investigate if unauthorized software was installed, modified, or deleted. Investigators can also identify if any core elements were modified, such as startup programs (for maintaining <a id="_idIndexMarker572" class="pcalibre1 pcalibre2 pcalibre"/>persistence), system configurations (to lower defenses), <span>and more.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\config\SECURITY</strong>: The <strong class="source-inline">SECURITY</strong> registry hive is responsible for recording all the configurations related to the operating system’s security. This includes user accounts, associated account groups, password hashes, permissions in registry entries, and security policies. Investigators can analyze this registry hive to reconstruct threat actors, unauthorized user activity, and security policy violations. Like the <strong class="source-inline">SAM</strong> database, the <strong class="source-inline">SECURITY</strong> hive is also protected by the Windows operating system kernel and requires elevated privileges <span>to modify.</span></li><li class="calibre13"><strong class="source-inline">C:\Windows\System32\config\*.LOG1</strong>: The <strong class="source-inline">LOG1</strong> file is a transaction log file that ensures the integrity of relevant registry hives (including <strong class="source-inline">SOFTWARE</strong>, <strong class="source-inline">SAM</strong>, and <strong class="source-inline">SECURITY</strong>). When changes are made to system configurations, changes are written to the relevant transaction log <strong class="source-inline">LOG1</strong> file, and when the changes are committed to the hive, the transaction log is marked <span>as complete.</span></li></ul></li>
<li class="calibre13"><span><strong class="bold">User profiles</strong></span><span>:</span><ul class="calibre17"><li class="calibre13"><strong class="source-inline">C:\Users\*\AppData\Roaming\Microsoft\Windows\PowerShell\PSReadline\ConsoleHost_history.txt</strong>: Provides command history for PowerShell console executions by <span>a user.</span></li><li class="calibre13"><strong class="source-inline">C:\Users\*\NTUser.DAT</strong>: For every user, an <strong class="source-inline">NTUSser.DAT</strong> file is created automatically by Windows that contains information about user-specific configurations or preferences, user interactions with various applications, actions taken, and most recently <span>accessed resources.</span></li><li class="calibre13"><strong class="source-inline">C:\Users\*\NTUser.DAT.LOG1</strong>: Similar to registry hives, this transaction log file records all the transactions (changes) before committing the changes within the <strong class="source-inline">NTUser.DAT</strong> file. Investigators can use this for further correlation of <span>user activities.</span></li></ul></li>
<li class="calibre13"><strong class="bold">Root drive (C:\, </strong><span><strong class="bold">D;\, …)</strong></span><span>:</span><ul class="calibre17"><li class="calibre13"><strong class="source-inline">%SYSTEMDRIVE%\$Recycle.Bin\**</strong>: <strong class="source-inline">%SYSTEMDRIVE%</strong> refers to the root drive partition nomenclature, which contains the <strong class="source-inline">RecycleBin</strong> folder. This folder, in turn, contains <a id="_idIndexMarker573" class="pcalibre1 pcalibre2 pcalibre"/>all the files and subfolders that are tagged for deletion. Investigators can use this to identify if any files were attempted to be deleted by the user or the <span>threat actor.</span></li><li class="calibre13"><strong class="source-inline">%SYSTEMDRIVE%\$LogFile</strong>: <strong class="source-inline">$LogFile</strong> is a special Windows file associated with the filesystem, specifically the <strong class="bold">New Technology File System</strong> (<strong class="bold">NTFS</strong>). <strong class="source-inline">$LogFile</strong> records <a id="_idIndexMarker574" class="pcalibre1 pcalibre2 pcalibre"/>metadata and directory and file creations, modifications, and deletions. Investigators can parse <strong class="source-inline">$LogFile</strong> to identify system changes over time and obtain information that allows information to be recovered from file storage that’s relevant to <span>the investigation.</span></li><li class="calibre13"><strong class="source-inline">%SYSTEMDRIVE%\$MFT</strong>: This refers to the <strong class="bold">Master File Table</strong> (<strong class="bold">MFT</strong>) of the NTFS filesystem. It provides <a id="_idIndexMarker575" class="pcalibre1 pcalibre2 pcalibre"/>metadata associated with files and directories, including timestamps, file sizes, and directory-to-file relationships. Investigators can use MFT to reconstruct the sequence of events concerning <span>filesystem changes.</span></li></ul></li>
</ul>
<h4 class="calibre136">Linux operating systems</h4>
<p class="calibre3">Given that most of the Linux <a id="_idIndexMarker576" class="pcalibre1 pcalibre2 pcalibre"/>ecosystem is open sourced and used in various industry sectors, including its application in the cloud, Linux provides investigators with much forensic value. When collecting forensic artifacts from Linux or performing live forensics, you should prioritize the <span>following artifacts:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="source-inline">/etc/passwd</strong>: A plaintext file that contains information such as the <strong class="bold">username</strong>, <strong class="bold">User ID</strong> (<strong class="bold">UID</strong>), <strong class="bold">Group ID</strong> (<strong class="bold">GID</strong>), <strong class="bold">home path</strong>, and <strong class="bold">default </strong><span><strong class="bold">shell</strong></span><span> application.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/group</strong>: Similar to the <strong class="source-inline">/etc/passwd</strong> file, it stores information related to user groups on <span>the system.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/crontab</strong>: Cron is a job scheduler that automates the execution of commands or scripts per <a id="_idIndexMarker577" class="pcalibre1 pcalibre2 pcalibre"/>a predefined schedule. Threat actors can use cron jobs to maintain persistence within <span>an environment.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/fstab</strong>: Contains information about the filesystem, drives, and partitions, including how devices are mounted <span>at startup.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/rc.d/**</strong>: Contains startup and shutdown scripts for services. Threat actors can use this directory to maintain persistence and <span>evade detection.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/init.d/**</strong>: Similar to <strong class="source-inline">/etc/rc.d/**</strong>, this<a id="_idIndexMarker578" class="pcalibre1 pcalibre2 pcalibre"/> directory contains <strong class="bold">init</strong> <strong class="bold">scripts</strong> or initialization scripts for system services. Threat actors can use this directory to inject malicious scripts during <span>system initialization.</span></li>
<li class="calibre13"><strong class="source-inline">/etc/systemd.d/**</strong>: Contains additional configurations for init scripts. It also allows you to override or extend script scope <span>and services.</span></li>
<li class="calibre13"><strong class="source-inline">/var/log/**</strong>: This directory contains valuable event logs stored in files generated by system processes <span>and applications.</span></li>
<li class="calibre13"><strong class="source-inline">/home/&lt;username&gt;/*</strong>: The home directory is the user’s directory (defined within <strong class="source-inline">/etc/passwd</strong>), allowing the user to store files, configurations, and <span>user-specific data.</span></li>
<li class="calibre13"><strong class="source-inline">/home/*/.ssh/known_hosts</strong>: Contains a list of public keys for remote servers that the user has previously used. It provides evidence of attempts of SSH-based connection to these <span>remote servers.</span></li>
<li class="calibre13"><strong class="source-inline">/home/*/.ssh/autorized_keys</strong>: It contains a list of public keys allowed to authenticate as the user for remote access. When a user wants to log into a remote server using SSH key-based authentication, their public key is added to the <strong class="source-inline">authorized_keys</strong> file on <span>the server.</span></li>
</ul>
<h3 class="calibre11">Other artifacts/metadata</h3>
<p class="calibre3">Once we have collected host-specific artifacts, investigators investigating the cloud instance should also <a id="_idIndexMarker579" class="pcalibre1 pcalibre2 pcalibre"/>collect <strong class="bold">instance metadata</strong>, including instance configuration, IP address allocations, VPC Subnet assignments, policy configurations, <span>and more.</span></p>
<p class="calibre3">As we can see, host forensics involves meticulously examining a computer or device’s storage, operating <a id="_idIndexMarker580" class="pcalibre1 pcalibre2 pcalibre"/>system, and files to identify and gather critical digital evidence. This process uncovers artifacts such as log files, user profiles, registry entries, and system logs, aiding in reconstructing events, user activities, and potential <span>security breaches.</span></p>
<p class="calibre3">Shifting our focus from host forensics, let’s jump into memory forensics. Memory forensics uncovers vital insights by examining the volatile memory of a system, providing a deeper understanding of ongoing processes, active applications, and potentially <span>concealed artifacts.</span></p>

<h3 class="calibre11">Fast forensic collection tools</h3>
<p class="calibre3">Fast forensics refers to using streamlined and efficient methodologies and tools in digital forensics investigations. Traditional forensic processes can be time-consuming, leading to delays in investigations. Fast forensics aims to address this issue by prioritizing speed without compromising the integrity of the investigation. Investigators should consider fast forensics approaches to enhance their response times, allowing them to identify and <a id="_idIndexMarker581" class="pcalibre1 pcalibre2 pcalibre"/>mitigate cyber threats quickly, respond to incidents promptly, and minimize the impact of <span>the incident.</span></p>
<p class="calibre3">Several tools have been developed to support fast forensics. Fast forensics is typically practical when dealing with a large organization with endpoints spread geographically. Some of the notable fast forensic tools are <span>as follows:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">CyLR</strong>: CyLR is a live response tool<a id="_idIndexMarker582" class="pcalibre1 pcalibre2 pcalibre"/> that collects artifacts from various sources and creates a package that<a id="_idIndexMarker583" class="pcalibre1 pcalibre2 pcalibre"/> can be utilized in offline analysis. CyLR packages can be analyzed using Magnet Axiom or Sleutkit Autopsy tools. CyLR can be deployed via EDR or popular script deployment techniques for automated collections. It can be used to collect from Windows and Linux <span>operating systems.</span></li>
<li class="calibre13"><strong class="bold">KAPE</strong>: Developed by Eric<a id="_idIndexMarker584" class="pcalibre1 pcalibre2 pcalibre"/> Zimmerman, <strong class="bold">Kroll Artifact Parser and Extractor</strong> (<strong class="bold">KAPE</strong>) is known for its modular and <a id="_idIndexMarker585" class="pcalibre1 pcalibre2 pcalibre"/>extensible framework, making it highly adaptable to various digital forensic scenarios. The tool is particularly valued for its ability to collect artifacts from various<a id="_idIndexMarker586" class="pcalibre1 pcalibre2 pcalibre"/> sources within an operating system, helping investigators efficiently extract crucial evidence. KAPE utilizes a configuration file to define specific artifacts and locations of interest, allowing forensic practitioners to customize their data collection based on the requirements of a <span>particular investigation.</span></li>
<li class="calibre13"><strong class="bold">PowerForensics</strong>: PowerForensics, on the other hand, is a specific PowerShell module focused on <a id="_idIndexMarker587" class="pcalibre1 pcalibre2 pcalibre"/>forensics. It provides a set of cmdlets that enable users to<a id="_idIndexMarker588" class="pcalibre1 pcalibre2 pcalibre"/> interact with the NTFS filesystem, allowing for valuable forensic information to be extracted from Windows machines. PowerForensics can be used to analyze artifacts such as file metadata, file content, and other <span>filesystem structures.</span></li>
<li class="calibre13"><strong class="bold">Kansa</strong>: Kansa is an open <a id="_idIndexMarker589" class="pcalibre1 pcalibre2 pcalibre"/>source incident response and threat-hunting framework <a id="_idIndexMarker590" class="pcalibre1 pcalibre2 pcalibre"/>written in PowerShell. It facilitates collecting and analyzing artifacts from Windows systems to aid in security investigations. Kansa provides a set of PowerShell scripts and modules that can automate various aspects of the incident response process. The framework allows security professionals and incident responders to run predefined or custom PowerShell scripts across multiple endpoints in <span>a network.</span></li>
</ul>
<p class="calibre3">We have included more details in the <em class="italic">Further </em><span><em class="italic">reading</em></span><span> section.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-146" class="calibre10"><a id="_idTextAnchor157" class="pcalibre1 pcalibre2 pcalibre"/>Memory forensics</h2>
<p class="calibre3">Memory forensics is an advanced digital investigation technique that analyzes a computer or device’s volatile <a id="_idIndexMarker591" class="pcalibre1 pcalibre2 pcalibre"/>memory (<strong class="bold">random access memory</strong> – <strong class="bold">RAM</strong>). Unlike traditional host forensics, which <a id="_idIndexMarker592" class="pcalibre1 pcalibre2 pcalibre"/>examines storage and files, memory forensics dives into the live state of a system, revealing active processes, running applications, and hidden artifacts that can provide valuable insights into cyberattacks, malicious activities, and volatile data that might not be <a id="_idIndexMarker593" class="pcalibre1 pcalibre2 pcalibre"/>stored on disk. Everything that’s executed within the operating system of the host has to go through the host’s memory. This approach offers a unique perspective on the digital landscape, enabling investigators to uncover<a id="_idIndexMarker594" class="pcalibre1 pcalibre2 pcalibre"/> crucial evidence that might otherwise <span>remain concealed.</span></p>
<p class="calibre3">Memory forensics captures information such as running processes and threads, malware or rootkits, open file handles, caches and clipboard contents, encryption keys, hardware and software configuration, registry keys, websites visited, and commands entered on <span>the console.</span></p>
<p class="calibre3">Memory forensics in cloud environments presents a unique set of challenges and opportunities. As organizations increasingly migrate their systems to cloud platforms, understanding and analyzing volatile memory becomes crucial for detecting security breaches, insider threats, and <span>unauthorized access.</span></p>
<p class="calibre3">Especially in the cloud, memory forensics faces challenges due to the shared nature of resources, dynamic provisioning, and limited access to physical hardware. Virtualization and containerization add layers of complexity, making it essential to adapt traditional memory forensics <a id="_idIndexMarker595" class="pcalibre1 pcalibre2 pcalibre"/>methodologies. However, the cloud also offers advantages such as centralization of logs, ease of scalability, and the potential to capture memory snapshots across various instances simultaneously. This can aid in identifying sophisticated attacks that might target multiple instances or users within the <span>cloud environment.</span></p>
<p class="calibre3">Let’s look at some primary artifacts investigators should typically collect and analyze. These artifacts resemble the Windows operating system running in the cloud or on a server. Given that memory forensics is worthy of a separate book, we assume you are familiar with the basics of computer memory and its various elements <span>and functionalities.</span></p>

<h3 class="calibre11">Memory acquisition tools</h3>
<p class="calibre3">In the context of cloud forensics, there are a few options for investigators: collect and analyze the memory images <a id="_idIndexMarker596" class="pcalibre1 pcalibre2 pcalibre"/>of a live system or collect and analyze the memory artifacts written to the disk in an offline host. The following figure illustrates the distinction between live and dead systems and the tools available for collecting memory images <span>from them:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer134"><img alt="Figure 8.2 – Memory acquisition sources/tools" src="../images/00003.jpeg" class="calibre137"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.2 – Memory acquisition sources/tools</p>
<p class="calibre3">Let’s look at some of the toolsets that can acquire memory in a live system while looking at memory remnants of a<a id="_idIndexMarker597" class="pcalibre1 pcalibre2 pcalibre"/> dead system that can be collected for <span>forensic investigations:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Live systems</strong>: Live system memory <a id="_idIndexMarker598" class="pcalibre1 pcalibre2 pcalibre"/>acquisition tools are essential components of memory forensics, allowing investigators to capture and analyze the volatile memory of a running host system. Here are a few live system memory <span>acquisition tools:</span><ul class="calibre17"><li class="calibre13"><strong class="bold">WinPmem</strong>: <strong class="bold">WinPmem</strong> is a memory acquisition tool for <strong class="bold">Windows</strong> operating systems. It operates as a kernel module to ensure reliable memory captures without disrupting<a id="_idIndexMarker599" class="pcalibre1 pcalibre2 pcalibre"/> the system’s operation. It provides forensically sound memory dumps for subsequent analysis using compatible tools <a id="_idIndexMarker600" class="pcalibre1 pcalibre2 pcalibre"/>such <span>as </span><span><strong class="bold">Volatility</strong></span><span>.</span></li><li class="calibre13"><strong class="bold">FTKImager</strong>: This is one of the most popular and easy-to-use tools for memory capture for Windows-based<a id="_idIndexMarker601" class="pcalibre1 pcalibre2 pcalibre"/> operating systems. Designed primarily for disk imaging and data acquisitions, it can capture live memory images, including collecting pagefiles and memory dumps <span>if required.</span></li><li class="calibre13"><strong class="bold">Magnet RAM Capture</strong>: Magnet<a id="_idIndexMarker602" class="pcalibre1 pcalibre2 pcalibre"/> RAM Capture is a tool designed for quickly capturing live memory from Windows systems. It’s known for its simplicity and efficiency, making it suitable for experienced analysts and those new to <span>memory forensics.</span></li><li class="calibre13"><strong class="bold">Belkasoft RAM Capture</strong>: <strong class="bold">Belkasoft RAM Capture</strong> is a memory acquisition tool designed to capture the live <a id="_idIndexMarker603" class="pcalibre1 pcalibre2 pcalibre"/>memory (RAM) of Windows computers. It was developed by Belkasoft, a digital forensics and incident response software company. Like other memory acquisition tools, Belkasoft RAM Capture is utilized in digital investigations to gather volatile data from running systems for<a id="_idIndexMarker604" class="pcalibre1 pcalibre2 pcalibre"/> <span>subsequent analysis.</span></li><li class="calibre13"><strong class="bold">GRR Rapid Response</strong>: <strong class="bold">GRR Rapid Response</strong> (<strong class="bold">GRR</strong>) is an open source incident response platform <a id="_idIndexMarker605" class="pcalibre1 pcalibre2 pcalibre"/>created by Google. It allows security teams to perform remote live forensics and investigations across many endpoints in a networked environment. GRR offers real-time data collection, remote memory acquisition, filesystem analysis, and process monitoring. Its client-server architecture allows administrators and analysts to manage and control agents deployed on target <span>systems centrally.</span></li><li class="calibre13"><strong class="bold">dd</strong>: In Linux systems, <strong class="source-inline">dd</strong> is a command that’s commonly used on Linux/Unix platforms for low-level copying and conversion of data. In the context of digital forensics, <strong class="source-inline">dd</strong> is used<a id="_idIndexMarker606" class="pcalibre1 pcalibre2 pcalibre"/> in Linux/Unix systems for acquiring a copy of the memory or RAM. Note that it’s not typically recommended due to the potential risks and challenges involved. Improper use of <strong class="source-inline">dd</strong> for memory acquisition can result in inaccurate or incomplete memory captures and disrupt the target <span>system’s operation.</span></li></ul></li>
<li class="calibre13"><strong class="bold">Offline systems</strong>: Also known as dead <a id="_idIndexMarker607" class="pcalibre1 pcalibre2 pcalibre"/>systems, these are the systems that investigators cannot access remotely or connect to. In offline systems, investigators look for memory remnants left on the disk when a host was pulled offline. These affect what information can be gathered by investigators. Some of the most common artifacts are <span>as follows:</span><ul class="calibre17"><li class="calibre13"><strong class="bold">Page files (pagefile.sys and hiberfil.sys)</strong>: In Windows-based operating systems, a page file (<strong class="source-inline">pagefile.sys</strong>) is a virtual memory extension, allowing the memory to use this storage to store data temporarily. In older Windows versions, this file is known as <strong class="source-inline">swapfile.sys</strong>. Typically, this is performed when physical RAM space is at capacity and applications are queued for execution; Windows will automatically offload memory pages into disk via <strong class="source-inline">pagefile.sys</strong>. From a forensic perspective, <strong class="source-inline">pagefile.sys</strong> can contain remnants of sensitive or valuable information, including fragments of files, registry data, and even passwords. These remnants might not be present in the main memory or traditional disk<a id="_idIndexMarker608" class="pcalibre1 pcalibre2 pcalibre"/> storage, making the page file a potential source of evidence. Note that the page file is volatile, meaning its contents are not retained after a system shutdown or reboot. However, if a system is hibernated, the contents of the page file can persist in the hibernation file (<strong class="source-inline">hiberfil.sys</strong>) for potential recovery. It’s a good practice to capture page files during <span>memory acquisition.</span></li><li class="calibre13"><strong class="bold">Crash dumps</strong>: Crash dumps are also an excellent source for memory analysis. Typically, crash dumps are stored as <strong class="source-inline">MEMORY.DMP</strong> in the <strong class="source-inline">%SystemRoot%</strong> folder of your Windows operating system. Based on the size of the dump file, investigators can identify if it’s a complete memory dump (also known<a id="_idIndexMarker609" class="pcalibre1 pcalibre2 pcalibre"/> as <strong class="bold">Kernal Memory Dump</strong>) or a snapshot of a memory page that was dumped during a crash (<strong class="bold">Active Memory Dump</strong>). In any<a id="_idIndexMarker610" class="pcalibre1 pcalibre2 pcalibre"/> case, existing memory analysis tools should allow investigators to parse and analyze these crash dumps. Note that in Linux systems, memory pages written to disk are known as <strong class="bold">swap files</strong> and are only utilized when the allocated memory is<a id="_idIndexMarker611" class="pcalibre1 pcalibre2 pcalibre"/> insufficient. Investigators can only collect the swap files. Swap files are not retained on the disk when rebooting a Linux system. Note that investigators may see swap files on disk if a Linux system is hibernated. However, this is rare; Linux systems are not <span>commonly hibernated.</span></li></ul></li>
</ul>
<p class="calibre3">While there are plenty of options for collecting memory images (live or dead systems), it is essential to note that from a cloud forensics standpoint, it is no different from collecting memory swap files or pages from the disk as that of a dead box system. Investigators must collect the correct disk copies associated with the respective cloud instances. Now that we have a memory snapshot or an image, let’s explore the tools to <span>analyze them.</span></p>

<h3 class="calibre11">Memory analysis tools</h3>
<p class="calibre3">In this section, we’ll look at some of the tools that are commonly used during an investigation, especially for analyzing system memory artifacts. Once memory images have been collected, there are special<a id="_idIndexMarker612" class="pcalibre1 pcalibre2 pcalibre"/> tools that carve information <span>off memory:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Volatility</strong>: Volatility is one of the<a id="_idIndexMarker613" class="pcalibre1 pcalibre2 pcalibre"/> most popular open source memory forensics frameworks and a preferred choice by investigators for memory analysis. Written entirely in Python, it allows you to analyze memory snapshots and supports various operating systems. It provides multiple plugins for extracting information about running processes, network activity, registry data, and more. Volatility supports multiple memory dump formats and is extensively used by professionals in the field. Volatility can also natively analyze older Windows versions of <strong class="bold">hiberfil.sys</strong> and <strong class="bold">pagefile.sys</strong>. However, various other specific tools are developed to address challenges and handle changes to hibernate files with changes to Windows operating systems in a much <span>better way.</span></li>
<li class="calibre13"><strong class="bold">Velociraptor</strong>: Velociraptor is an open source endpoint monitoring and digital forensics platform designed to<a id="_idIndexMarker614" class="pcalibre1 pcalibre2 pcalibre"/> provide high-fidelity data collection and analysis capabilities across a network of endpoints. One of the critical features of Velociraptor is its ability to perform live memory analysis on endpoints to uncover insights, detect anomalies, and gather evidence for incident response and forensic investigations. Analysts can tailor memory analysis tasks to their specific investigation needs with support for customizable queries and plugins. Beyond memory analysis, Velociraptor facilitates proactive threat detection through predefined indicators and patterns. Its centralized management, workflow automation, scalability, and active user community contribute to its effectiveness in various environments (physical, virtual, and <span>the cloud).</span></li>
<li class="calibre13"><strong class="bold">Rekall</strong>: Rekall is a fork on Volatility and enhanced it by enabling live memory forensics. Like Volatility, Rekall includes most <a id="_idIndexMarker615" class="pcalibre1 pcalibre2 pcalibre"/>of the features of Volatility and can analyze <strong class="bold">hiberfil.sys</strong>, <strong class="bold">pagefile.sys</strong>, and swap files in different <span>operating systems.</span></li>
<li class="calibre13"><strong class="bold">GRR</strong>: GRR is another open source incident response platform that is agent-based. Investigators must deploy an agent to capture telemetry or perform live memory forensics. Although <a id="_idIndexMarker616" class="pcalibre1 pcalibre2 pcalibre"/>more powerful memory analysis tools <a id="_idIndexMarker617" class="pcalibre1 pcalibre2 pcalibre"/>are available, such as Velociraptor and Volatility, GRR remains a valuable tool in any organization’s DFIR arsenal due to its unique strengths and capabilities. GRR is designed explicitly for collecting endpoint data, including real-time analysis of memory files, making it a versatile tool for any <span>cybersecurity team.</span></li>
<li class="calibre13"><strong class="bold">Magnet AXIOM</strong>: Designed to assist law<a id="_idIndexMarker618" class="pcalibre1 pcalibre2 pcalibre"/> enforcement agencies, corporate investigators, and DFIR professionals, Magnet AXIOM is a license-based commercial tool developed by Magnet Forensics that focuses on efficiently collecting, analyzing, and presenting digital evidence from various sources. It supports data acquisition from diverse sources such as computers, smartphones, and cloud services. Magnet AXIOM also supports the analysis of artifacts such as files, emails, and chat messages. It supports memory images captured from various operating systems, including <strong class="bold">hiberfil.sys</strong>, <strong class="bold">pagefile.sys</strong>, and swap files. It can also parse quick forensics packages collected through multiple third-party tools. Magnet AXIOM supports timeline analysis and reconstructs events <a id="_idIndexMarker619" class="pcalibre1 pcalibre2 pcalibre"/>from digital evidence. It also supports keyword searches, data carving, and advanced filtering to help investigators quickly pinpoint relevant information. Finally, it streamlines report generation, supports collaboration, and specializes in mobile <span>device analysis.</span></li>
</ul>
<p class="calibre3">In conclusion, host and memory forensics are indispensable pillars in digital investigations. The analysis of artifacts on operating systems, coupled with the extraction of volatile memory data, allows digital forensic investigators to reconstruct digital trails, uncover hidden evidence, and decipher the story behind cyber incidents. Host forensics comprehensively explains a system’s history and <span>user activities.</span></p>
<p class="calibre3">In contrast, memory forensics enables real-time snapshots to be captured, exposing the inner workings of processes and potential threats. Intertwined with advanced tools and methodologies, these disciplines collectively contribute to the pursuit of truth and justice in the dynamic landscape of digital forensics. We will learn how to acquire disk and memory images from the cloud in <a href="part0030_split_000.html#_idTextAnchor199" class="pcalibre1 pcalibre2 pcalibre"><span><em class="italic">Chapter 10</em></span></a>. The following section focuses on techniques to perform<a id="_idIndexMarker620" class="pcalibre1 pcalibre2 pcalibre"/> live forensic analysis using various tools, as well as perform threat hunting, given that threat actors have evolved and use sophisticated techniques to hide malware in <span>plain sight.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-147" class="calibre5"><a id="_idTextAnchor158" class="pcalibre1 pcalibre2 pcalibre"/>Live forensic analysis and threat hunting</h1>
<p class="calibre3">Digital forensic investigators operate on the principle that malware must always run on memory; there is nowhere they <a id="_idIndexMarker621" class="pcalibre1 pcalibre2 pcalibre"/>can hide. However, in recent times, technology has evolved to make memory massive and less volatile, giving rise to fileless malware – that is, malware<a id="_idIndexMarker622" class="pcalibre1 pcalibre2 pcalibre"/> that does not touch the disk – which maintains this hidden nature until execution time. The following sections will cover some of the tools that modern corporate investigators utilize to identify malware and conduct threat hunting, helping you understand common persistence mechanisms that <span>malware uses.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-148" class="calibre10"><a id="_idTextAnchor159" class="pcalibre1 pcalibre2 pcalibre"/>EDR-based threat hunting</h2>
<p class="calibre3">Advancements in computational technologies, cloud infrastructure, and support for massive disk and memory sizes have made it necessary for a new set of tools that can continuously monitor a host and <a id="_idIndexMarker623" class="pcalibre1 pcalibre2 pcalibre"/>collect live telemetry data on disk and memory, capture every footprint of an application, spot malware, and stop the attack before it can<a id="_idIndexMarker624" class="pcalibre1 pcalibre2 pcalibre"/> get worse. This technology, known as <strong class="bold">EDR</strong>, is a new class of security tools (commercial tools) that collects advanced telemetry and uses various techniques beyond standard signature-based detection to identify and detect malware and respond to the threat tactically. We typically refer to EDRs as antiviruses on steroids as they can do much more than a simple antivirus, which threat actors can easily turn off or evade in terms of <span>being detected.</span></p>
<p class="calibre3">EDRs generally operate at the operating system kernel level and can identify various activities on disk and memory beyond what a user can see. EDRs can track process calls, information regarding child processes, Windows <strong class="bold">application programing interface</strong> (<strong class="bold">API</strong>) usage, the command line passed<a id="_idIndexMarker625" class="pcalibre1 pcalibre2 pcalibre"/> to each process, network connections, process thread information, <strong class="bold">Dynamic Link Libraries</strong> (<strong class="bold">DLLs</strong>), file handles, registry handles, <span>and more.</span></p>
<p class="calibre3">Note that EDRs are not forensic tools. However, they do enhance forensic investigation by providing high-fidelity telemetry that would otherwise be missed if EDRs were not deployed within an organization. Most popular EDRs also offer forensic teams the ability to perform live queries on the host. You can query the host system using preset command-line parameters and perhaps even run a custom-built script to collect additional information or artifacts. Forensic investigators typically use live quer<a id="_idTextAnchor160" class="pcalibre1 pcalibre2 pcalibre"/>ying to download artifacts, logs, or even quick forensic packages for <span>offline analysis.</span></p>

<h3 class="calibre11">Diving deep with EDR hunting</h3>
<p class="calibre3">Let’s assume that, as investigators, an EDR tool was deployed as part of the breach response. We are using the <strong class="bold">SentinelOne Singularity platform</strong> (used with permission) to demonstrate hunting using <span>this </span><span><a id="_idIndexMarker626" class="pcalibre1 pcalibre2 pcalibre"/></span><span>method.</span></p>
<p class="calibre3">Once the SentinelOne agent has been installed on the infected system, it typically scans the system and starts breach containment, meaning any detected malware is contained. An alert is raised on the central Singularity console. This becomes the starting point for investigating or hunting, and investigators can continue to pursue their investigation via this EDR tool. In the following screenshot, notice that there’s additional information under the <strong class="bold">Threats</strong> dashboard while providing options for kick-starting threat hunting. The <strong class="bold">SHA1</strong> hash also allows <a id="_idIndexMarker627" class="pcalibre1 pcalibre2 pcalibre"/>investigators to look at other threat intelligence sources, such <span>as VirusTotal:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer135"><img alt="Figure 8.3 – Sample alert on SentinelOne EDR (produced with permission)" src="../images/00020.jpeg" class="calibre138"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.3 – Sample alert on SentinelOne EDR (produced with permission)</p>
<p class="calibre3">Once investigators can obtain enough information about the circumstances of the threat, they can begin to perform their threat hunting. SentinelOne offers multiple ways to hunt for threats. One of the easiest ways is through their Singularity <strong class="bold">eXtended Detection and Response</strong> (<strong class="bold">XDR</strong>) module, which presents additional telemetry data, allowing investigators to slice and dice them, more<a id="_idIndexMarker628" class="pcalibre1 pcalibre2 pcalibre"/> or less through a live flow of telemetry data. Based on the threat dashboard information, we can use the SHA1 hash to conduct deep dive hunting using Singularity XDR. We start by searching for the hash value that was identified earlier. The SentinelOne Singularity XDR module offers search filters for investigators to hunt through telemetry data easily. In the following example, we’re using the following to search for the <strong class="source-inline">tgt.file.sha1 = "41e8db9bb005fce152e08c20e6392e0a5d44bb6e"</strong> <span>SHA1 entry:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer136"><img alt="Figure 8.4 – SHA1 hash-based hunting in the SentinelOne Singularity XDR module" src="../images/00185.jpeg" class="calibre139"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.4 – SHA1 hash-based hunting in the SentinelOne Singularity XDR module</p>
<p class="calibre3">Notice that each result has a checkbox that provides more information regarding the data collected. This includes information regarding the event itself, the account it was executed under, the SentinelOne agent’s name and endpoint makeup, and details of the detection, including process information and command-line parameters supplied. Each entry can further be used to hunt deeper, which is especially useful for complex <span>threat scenarios.</span></p>
<p class="calibre3">SentinelOne Singularity XDR <a id="_idIndexMarker629" class="pcalibre1 pcalibre2 pcalibre"/>natively presents a collection of all the fields in the telemetry data. The <strong class="bold">FIELDS</strong> view allows investigators to quickly jump in and investigate without spending time identifying their way around it. Here is an example of the field list that’s part of the navigation options. It also provides aggregated field results for investigators to identify any anomalies. Each result can be clicked through to apply relevant search filters <span>for hunts:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer137"><img alt="Figure 8.5 – SentinelOne Singularity XDR’s fields" src="../images/00114.jpeg" class="calibre140"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.5 – SentinelOne Singularity XDR’s fields</p>
<p class="calibre3">Once initial hunts have been performed using the Singularity XDR module, based on the results, investigators can quickly pivot to obtain more information regarding the activities around the detected threat. Through telemetric data, we know that the threat actor used PowerShell to attempt to access and execute the malware. The sample screenshot shows the event data<a id="_idIndexMarker630" class="pcalibre1 pcalibre2 pcalibre"/> identifying this threat actor’s action, allowing investigators to pivot further to obtain more situational information about <span>the event.</span></p>
<p class="calibre3">Investigators can use the <strong class="bold">Event Details</strong> tab when a relevant event entry is accessed (see <span><em class="italic">Figure 8</em></span><em class="italic">.3</em>). <strong class="bold">Event Details</strong> provides<a id="_idIndexMarker631" class="pcalibre1 pcalibre2 pcalibre"/> more information about the event, including the commands for invoking <span>the malware:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer138"><img alt="Figure 8.6 – Event Details based on the SigularityXDR search result" src="../images/00153.jpeg" class="calibre141"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.6 – Event Details based on the SigularityXDR search result</p>
<p class="calibre3">As we can see, EDR enhances the process of detecting and preventing malware from being executed. In the context of cloud forensics, having an EDR/XDR tool to investigate a cloud endpoint combined with the logs generated on the cloud console enhances the investigator’s ability to analyze and identify how the threat occurred. Furthermore, from a legal standpoint, this allows <a id="_idIndexMarker632" class="pcalibre1 pcalibre2 pcalibre"/>investigators to corroborate the events from various sources, ensuring the identified investigations are forensically sound and <span>legally acceptable.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-149" class="calibre10"><a id="_idTextAnchor161" class="pcalibre1 pcalibre2 pcalibre"/>Hunting for malware</h2>
<p class="calibre3">Hunting for malware is a proactive <a id="_idIndexMarker633" class="pcalibre1 pcalibre2 pcalibre"/>approach to uncovering malicious applications/software within a host. This technique goes beyond traditional security measures by actively seeking signs of compromise, identifying root causes, and preventing future incidents. Incident responders analyze system behavior, employ threat intelligence, and leverage memory analysis to uncover signs of compromise. By scrutinizing logs, conducting sandboxing, and utilizing automated tools, they detect anomalies and patterns associated with malware. Here are some of the ways malware will commonly try to <span>evade detection:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Service hijacking</strong>: Service hijacking involves malicious actors gaining control over system services, enabling them to execute arbitrary code within the context of legitimate processes. By compromising trusted services, attackers can execute malicious commands or payloads, evading detection. One real-life example is the <strong class="bold">Zeus</strong> banking Trojan, which <a id="_idIndexMarker634" class="pcalibre1 pcalibre2 pcalibre"/>exploited the WMI service to execute malicious code and maintain persistence on <span>infected systems.</span></li>
<li class="calibre13"><strong class="bold">Process injection</strong>: Process injection is a technique that malware uses to insert its code into a legitimate process, effectively hiding its presence within a trusted application’s memory space. Common injection methods include DLL injection, reflective DLL injection, and process hollowing. Detecting process injection involves monitoring memory regions for unexpected modifications, analyzing process memory for code inconsistencies, and examining API calls to identify signs of injected code execution. The <strong class="bold">TrickBot</strong> malware<a id="_idIndexMarker635" class="pcalibre1 pcalibre2 pcalibre"/> used process injection techniques such as reflective DLL injection to inject malicious code into legitimate processes, evading detection by <span>security solutions.</span></li>
<li class="calibre13"><strong class="bold">Alternate Data Streams</strong> (<strong class="bold">ADS</strong>): ADS provides a way to hide data within a legitimate file by attaching additional data streams. Malware can use ADS to store its code or configuration, making<a id="_idIndexMarker636" class="pcalibre1 pcalibre2 pcalibre"/> detection challenging. Hunting for ADS involves analyzing file metadata, checking for multiple data streams within files, and monitoring unusual data <span>stream associations.</span></li>
<li class="calibre13"><strong class="bold">Web shells</strong>: Web shells are malicious scripts or code snippets embedded within web applications <a id="_idIndexMarker637" class="pcalibre1 pcalibre2 pcalibre"/>or servers, allowing attackers to gain remote access and control. The <strong class="bold">Shellshock</strong> vulnerability allowed attackers to inject<a id="_idIndexMarker638" class="pcalibre1 pcalibre2 pcalibre"/> malicious code into web server environments, effectively deploying web shells to gain <span>unauthorized access.</span></li>
<li class="calibre13"><strong class="bold">Packing</strong>: Packing involves compressing or encrypting malware files to obfuscate their contents and prevent straightforward analysis. Malware typically unpacks in memory during runtime, making it difficult for investigators to sample it and identify its true nature. Detecting packing involves identifying packed executable headers, analyzing file entropy levels, and employing unpacking tools or techniques to reveal the original code. One of the most common application packers that<a id="_idIndexMarker639" class="pcalibre1 pcalibre2 pcalibre"/> legitimate applications and malware use <span>is </span><span><strong class="bold">UPX</strong></span><span>.</span></li>
<li class="calibre13"><strong class="bold">Code signing</strong>: Malware authors might sign their code with stolen or fraudulently obtained digital certificates to appear legitimate and avoid detection by security software. Detecting code signed with valid certificates involves checking the certificate’s authenticity, examining the certificate chain, and monitoring for revoked certificates or anomalies in the certificate’s usage. There have been multiple examples in recent history where attackers stole an organization’s code signing certificate to sign malware and use it for downstream supply <span>chain attacks.</span></li>
<li class="calibre13"><strong class="bold">Fileless malware</strong>: Fileless malware operates in memory without leaving traces on disk, making detection difficult. This technique often involves leveraging scripting languages or exploiting system tools. Detecting these techniques requires collecting specific logs, such as PowerShell, and obtaining a memory image. EDRs are a great way to detect these attacks since they monitor and track filesystem changes and memory activities. Popular fileless malware methods use PowerShell’s <strong class="bold">IEX</strong> cradle to download an application or additional scripts in memory and execute it at runtime without ever touching the disk. Recently, threat actors have been spotted using PowerShell Empire, a framework that allows fileless malware <span>delivery techniques.</span></li>
</ul>
<p class="calibre3">As we can see, threat actors can hide and evade detection on a host in various ways. While we discussed the most <a id="_idIndexMarker640" class="pcalibre1 pcalibre2 pcalibre"/>common and obvious ones, advanced threat actor groups use innovative techniques. To some extent, malware running in a cloud infrastructure may make an investigator’s life easier because CSPs have integrated various security tools to identify and spot <span>malicious activities.</span></p>
<p class="calibre3">Moreover, most CSPs partner with multiple security vendors that provide tooling and visibility into these hosts, thus helping investigators identify malware more quickly than traditional investigative methods. For example, AWS GuardDuty can scan the VMs directly without any agent installation and provide visibility into the malware detection capability. At the same time, GCP’s CloudSCC and Azure’s Security Center have similar features that monitor the VMs and notify the administrator of malicious programs. In either case, these detections are provided by CSP’s partners yet available for investigators in the respective CSP security consoles. We’ll cover these tools in <em class="italic">Chapters</em> <em class="italic">4</em>, <em class="italic">5</em>, <span>and </span><span><em class="italic">6</em></span><span>.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-150" class="calibre10"><a id="_idTextAnchor162" class="pcalibre1 pcalibre2 pcalibre"/>Common persistence mechanisms</h2>
<p class="calibre3">We will now look at common persistence mechanisms that malware can typically employ. It is no different in the context of the cloud since malware will run irrespective of the <span>underlying infrastructure:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">AutoStart Extension Point</strong> (<strong class="bold">ASEP</strong>): ASEP is a concept in the Microsoft Windows operating system that <a id="_idIndexMarker641" class="pcalibre1 pcalibre2 pcalibre"/>allows applications <a id="_idIndexMarker642" class="pcalibre1 pcalibre2 pcalibre"/>and services to launch and run when the system starts up automatically. These extension points allow developers to integrate their software into the Windows startup process, ensuring their applications or services are available and operational as soon as the system boots up. However, given its features and the intent to help developers, it aids malware operators in persisting in an environment by automatically launching itself when a host is booted. Here are some <span>common</span><span><a id="_idIndexMarker643" class="pcalibre1 pcalibre2 pcalibre"/></span><span> ASEPs:</span><ul class="calibre17"><li class="calibre13"><strong class="bold">Service Control Manager (SCM) services</strong>: System services are configured to start automatically during boot through SCM. These services run in the background and perform various <span>system-level tasks.</span></li><li class="calibre13"><strong class="bold">Run keys in the Windows Registry</strong>: Entries in the <strong class="source-inline">Run</strong> key of the Windows Registry start applications or scripts during every user login, allowing user-specific <a id="_idIndexMarker644" class="pcalibre1 pcalibre2 pcalibre"/>applications to launch automatically. Some of the common Registry keys to scan for are <span>as follows:</span>
<pre class="source-code"><strong class="bold">Computer\HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Run</strong>
<strong class="bold">Computer\HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\RunOnce</strong>
<strong class="bold">Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Run</strong>
<strong class="bold">Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce</strong></pre></li><li class="calibre13"><strong class="bold">Startup folders</strong>: Applications placed in the user’s or system’s startup folders are automatically launched when a user logs in, enabling user-specific customization of startup behavior. One of the most common folders that’s used by threat actors to maintain persistence is <span>as follows:</span>
<pre class="source-code"><strong class="bold">%AppData%\Roaming\Microsoft\Windows\Start Menu\Programs\Startup</strong></pre></li><li class="calibre13"><strong class="bold">Scheduled tasks</strong>: Scheduled tasks configured to run at system startup or user login times are managed by Windows Task Scheduler and can perform various actions, such as updates or <span>maintenance tasks.</span></li></ul></li> <li class="calibre13"><strong class="bold">Group Policy Objects</strong> (<strong class="bold">GPOs</strong>): GPOs<a id="_idIndexMarker645" class="pcalibre1 pcalibre2 pcalibre"/> are one of the most common ways threat actors maintain persistence within <a id="_idIndexMarker646" class="pcalibre1 pcalibre2 pcalibre"/>an organization. They do this by creating a malicious GPO with the company’s domain controller to prepare and dispatch malware copies for persistence and execution. Moreover, threat actors can tweak the GPO to control how malware is deployed across the enterprise centrally. Threat actors can also create a scheduled task when a condition is met and can either download scripts or trigger malware execution. Most ransomware operators<a id="_idIndexMarker647" class="pcalibre1 pcalibre2 pcalibre"/> use GPOs to centrally control and deploy ransomware across the organization, leaving no time for security <a id="_idIndexMarker648" class="pcalibre1 pcalibre2 pcalibre"/>administrators to stop <span>the attack.</span></li>
<li class="calibre13"><strong class="bold">DLL hijacking</strong>: DLL hijacking is a technique<a id="_idIndexMarker649" class="pcalibre1 pcalibre2 pcalibre"/> that’s exploited by attackers to achieve malware persistence by manipulating how Windows loads DLLs. During DLL loading, Windows follows a <a id="_idIndexMarker650" class="pcalibre1 pcalibre2 pcalibre"/>specific order to locate the required DLLs, including standard directories such as the application’s folder and system directories. Attackers identify a vulnerable application and place a malicious DLL with the same name as the needed one in a guide the application searches. When the application starts, the malicious DLL is loaded instead of the legitimate one, executing the attacker’s code within the application’s context. This allows the attacker to establish persistence, run arbitrary code, and potentially gain control over the compromised system. Due to the regular use of the targeted application, the malicious code executes consistently, ensuring persistence even after the <span>system reboots.</span></li>
<li class="calibre13"><strong class="bold">WMI event consumers</strong>: Malware persistence through WMI event consumers involves leveraging WMI’s capabilities <a id="_idIndexMarker651" class="pcalibre1 pcalibre2 pcalibre"/>to execute malicious actions at specific trigger events. Here are the steps <a id="_idIndexMarker652" class="pcalibre1 pcalibre2 pcalibre"/>attackers might take to achieve this form of persistence. These steps are ultimately stored in a <strong class="bold">Managed Object Format</strong> (<strong class="bold">MOF)</strong> file, which is used<a id="_idIndexMarker653" class="pcalibre1 pcalibre2 pcalibre"/> to register new classes within the <span>WMI repository:</span><ol class="calibre142"><li class="alphabets"><strong class="bold">Create a malicious event consumer</strong>: Attackers create a WMI event consumer, a script or executable designed to execute when a specific WMI event occurs. System events such as startup, login, or other custom triggers could trigger this event. The malicious event consumer contains instructions to execute the <span>attacker’s payload.</span></li><li class="alphabets"><strong class="bold">Event filter and binding</strong>: Attackers create an event filter that defines the conditions under which the malicious event consumer should execute. This filter is associated with a specific trigger event. They then bind the event filter to the malicious event consumer, establishing a link between the filter and the payload <a id="_idIndexMarker654" class="pcalibre1 pcalibre2 pcalibre"/>containing the script <span>or executable.</span></li><li class="alphabets"><strong class="bold">Trigger event execution</strong>: When the defined trigger event occurs, the associated event filter evaluates whether the conditions are met. If the conditions are satisfied, the <a id="_idIndexMarker655" class="pcalibre1 pcalibre2 pcalibre"/>WMI event consumer executes the payload, which could be malware, a script, or an executable. This payload runs in the context of the WMI service, allowing attackers to establish persistence, execute arbitrary code, and potentially gain control over the <span>compromised system.</span></li></ol>
<p class="calibre3">Investigators should refer to the following commands to query the WMI repository and determine <span>malicious WMI:</span></p>
<pre class="source-code"><strong class="bold">&gt; Get</strong><strong class="bold">-WmiObject -Class __FilterToConsumerBinding -Namespace root\subscription</strong>
<strong class="bold">&gt; Get</strong><strong class="bold">-WmiObject -Class __EventFilter -Namespace root\subscription</strong>
<strong class="bold">&gt; Get</strong><strong class="bold">-WmiObject -Class __EventConsumer -Namespace root\subscription</strong>
<strong class="bold">strings -q C:\windows\system32\wbem\repository\objects.data</strong></pre></li> </ul>
<p class="calibre3">In summary, the convergence of live forensics, EDR solutions, proactive malware hunting, and advanced persistence mechanisms underscore a comprehensive approach to modern cybersecurity. Live forensics, powered by cutting-edge tools and techniques, provides real-time insights into ongoing threats, enabling swift <span>incident response.</span></p>
<p class="calibre3">We are now transitioning to the realm of network forensics. Network forensics is pivotal in uncovering cyber threats as it allows us to examine network activities, traffic patterns, and communication behavior. By delving into the intricacies of network data, we will gain invaluable insights into<a id="_idIndexMarker656" class="pcalibre1 pcalibre2 pcalibre"/> potential breaches, malicious <a id="_idIndexMarker657" class="pcalibre1 pcalibre2 pcalibre"/>activities, and the broader tactics employed by adversaries. This is highly vital in the context of <span>cloud forensics.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-151" class="calibre5"><a id="_idTextAnchor163" class="pcalibre1 pcalibre2 pcalibre"/>Network forensics</h1>
<p class="calibre3">As this section’s title suggests, network forensics is an approach to forensically analyzing network protocols, packets, and <a id="_idIndexMarker658" class="pcalibre1 pcalibre2 pcalibre"/>any artifacts on the wire. Network forensics in the context of cloud environments involves analyzing network traffic, communication patterns, and data flows between CSP services and external users to uncover potential security breaches, data exfiltration, and unauthorized access. By examining network data within the cloud infrastructure, we can create a comprehensive picture of events, identify anomalies, and detect the traces left behind by cyber threats. Fundamentally, investigators must have access to network data to conduct this analysis. This deeper scrutiny allows us to respond to incidents effectively, mitigate risks, and maintain a resilient cloud <span>security posture.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-152" class="calibre10"><a id="_idTextAnchor164" class="pcalibre1 pcalibre2 pcalibre"/>Basic networking concepts</h2>
<p class="calibre3">In network forensics, investigators must always remember that any network communication is broken down into the layers outlined by the network communication model, which can be <strong class="bold">Open Systems Interconnection</strong> (<strong class="bold">OSI</strong>) or <strong class="bold">Transmission Control Protocol/Internet Protocol</strong> (<strong class="bold">TCP/IP</strong>). These models provide a clear picture to investigators of how a threat actor accessed a host and what protocols were used by the threat actor. Let’s look at these two <a id="_idIndexMarker659" class="pcalibre1 pcalibre2 pcalibre"/>models in more detail. Note that we assume you are familiar with this area; this is only <span>a refresher:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">OSI model</strong>: This model is a framework that standardizes the functions and interactions of various networking protocols, serving as a guide for designing and understanding how different networking technologies communicate. The model is divided into seven layers, each responsible for specific tasks and functions. The following figure outlines the seven layers and associated protocols that operate within each of those layers. In the context of the cloud, lower-level layers are managed by CSPs. This<a id="_idIndexMarker660" class="pcalibre1 pcalibre2 pcalibre"/> includes the <strong class="bold">physical layer</strong> and <strong class="bold">data link layer</strong>. In contrast, the <strong class="bold">network layer</strong> is a shared responsibility <a id="_idIndexMarker661" class="pcalibre1 pcalibre2 pcalibre"/>between cloud customers and CSPs since this layer allows <a id="_idIndexMarker662" class="pcalibre1 pcalibre2 pcalibre"/>customers to create their VPCs. To<a id="_idIndexMarker663" class="pcalibre1 pcalibre2 pcalibre"/> some extent, the <strong class="bold">transport layer</strong> is a shared responsibility if a specific IPsec needs to be configured, for example. On the other hand, the <strong class="bold">session</strong>, <strong class="bold">presentation</strong>, and <strong class="bold">application layers</strong> are the responsibility <a id="_idIndexMarker664" class="pcalibre1 pcalibre2 pcalibre"/>of cloud customers. While<a id="_idIndexMarker665" class="pcalibre1 pcalibre2 pcalibre"/> CSPs act as enablers, cloud customers are responsible for configuring and <span>securing them:</span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer139"><img alt="Figure 8.7 – OSI model and cloud responsibilities" src="../images/00171.jpeg" class="calibre143"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.7 – OSI model and cloud responsibilities</p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">TCP/IP reference model</strong>: This model is a widely used framework for understanding and describing the functionalities of networking protocols that power the internet and many other networks. Unlike the seven-layer OSI model, the TCP/IP model consists of four layers, each with its own set of protocols and responsibilities; however, it closely aligns with the OSI model. The following figure shows a breakdown of the TCP/IP reference model’s layers and supported protocols. In the context of the cloud, the<a id="_idIndexMarker666" class="pcalibre1 pcalibre2 pcalibre"/> reference models’ lower layers, the <strong class="bold">link layer</strong> and the <strong class="bold">CSPs</strong>, typically handle the <strong class="bold">internet layer</strong>. In contrast, the<a id="_idIndexMarker667" class="pcalibre1 pcalibre2 pcalibre"/> customer and CSPs share responsibility for the <strong class="bold">transport layer</strong>, allowing <a id="_idIndexMarker668" class="pcalibre1 pcalibre2 pcalibre"/>customers to create their <a id="_idIndexMarker669" class="pcalibre1 pcalibre2 pcalibre"/>VPC subnets. Typically, it’s the customer’s responsibility <a id="_idIndexMarker670" class="pcalibre1 pcalibre2 pcalibre"/>to manage the <span><strong class="bold">application layer</strong></span><span>:</span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer140"><img alt="Figure 8.8 – TCP/IP reference model and cloud responsibilities" src="../images/00136.jpeg" class="calibre144"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.8 – TCP/IP reference model and cloud responsibilities</p>
<p class="calibre3">In the next section, we will explore log sources that are vital for investigative purposes, and we will also explore some tools for easier <span>network analysis.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-153" class="calibre10"><a id="_idTextAnchor165" class="pcalibre1 pcalibre2 pcalibre"/>Cloud network forensics – log sources and tools</h2>
<p class="calibre3">Let’s look at some of the network<a id="_idIndexMarker671" class="pcalibre1 pcalibre2 pcalibre"/> artifacts that can be leveraged from CSPs and <span>cloud instances:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">VPC flow logs</strong>: In the previous chapters (<a href="part0022_split_000.html#_idTextAnchor065" class="pcalibre1 pcalibre2 pcalibre"><span><em class="italic">Chapter 4</em></span></a>, <a href="part0024_split_000.html#_idTextAnchor095" class="pcalibre1 pcalibre2 pcalibre"><span><em class="italic">Chapter 5</em></span></a>, and <a href="part0025_split_000.html#_idTextAnchor108" class="pcalibre1 pcalibre2 pcalibre"><span><em class="italic">Chapter 6</em></span></a>), we looked at how to turn on and enable VPC flow logs. These are the most essential logs an investigator can access for network investigation. While VPC flow logs do not capture entire network traffic packets, analyzing and determining the source and destination nodes for malicious activities <span>is vital.</span></li>
<li class="calibre13"><strong class="bold">tcpdump outputs</strong>: In situations where VPC flow logs are not helpful, such as when examining data exfiltration and uncovering what data was exported by the threat actor, investigators need an entire packet dump. tcpdump is a widely popular network capture framework for collecting complete network packets and is vital for identifying threat actor activities on the network. With tcpdump outputs, investigators can trace the threat actor activities and sessions, determine transferred files or malware, and even reconstruct the files or malware from the packet <a id="_idIndexMarker672" class="pcalibre1 pcalibre2 pcalibre"/>capture for further analysis. With full packets, investigators can also replay and identify vulnerabilities exploited in <span>the environment.</span></li>
<li class="calibre13"><strong class="bold">Logs from cloud-based firewalls and web application firewalls</strong> (<strong class="bold">WAFs</strong>): Most cloud-based organizations may also deploy a cloud-based firewall, which is a virtual firewall instance, or use CSP’s native firewalling capabilities (with its limited features). Cloud-based firewalls generate logs that detail network traffic, including allowed and denied connections, while WAFs focus on web application-related traffic, filtering out malicious requests. Analyzing these logs can identify patterns of unauthorized access, attacks, and potential breaches. Investigators can reconstruct events, pinpoint vulnerabilities, and understand attackers’ tactics by correlating firewall and WAF logs with other forensic data, such as system logs and packet captures. These logs serve as a crucial source of evidence in cloud network forensics, facilitating swift and accurate incident detection, response, and <span>mitigation measures.</span></li>
</ul>
<p class="calibre3">Now that we have an idea of the tools we can use to capture network traffic information, let’s look at an example of using one of the popular network investigation tools available <span>for investigators.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-154" class="calibre10"><a id="_idTextAnchor166" class="pcalibre1 pcalibre2 pcalibre"/>Network investigation tools</h2>
<p class="calibre3">Network investigation tools in DFIR are<a id="_idIndexMarker673" class="pcalibre1 pcalibre2 pcalibre"/> essential resources that allow investigators to delve deep into network activities, analyze traffic patterns, and uncover potential security breaches. For example, investigators can tap the network to identify data <a id="_idIndexMarker674" class="pcalibre1 pcalibre2 pcalibre"/>exfiltration or use malicious code on a cloud instance. These tools provide the means to scrutinize network data, identify anomalies, trace the origins of threats, and reconstruct the sequence of events leading up to and during a security incident without accessing the host directly in the middle of <span>an incident.</span></p>
<p class="calibre3">Not only do network investigations allow for a deeper dive, but they also provide essential IoCs that investigators can correlate with multiple sources, such as by using the IP address of the threat-actor-controlled remote server for data exfiltration, which can then be tracked within the SIEM tools to determine if there is any other evidence of exfiltration across the environment. Let’s explore the diverse network investigation tools and techniques that <a id="_idIndexMarker675" class="pcalibre1 pcalibre2 pcalibre"/>are integral to modern <span>DFIR practices.</span></p>

<h3 class="calibre11">Arkime</h3>
<p class="calibre3">Arkime, formerly known as Moloch, is a powerful <a id="_idIndexMarker676" class="pcalibre1 pcalibre2 pcalibre"/>open source network investigation tool designed to capture, store, and analyze large volumes of network traffic data. Arkime provides investigators<a id="_idIndexMarker677" class="pcalibre1 pcalibre2 pcalibre"/> and incident responders with a comprehensive platform to investigate and understand network activities, detect anomalies, and uncover potential security threats. With its focus on scalability and flexibility, Arkime<a id="_idIndexMarker678" class="pcalibre1 pcalibre2 pcalibre"/> is particularly suited for analyzing vast network traffic in both on-premises and <span>cloud environments.</span></p>
<p class="calibre3">Arkime offers several key features that make it a valuable tool in <span>network investigation:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Packet capture and storage</strong>: Arkime captures and stores network packets in a scalable and <a id="_idIndexMarker679" class="pcalibre1 pcalibre2 pcalibre"/>efficient manner, allowing for the retention of extensive historical traffic data <span>for analysis.</span></li>
<li class="calibre13"><strong class="bold">Search and analysis</strong>: The tool provides advanced search and filtering capabilities, enabling users to query and analyze network data based on various attributes, such as IP addresses, ports, protocols, and <span>time ranges.</span></li>
<li class="calibre13"><strong class="bold">Session reconstruction</strong>: Arkime can reconstruct network sessions from captured data, providing a holistic view of conversations and interactions between network nodes and understanding the context <span>of communication.</span></li>
<li class="calibre13"><strong class="bold">Metadata extraction</strong>: Arkime extracts metadata from network traffic and packet data, providing a higher-level overview of communications and facilitating efficient <span>data analysis.</span></li>
<li class="calibre13"><strong class="bold">Visualization and reporting</strong>: The tool visualizes network traffic patterns, helping analysts identify trends and anomalies. It also supports customizable reporting for <span>documenting findings.</span></li>
<li class="calibre13"><strong class="bold">Customization and extensibility</strong>: Arkime allows users to develop custom plugins and parsers to accommodate specific network protocols or applications, enhancing flexibility<a id="_idIndexMarker680" class="pcalibre1 pcalibre2 pcalibre"/> <span>and adaptability.</span></li>
<li class="calibre13"><strong class="bold">Integration with other tools</strong>: Arkime can integrate with other network and security tools, enabling seamless information sharing and enhanced <span>analysis capabilities.</span></li>
</ul>
<p class="calibre3">Arkime’s ability to<a id="_idIndexMarker681" class="pcalibre1 pcalibre2 pcalibre"/> handle large-scale data analysis and its focus on aiding network investigations make it an indispensable tool in the DFIR toolkit. By leveraging its features, investigators can effectively detect, respond to, and mitigate network-related threats, ultimately bolstering their organization’s cybersecurity posture. Let’s look at an example of ingesting and analyzing a sample full packet capture from a cloud instance <span>using Arkime.</span></p>

<h4 class="calibre136">Working with Arkime</h4>
<p class="calibre3">Working with Arkime is easy; it has an intuitive <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) that allows you to perform slice-and-dice <a id="_idIndexMarker682" class="pcalibre1 pcalibre2 pcalibre"/>activities on network logs. Investigators have a choice of capturing the live network traffic or uploading an offline PCAP file that was collected from the infected systems. We assume investigators have access to a virtual Arkime environment. Arkime’s UI can be accessed from any browser at the default location – that <span>is, </span><span><strong class="source-inline">http://&lt;Arkime.ipAddress&gt;:8005</strong></span><span>:</span></p>

<ol class="calibre14">
<li class="calibre13">The following figure represents the binary file that’s shipped with Arkime, which is created explicitly for offline PCAP ingestion – <span><strong class="source-inline">capture</strong></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer141"><img alt="Figure 8.9 – Arkime “capture” for processing PCAPs offline" src="../images/00087.jpeg" class="calibre145"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.9 – Arkime “capture” for processing PCAPs offline</p>

<ol class="calibre14">
<li value="2" class="calibre13">Based on the options available, we will use the following command-line parameters to <a id="_idIndexMarker683" class="pcalibre1 pcalibre2 pcalibre"/>upload PCAP files to Arkime for <span>further processing:</span>
<pre class="source-code"><strong class="bold">/opt/arkime/bin/capture -q --copy -r &lt;PCAPFILENAME.PCAP&gt; -t &lt;tagname&gt;</strong></pre></li> <li class="calibre13">Note that investigators must ensure that before uploading the PCAP file, all the associated dependency files are up to date. These include <strong class="source-inline">ipv4-address-space.csv</strong> (representing IPv4 address space<a id="_idIndexMarker684" class="pcalibre1 pcalibre2 pcalibre"/> ranges per <strong class="bold">Regional Internet Registries</strong> (<strong class="bold">RIR</strong>)) and <strong class="bold">manuf</strong> files (list of MAC address allocations per the <strong class="bold">Network Interface Controller</strong> (<strong class="bold">NIC</strong>) manufacturers). MAC addresses are<a id="_idIndexMarker685" class="pcalibre1 pcalibre2 pcalibre"/> unique identifiers that are assigned to NICs for network communication. In incident response, the MAC address helps identify the make of the NIC controller and ultimately determine what computing systems this may have been used, taking it a step closer to the threat <span>actor device.</span></li>
<li class="calibre13">Once uploaded to Arkime, we can jump onto the web browser to perform investigations. The following figure shows a real-life example of a ransomware attack captured on a cloud instance. The organization captured network traffic packets by mirroring their host network traffic and sending them to another host to record traffic. The following figure provides an overview of the UI and general feature sets. We can see activity spikes per session; you can slice the entries per time and perform additional searches. For each entry in the lower half of the page, there is a <strong class="bold">+</strong> option, which allows you to dig deeper into the particular TCP <a id="_idIndexMarker686" class="pcalibre1 pcalibre2 pcalibre"/>session. It also provides an entire conversation of the TCP stream (provided the TCP traffic was in clear text and not over <span>SSL traffic):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer142"><img alt="Figure 8.10 – Arkime main screen – post-PCAP ingestion and processing" src="../images/00128.jpeg" class="calibre146"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.10 – Arkime main screen – post-PCAP ingestion and processing</p>

<ol class="calibre14">
<li value="5" class="calibre13">As shown in the following screenshot, investigators also have the option of parsing the packets, depending on the requirements or nature of the traffic itself. It also provides the opportunity to <span>decode packets:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer143"><img alt="Figure 8.11 – Arkime TCP session details (conversation)" src="../images/00148.jpeg" class="calibre147"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.11 – Arkime TCP session details (conversation)</p>
<p class="calibre3">As we can see from a session’s detailed view, investigators can often view the individual packets that make up the session. This includes information about the packet size, timestamp, and packet <a id="_idIndexMarker687" class="pcalibre1 pcalibre2 pcalibre"/>payload, and this level of detail can be crucial for security analysts investigating network activities. Now, let’s switch to session awareness and filtering capabilities <span>in Arkime.</span></p>

<h4 class="calibre136">Session awareness – filtering</h4>
<p class="calibre3">In this section, we’ll look at the session awareness and filtering capabilities. Arkime has a <strong class="bold">Session and Protocol Information</strong> (<strong class="bold">SPI</strong>) view. This<a id="_idIndexMarker688" class="pcalibre1 pcalibre2 pcalibre"/> view is a critical component of Arkime’s web-based interface and allows users to analyze and dissect network sessions and their associated<a id="_idIndexMarker689" class="pcalibre1 pcalibre2 pcalibre"/> protocols. Here are some of the use cases that investigators can leverage <span>using SPI:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Session list</strong>: The SPI view provides a list of network sessions that have been captured and indexed by Arkime. A session represents a sequence of network packets between a source and destination IP address <span>and port.</span></li>
<li class="calibre13"><strong class="bold">Session detail</strong>: Clicking on a specific session from the list will allow you to view detailed information about that session. This can include source and destination IP addresses, source and destination ports, protocol used, packet capture details, <span>and more.</span></li>
<li class="calibre13"><strong class="bold">Protocol analysis</strong>: Arkime’s SPI view often provides protocol-specific information. This means you can analyze network sessions based on their protocols, including HTTP, DNS, and FTP. This can help in identifying suspicious or <span>malicious activities.</span></li>
<li class="calibre13"><strong class="bold">Filters and searches</strong>: Just like in other views of Arkime, the SPI view allows you to apply filters and search criteria to narrow down the sessions you’re interested in. This can be helpful when you’re dealing with large amounts of captured network data. With the SPI view and list of IP addresses, investigators can export the breakdown<a id="_idIndexMarker690" class="pcalibre1 pcalibre2 pcalibre"/> of the attributes and perform threat intelligence searches on each indicator to obtain more information. Some of the filtering capabilities are <span>as follows:</span><ul class="calibre17"><li class="calibre13"><strong class="bold">Simple filters</strong>: You can filter sessions based on simple attributes such as IP addresses, ports, protocols, and timestamp ranges. For example, you can filter for all sessions involving a specific IP address or sessions that occurred within a specific <span>time frame.</span></li><li class="calibre13"><strong class="bold">Combining filters</strong>: Moloch’s query language allows you to combine multiple conditions using logical operators (AND, OR, NOT) to create more complex filters. This helps you narrow down your focus to <span>specific scenarios.</span></li><li class="calibre13"><strong class="bold">Regular expressions</strong>: You can use regular expressions to match patterns within session attributes. For instance, you might want to filter sessions with URLs containing a <span>specific keyword.</span></li><li class="calibre13"><strong class="bold">Custom fields</strong>: Arkime allows you to define and use custom fields, which can be extracted from session data or added during ingestion. You can then filter sessions based on these <span>custom fields.</span></li><li class="calibre13"><strong class="bold">Saved queries</strong>: Once you’ve constructed a useful filter, you can save it as a named query for easy reuse in <span>the future.</span></li></ul></li>
<li class="calibre13"><strong class="bold">Session graphs</strong>: Depending on the capabilities of the SPI view, you might also have access to visual representations of session data, such as graphs illustrating communication flows <span>between hosts.</span></li>
<li class="calibre13"><strong class="bold">Threat detection</strong>: Security analysts often use Arkime’s SPI view to detect potential threats or anomalies within network traffic. Unusual patterns, unexpected protocols, or suspicious payloads can indicate <span>malicious activity.</span></li>
</ul>
<p class="calibre3">The following screenshot shows an example PCAP file. Here, Arkime will break down the sessions and protocols and provide a deeper dive into each packet. As we can see, the SPI view breaks down destination IPs and other protocols. Each result can be clicked, and further analysis can <span>be</span><span><a id="_idIndexMarker691" class="pcalibre1 pcalibre2 pcalibre"/></span><span> performed:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer144"><img alt="Figure 8.12 – Arkime’s SPI view for session and protocol analysis" src="../images/00107.jpeg" class="calibre148"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.12 – Arkime’s SPI view for session and protocol analysis</p>
<p class="calibre3">To summarize, Arkime is a powerful tool that allows you to break network traffic down for investigators to consume. It also has powerful enrichment capabilities. For example, Arkime can integrate an IP geolocation database to determine the location of origin IPs. Arkime also allows integration with other threat intelligence sources. Another feature is that it provides content extraction, meaning you can download files from Arkime for further <span>manual analysis.</span></p>

<h3 class="calibre11">Wireshark/tcpdump</h3>
<p class="calibre3">tcpdump and Wireshark are powerful tools that are commonly used in DFIR to capture and analyze network traffic. Both tools serve similar purposes but have distinct characteristics and use cases. Often, these<a id="_idIndexMarker692" class="pcalibre1 pcalibre2 pcalibre"/> tools are used to reverse engineer network protocols from a network investigation point <span>of view.</span></p>

<h4 class="calibre136">tcpdump</h4>
<p class="calibre3">tcpdump is a command-line packet analyzer that captures network packets in real time or from a previously captured file. It operates at the lower level of the network stack, allowing it to capture <a id="_idIndexMarker693" class="pcalibre1 pcalibre2 pcalibre"/>packets at a very granular level, including Ethernet frames, IP packets, TCP/UDP segments, and more. It’s often used for network monitoring, troubleshooting, and security analysis. In DFIR, tcpdump is particularly useful for live packet capture during incident response, capturing network traffic for <span>later analysis.</span></p>
<p class="calibre3">From a network investigation point of view, sometimes, investigators do not have full cloud access to set up a packet mirroring service that mirrors all the packets from the infected cloud instance to another instance under the organization’s tenant. Knowledge of these tools certainly helps with preparing and initiating a network capture if access to the configuration elements <span>is limited.</span></p>
<p class="calibre3">Investigators can initiate a full packet dump using the <strong class="source-inline">sudo tcpdump</strong> command (you will need administrator access). <span><em class="italic">Figure 8</em></span><em class="italic">.13</em> shows the sample output of a tcpdump dump without any filters; it displays the <span>following fields:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Current timestamp (local time)</strong>: Displayed <span>in </span><span><strong class="source-inline">HH:MM:SS.microseconds</strong></span><span>.</span></li>
<li class="calibre13"><strong class="bold">Source and destination ports</strong>: <strong class="source-inline">t2.lan.ssh</strong> is the source IP/port sending data to <strong class="source-inline">dfirlab.lan.27018</strong>, and <strong class="source-inline">dfirlab.lan.27018</strong> is the source IP sending data to <strong class="source-inline">t2.lan.ssh</strong> as <span>a response.</span></li>
<li class="calibre13"><strong class="bold">Flags</strong>: <strong class="source-inline">[P.]</strong> indicates that the packet carries application data (PUSH flag set), while <strong class="source-inline">[.]</strong> indicates a regular acknowledgment (ACK <span>flag set).</span></li>
<li class="calibre13"><strong class="bold">Sequence numbers</strong>: <strong class="source-inline">seq 188:440</strong> indicates that the sequence numbers of the data in the packet range from <strong class="source-inline">188</strong> to <strong class="source-inline">440</strong>. This is used to keep track of the order of the data packets. <strong class="source-inline">seq 6000:6256</strong> indicates a subsequent <span>sequence range.</span></li>
<li class="calibre13"><strong class="bold">Acknowledgment</strong> (<strong class="bold">ACK</strong>): <strong class="source-inline">ack 1</strong> indicates the acknowledgment number of the next expected byte. <strong class="source-inline">ack 1148</strong> are acknowledgments of the <span>received data.</span></li>
<li class="calibre13"><strong class="bold">Window size</strong>: <strong class="source-inline">win 4097</strong> and <strong class="source-inline">win 501</strong> indicate the receiver window size, which is the amount of data the receiver <span>can buffer.</span></li>
<li class="calibre13"><strong class="bold">Length</strong>: <strong class="source-inline">length 252</strong> indicates the packet’s data <span>payload length.</span></li>
</ul>
<p class="calibre3">Note that you will have <a id="_idIndexMarker694" class="pcalibre1 pcalibre2 pcalibre"/>to explicitly indicate a filename to capture the packets to a file on the disk in a PCAP format for <span>offline analysis:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer145"><img alt="Figure 8.13 – tcpdump in action" src="../images/00038.jpeg" class="calibre149"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.13 – tcpdump in action</p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">TCP dump filters</strong>: Investigators should familiarize themselves with tcpdump filters. When applied during capture, these filters will only collect network packets that match the conditions specified in the filter. In the case of tcpdump, it uses <strong class="bold">Berkeley Packet Filter</strong> (<strong class="bold">BPF</strong>), which <a id="_idIndexMarker695" class="pcalibre1 pcalibre2 pcalibre"/>allows for low-level filtering and specifies requirements for capturing <span>particular packets.</span>
<p class="calibre3">The following figure shows an example of using a BPF filter for capturing specific network packets – that is, <strong class="source-inline">sudo tcpdump -i &lt;interface_name&gt; proto 17</strong>. In this example, we filter packets based on protocol number 17, which refers to <span>UDP traffic:</span></p>
</li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer146"><img alt="Figure 8.14 – tcpdump with BPF filter" src="../images/00073.jpeg" class="calibre150"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.14 – tcpdump with BPF filter</p>
<p class="calibre3">Let’s jump over to a more intuitive version of packet capture software – Wireshark. It has very similar<a id="_idIndexMarker696" class="pcalibre1 pcalibre2 pcalibre"/> functionality; however, you can do real-time analysis as packets <span>are captured.</span></p>

<h4 class="calibre136">Wireshark</h4>
<p class="calibre3">Wireshark, on the other hand, is a user-friendly graphical network protocol analyzer. It provides a detailed, visually<a id="_idIndexMarker697" class="pcalibre1 pcalibre2 pcalibre"/> rich interface for examining captured packets. Wireshark can open and analyze tcpdump capture files and packets directly from network interfaces. It operates at a higher level of abstraction than tcpdump, presenting dissected and decoded network protocols in a more human-readable format. Investigators widely use Wireshark to analyze network traffic for evidence of malicious activity, network anomalies, and data exfiltration. The following figure shows a simplistic view of the Wireshark <span>user interface:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer147"><img alt="Figure 8.15 – Wireshark user interface" src="../images/00093.jpeg" class="calibre151"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.15 – Wireshark user interface</p>
<p class="calibre3">Wireshark offers many <a id="_idIndexMarker698" class="pcalibre1 pcalibre2 pcalibre"/>capabilities to filter packets and parse them through their UI and has clearly defined information on how to handle and filter packet information. Some of the cheat sheets have been referenced in the <em class="italic">Further reading</em> section of <span>this chapter.</span></p>

<h3 class="calibre11">CyberChef</h3>
<p class="calibre3">CyberChef is a powerful web-based tool for various data manipulation tasks that’s often used in cybersecurity and digital forensics. It provides a user-friendly interface and recipes for encoding, decoding, transforming, and <a id="_idIndexMarker699" class="pcalibre1 pcalibre2 pcalibre"/>analyzing data in various formats. CyberChef can be incredibly useful for processing and analyzing network-related<a id="_idIndexMarker700" class="pcalibre1 pcalibre2 pcalibre"/> data in the context of network investigation. Here’s how CyberChef can be used in <span>network investigation:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Data transformation</strong>: Network investigators often encounter encoded or obfuscated data in network traffic. CyberChef provides various data transformation operations, such as base64 encoding/decoding, URL encoding/decoding, XOR operations, and more. These transformations can help investigators unveil hidden information in <span>network packets.</span></li>
<li class="calibre13"><strong class="bold">Data extraction</strong>: When examining network traffic, extracting relevant information from raw data is essential. CyberChef’s <strong class="source-inline">grep</strong> function can search for specific patterns, headers, or <a id="_idIndexMarker701" class="pcalibre1 pcalibre2 pcalibre"/>keywords within the traffic. This can help with identifying crucial details such as IP addresses, domain names, or <span>file paths.</span></li>
<li class="calibre13"><strong class="bold">Hashing and encryption</strong>: CyberChef supports various hashing and encryption/decryption techniques. Investigators can hash strings or files to check for matches with known malicious hashes. Additionally, they can attempt to decrypt encrypted data if they can access the <span>necessary keys.</span></li>
<li class="calibre13"><strong class="bold">Parsing and decoding protocols</strong>: CyberChef’s capabilities can be extended using custom recipes. Investigators can create recipes that parse and decode network protocols such as HTTP, DNS, and SMTP. This can help in extracting meaningful information from protocol headers <span>and payloads.</span></li>
<li class="calibre13"><strong class="bold">File carving</strong>: CyberChef can be used to retrieve and reconstruct files from the captured data if network traffic contains file transfers. This is particularly useful when investigating potential data exfiltration or <span>malware distribution.</span></li>
<li class="calibre13"><strong class="bold">Data visualization</strong>: CyberChef can help visualize data transformations and conversions, making it easier to understand how data changes during its journey across the network. Visualizing data can assist in identifying anomalies or <span>suspicious patterns.</span></li>
<li class="calibre13"><strong class="bold">Automating workflows</strong>: CyberChef allows you to create and save recipes, enabling the automation of repetitive data manipulation tasks. This can significantly speed up the investigation process and ensure consistency in <span>data processing.</span></li>
<li class="calibre13"><strong class="bold">Collaboration and sharing</strong>: CyberChef recipes can be shared among investigators, allowing for collaboration and knowledge sharing within the investigative team. This can help less experienced team members benefit from the expertise <span>of others.</span></li>
<li class="calibre13"><strong class="bold">Quick analysis</strong>: For quick <a id="_idIndexMarker702" class="pcalibre1 pcalibre2 pcalibre"/>analysis of small pieces of data, CyberChef’s easy-to-use interface can provide immediate insights without the need for complex scripting <span>or coding.</span></li>
</ul>
<p class="calibre3">The following figure demonstrates an example of CyberChef in use. As indicated earlier, it comes with predefined recipes (algorithms) to parse a given dataset. It is a helpful tool for network investigators and digital forensic investigators. In this example, we’re taking a log export from AWS and parsing it in CyberChef. The log export was in JSON format; reading through it can be tricky if the export needs to be formatted correctly. Once the recipe has been defined, investigators can drag and drop the artifacts and review <span>the output:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer148"><img alt="Figure 8.16 – CyberChef in use" src="../images/00055.jpeg" class="calibre152"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.16 – CyberChef in use</p>
<p class="calibre3">As we can see, CyberChef is an enabler that extends an investigator’s capabilities and creativity in solving an investigative challenge. Predefined recipes come with CyberChef, while other investigators have published a combination of recipes that can be applied to address a challenging forensic situation. We have included the cheat sheet in the <em class="italic">Further reading</em> section <a id="_idIndexMarker703" class="pcalibre1 pcalibre2 pcalibre"/>of this chapter <span>for reference.</span></p>
<p class="calibre3">In the next section, we will switch gears and look at malware investigations. While a whole book could be written about this topic, the next section aims to provide a quick refresher on malware investigation concepts and how to explore some artifacts once you have captured them through host or network forensic investigation. We will look into setting up your lab environment and some essential tools that can be useful during <span>an investigation.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-155" class="calibre5"><a id="_idTextAnchor167" class="pcalibre1 pcalibre2 pcalibre"/>Malware investigations</h1>
<p class="calibre3">Malware investigations are a critical component of any incident response and involve the systematic process of identifying, analyzing, and responding to malicious software in an environment. The<a id="_idIndexMarker704" class="pcalibre1 pcalibre2 pcalibre"/> primary objective of a malware investigation is to understand the nature of the malware, its potential impact, and the extent of its infiltration. This information is essential for making informed decisions about containment, eradication, <span>and recovery.</span></p>
<p class="calibre3">Malware analysts conduct in-depth analysis, dissecting the malware’s code and behavior to reveal its capabilities, communication methods, and potential vulnerabilities that can be exploited. This information guides the understanding of the extent of compromise and aids in formulating appropriate countermeasures. An impact assessment evaluates the damage inflicted by the malware, including compromised data and affected systems, to prioritize <span>response actions.</span></p>
<p class="calibre3">Eradication follows, which involves removing the malware using tailored antivirus signatures, removing files, and patching vulnerabilities. Post-eradication, recovery efforts commence, accompanied by an assessment of the incident response process and the integration of lessons learned into future strategies. For instances requiring legal action or a more profound understanding, forensic analysis preserves evidence, documents attack details, and facilitates potential collaboration with law enforcement. This comprehensive process ensures a thorough and effective response to malware incidents, safeguarding organizational integrity <span>and security.</span></p>
<p class="calibre3">In the context of cloud environments, malware analysis introduces unique challenges and considerations due to the distributed and dynamic nature of cloud computing. Cloud-based malware analysis examines malicious software within virtualized or containerized environments hosted on <span>cloud platforms.</span></p>
<p class="calibre3">As we can see, malware analysis closely follows the incident response process, providing critical indicators that are vital<a id="_idIndexMarker705" class="pcalibre1 pcalibre2 pcalibre"/> for containing and <span>eradicating threats.</span></p>
<p class="calibre3">However, to perform malware analysis, you need a specialized set of tools and a completely isolated setup so that investigators do not detonate the malware on their devices and compromise the working papers of investigative information. For this reason, malware analysis is always performed in a <span>lab environment.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-156" class="calibre10"><a id="_idTextAnchor168" class="pcalibre1 pcalibre2 pcalibre"/>Setting up your malware analysis lab</h2>
<p class="calibre3">In this section, we will look at the basic architecture of setting up a malware analysis lab. For an on-premises lab, malware <a id="_idIndexMarker706" class="pcalibre1 pcalibre2 pcalibre"/>analysts typically use virtual infrastructure so that it is easier to tear it down and bring it back up as a fresh setup. Once malware is detonated in the lab, it leaves traces or artifacts that must be<a id="_idIndexMarker707" class="pcalibre1 pcalibre2 pcalibre"/> carefully studied. Similarly, investigators can set up a malware lab in the cloud. However, if you are detonating malware in the cloud, care must be taken that it does not compromise any other tenants and ensure that the malware lab is <span>locked down.</span></p>
<p class="calibre3">The following figure illustrates an example of setting up a malware analysis lab and some key components. As indicated earlier, investigators must take utmost care while handling malware and associated artifacts. They are live and can also infect or compromise the investigator’s computers. For this reason, most of the investigators will operate the malware in a lab. This is similar to handling an explosive and ensuring it does not cause damage. In the following figure, every connection is secured by a firewall and does not allow direct access to the detonation VM. This is where reverse engineers can explore malware and understand <span>its capabilities:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer149"><img alt="Figure 8.17 – Malware analysis lab architecture" src="../images/00129.jpeg" class="calibre153"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.17 – Malware analysis lab architecture</p>
<p class="calibre3">Some of the critical components of the<a id="_idIndexMarker708" class="pcalibre1 pcalibre2 pcalibre"/> lab are <span>as follows:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Firewalls</strong>: Any firewall, physical or virtual, should be good. All the network connections to the malware-hosting VMs are secured and filtered. In our example, we use pfSense, a<a id="_idIndexMarker709" class="pcalibre1 pcalibre2 pcalibre"/> popular firewall appliance also available for virtual deployments. You can deploy a similar one if you plan to set up a lab in the cloud. In this example, the firewall and its subnet ensure that the malware lab is only accessible via specific port forwarding and not widely accessible on the <span>internal network.</span></li>
<li class="calibre13"><strong class="bold">Malware repository</strong>: A dedicated application with a database where malware can securely reside. We use a Python-based Viper framework application, which allows file and static malware analysis. It also has a database that stores malware and helps organize malware and <span>exploit samples.</span></li>
<li class="calibre13"><strong class="bold">Detonation VMs</strong>: These are dedicated VMs hosted on a separate and isolated subnet for detonating malware or enabling reverse engineers to explore the internal workings of the malware. It is important to note that investigators must securely configure the malware VMs, and all the network connections are carefully allowed. For example, for detonating malware, if it needs to connect to the internet, care must be taken when allowing internet access to the malware. Detonation VMs can <a id="_idIndexMarker710" class="pcalibre1 pcalibre2 pcalibre"/>come pre-built; you can use any freely available open source-based malware analysis VMs, such as Linux-based <strong class="bold">Remnux</strong> or <strong class="bold">SANS SIFT workstation</strong>, or you can build one based on your tools. You can also build on Windows, such as <strong class="bold">Flare VM</strong> from Mandiant, which offers scripts to set up the necessary software and harden the VM. You can also download a limited-time version of Windows 10 or Windows 11 directly from Microsoft at no cost; this provides a 90-day license and enables you to reinstall after the license’s expiry date. This ensures that your Windows and malware analysis toolkits are up <span>to date.</span></li>
<li class="calibre13"><strong class="bold">Apache Guacamole</strong>: Apache Guacamole is an open source remote desktop gateway that provides web-based access to remote desktops and applications. It allows users to access their desktop environments and applications through a web browser session without requiring additional software installation on the client side. Guacamole supports various remote desktop protocols such as VNC, RDP, and SSH. The most crucial benefit of utilizing Apache Guacamole is enforcing additional controls, such as restrictions on copy-paste and limitations on the accessibility of network drives and printers. This is crucial as you do not want malware to spread or jump out of the VM’s secure <span>network zone.</span></li>
<li class="calibre13"><strong class="bold">Malware Analyst VM</strong>: This is purely for securely accessing the malware network via Apache Guacamole over a web browser session. When accessing the malware lab, it is always recommended that a dedicated workstation is used to access this lab environment and for no <span>other purposes.</span></li>
</ul>
<p class="calibre3">Investigators can also choose to deploy similar architecture in the cloud by utilizing free and open source tools for malware analysis. Various renditions of malware analysis architecture can also be<a id="_idIndexMarker711" class="pcalibre1 pcalibre2 pcalibre"/> deployed if there is a requirement to set up a more oversized malware analysis workbench. One example is Japan’s JPCERT group, which developed an architecture to deploy a<a id="_idIndexMarker712" class="pcalibre1 pcalibre2 pcalibre"/> lab in the cloud for memory forensics and analysis. A link to JPCERT’s memory forensic lab is available in the <em class="italic">Further </em><span><em class="italic">reading</em></span><span> section.</span></p>
<p class="calibre3">Let’s dive deeper into some common malware traits that investigators often end up encountering with packed malware and multiple versions of the <span>same malware.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-157" class="calibre10"><a id="_idTextAnchor169" class="pcalibre1 pcalibre2 pcalibre"/>Working with packed malware</h2>
<p class="calibre3">One of the top challenges that <a id="_idIndexMarker713" class="pcalibre1 pcalibre2 pcalibre"/>malware reverse engineers encounter is packed malware. Packers are tools that enterprises, as well as threat actors, use to compress and encrypt any code. In the context of threats, malware <a id="_idIndexMarker714" class="pcalibre1 pcalibre2 pcalibre"/>code is packed and deployed in an environment that makes it harder for reverse engineers to analyze. Note that we indicated <em class="italic">harder</em> and not <em class="italic">impossible</em>. Threat actors aim to make it harder to detect and research so that investigators cannot create detection packages before the threat severely impacts organizations. These packers help obfuscate the code, making it challenging to understand malware functionality. Historically, some of the packers are commonly observed by investigators as preferred choices by <span>threat actors:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Ultimate Packer for eXecutables</strong> (<strong class="bold">UPX</strong>): This is a very popular and well-known tool for packing<a id="_idIndexMarker715" class="pcalibre1 pcalibre2 pcalibre"/> executable files. UPX is an open source tool that can protect executable packages. Unpacking a UPX-packed executable is relatively easy as well. Any commonly used <strong class="bold">Portable Executable</strong> (<strong class="bold">PE</strong>) file analyzer<a id="_idIndexMarker716" class="pcalibre1 pcalibre2 pcalibre"/> would provide results if a given executable is packed. The following figure shows a side-by-side example of real-world malware caught in the wild; the one on the left is UPX-packed, while the one on the right is unpacked or uncompressed. Note the size of each of the sections. Remember that UPX compresses the malware binary; therefore, the size and memory address locations of the unpacked binary differ when unpacked. While this is a simplistic version, sophisticated threat actors can make it more complex by applying multiple packing levels or numerous forms <span>of packers:</span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer150"><img alt="Figure 8.18 – UPX packed versus unpacked malware" src="../images/00004.jpeg" class="calibre154"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.18 – UPX packed versus unpacked malware</p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Armadillo</strong>: Armadillo is a commercial packer and employs code encryption, virtualization, anti-debugging<a id="_idIndexMarker717" class="pcalibre1 pcalibre2 pcalibre"/> measures, and dynamic unpacking to shield executable code from reverse engineering and analysis. Although intended for legitimate purposes, threat actors have exploited its features to obfuscate malware, making<a id="_idIndexMarker718" class="pcalibre1 pcalibre2 pcalibre"/> research and detection more challenging for <span>security researchers.</span></li>
<li class="calibre13"><strong class="bold">Themida</strong>: Themida is a commercial software protection tool that’s commonly utilized by software developers to safeguard their applications against reverse engineering and unauthorized access. Like Armadillo, it employs sophisticated techniques such as code obfuscation, encryption, and anti-debugging measures to make it difficult for researchers to decipher the packed code’s functionality. Additionally, Themida offers anti-tampering mechanisms and virtualization to fortify the protection further. While initially intended for legitimate purposes, we have seen threat actors repurposing Themida to cloak malware and complicate detection by <span>security solutions.</span></li>
</ul>
<p class="calibre3">While dealing with malware packers can be tricky, investigators must remember that whatever is executed in memory, at some point, the packed code must be unpacked and decoded before running in memory. Irrespective of the choice of packers used by threat actors, reverse engineers may choose a difficult road by manually debugging the packed malware to reveal unpacked code under it and adjust other associated parameters so that further analysis of the malware <span>is possible.</span></p>
<p class="calibre3">Another challenging situation<a id="_idIndexMarker719" class="pcalibre1 pcalibre2 pcalibre"/> that investigators often encounter is multiple versions of the same binary. This raises concerns about the number of versions the threat actor may have deployed to evade <a id="_idIndexMarker720" class="pcalibre1 pcalibre2 pcalibre"/>detection; dealing with various versions of the same binary adds additional complexities to <span>the investigation.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h2 id="_idParaDest-158" class="calibre10"><a id="_idTextAnchor170" class="pcalibre1 pcalibre2 pcalibre"/>Binary comparison</h2>
<p class="calibre3">Binary comparison is a process that’s used in computer science and software engineering to determine the differences between two binary files, which are files containing compiled code or <a id="_idIndexMarker721" class="pcalibre1 pcalibre2 pcalibre"/>machine-readable data. This comparison involves analyzing the individual bytes <a id="_idIndexMarker722" class="pcalibre1 pcalibre2 pcalibre"/>or bits of the files to identify variations in content, structure, and code sequences or by evaluating the <strong class="bold">Control Flow Graph</strong> (<strong class="bold">CFG</strong>). Binary comparison is commonly employed in tasks such as version<a id="_idIndexMarker723" class="pcalibre1 pcalibre2 pcalibre"/> control, software debugging, malware analysis, and digital forensics. By identifying differences between binary files, developers, analysts, and researchers can understand code changes, track modifications, detect tampering, and uncover potential changes to the malware capabilities. The following screenshot shows an example of a Windows program represented in CFG format that demonstrates various decision trees, jump points, network access, and complexities of a program. We’re using a free, open source tool known as <strong class="bold">ProcDot</strong> for this purpose. The<a id="_idIndexMarker724" class="pcalibre1 pcalibre2 pcalibre"/> following screenshot is just for illustration to demonstrate the <span>code complexities:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer151"><img alt="Figure 8.19 – Compressed view of an application CFG with read/write paths" src="../images/00137.jpeg" class="calibre155"/></div>
</div>
<p class="img---caption" lang="en-US">Figure 8.19 – Compressed view of an application CFG with read/write paths</p>
<p class="calibre3">CFG is a graphical representation that’s used in computer science and software engineering to visualize control flow within a program or software application. It illustrates the various paths that a program’s execution can take by depicting its basic blocks (individual <a id="_idIndexMarker725" class="pcalibre1 pcalibre2 pcalibre"/>segments of code with a single entry point and a single exit point) and the connections between them. Each basic block typically corresponds to a sequence of instructions executed sequentially without <a id="_idIndexMarker726" class="pcalibre1 pcalibre2 pcalibre"/>branching. CFGs use nodes to represent basic blocks and edges to represent the flow of control between them, indicating where the program can branch or jump to different sections of code based on conditions or loop constructs. CFGs are particularly useful for understanding program behavior, analyzing code paths, and detecting potential bugs or vulnerabilities, and especially useful when comparing similar versions of the executable files. They are commonly used in program analysis, optimization, debugging, and security research. Let’s look at some of the specialized tools that are used to <span>compare binaries.</span></p>

<h3 class="calibre11">BinDiff</h3>
<p class="calibre3"><strong class="bold">BinDiff</strong> is a software comparison tool developed by <strong class="bold">Zynamics</strong>, a company acquired by Google. BinDiff is primarily used for analyzing and comparing binary files, such as executables, libraries, and other compiled code. It’s widely employed in reverse engineering and malware analysis to identify similarities and differences between software versions or find common <a id="_idIndexMarker727" class="pcalibre1 pcalibre2 pcalibre"/>code patterns among <span>different binaries.</span></p>
<p class="calibre3">To summarize, while we reflected upon some of the basic tools that are out there for malware investigations, researchers develop various tools that investigators must be prepared to stay up to date with so that they can leverage them when they notice complex malware that requires specialized tools. Knowing that there is a tool out there is more important than the knowledge of the tool itself. Given threat actors create malware differently all the time to make it difficult for us, it’s important to stay on top of the technologies, techniques, and concepts utilized by malware developers. In the next section, we will compare traditional and cloud forensics, debunking a <span>few myths.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-159" class="calibre5"><a id="_idTextAnchor171" class="pcalibre1 pcalibre2 pcalibre"/>Traditional forensics versus cloud forensics</h1>
<p class="calibre3">Traditional and cloud forensics play critical roles in incident response but differ in focus and methodologies due to the<a id="_idIndexMarker728" class="pcalibre1 pcalibre2 pcalibre"/> distinct environments <span>they address.</span></p>
<p class="calibre3">Here are <span>their similarities:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Evidence collection</strong>: Both traditional and cloud forensics involve collecting and preserving digital evidence to <a id="_idIndexMarker729" class="pcalibre1 pcalibre2 pcalibre"/>reconstruct events leading to an incident. This may include collecting memory dumps, log files, network traffic, and filesystem artifacts. Investigators often use cloud storage to store large volumes of artifacts, irrespective of the underlying CSP, as most breaches affect a cloud tenant at a CSP. In scenarios where the underlying CSP is believed to be compromised, it is recommended that investigators save all the necessary artifacts in a different CSP storage or offline <span>for analysis.</span></li>
<li class="calibre13"><strong class="bold">Analysis techniques</strong>: Both domains employ similar techniques for analyzing digital evidence, such as examining file structures, metadata, timestamps, and memory contents to understand the timeline and scope of <span>an incident.</span></li>
<li class="calibre13"><strong class="bold">Chain of custody</strong>: Maintaining the chain of custody is crucial in both scenarios to ensure the integrity<a id="_idIndexMarker730" class="pcalibre1 pcalibre2 pcalibre"/> and admissibility of evidence in <span>legal proceedings.</span></li>
</ul>
<p class="calibre3">Here are <span>their differences:</span></p>

<ul class="calibre12">
<li class="calibre13"><strong class="bold">Environment</strong>: Traditional <a id="_idIndexMarker731" class="pcalibre1 pcalibre2 pcalibre"/>forensics involves physical devices such as computers, servers, and mobile devices. In contrast, cloud forensics focuses on virtualized and distributed environments, including <strong class="bold">Infrastructure-as-a-Service</strong> (<strong class="bold">IaaS</strong>), <strong class="bold">Platform-as-a-Service</strong> (<strong class="bold">PaaS</strong>), and <span><strong class="bold">Software-as-a-Service</strong></span><span> (</span><span><strong class="bold">SaaS</strong></span><span>).</span></li>
<li class="calibre13"><strong class="bold">Evidence location</strong>: In traditional forensics, evidence is often stored locally on physical devices. In cloud forensics, evidence might be distributed across multiple virtual instances and storage services, requiring different collection and <span>preservation techniques.</span></li>
<li class="calibre13"><strong class="bold">Data ownership</strong>: Data ownership and control can be complex due to shared resources in the cloud. Identifying which party is responsible for maintaining and providing access to evidence can be <span>more challenging.</span></li>
<li class="calibre13"><strong class="bold">Data residency</strong>: Data might be stored in various geographic locations in the cloud, potentially affecting legal and regulatory considerations in <span>different jurisdictions.</span></li>
<li class="calibre13"><strong class="bold">Network traffic</strong>: In traditional forensics, capturing network traffic is relatively straightforward. Network traffic might be harder to access in cloud environments due to virtualized networks and <span>third-party services.</span></li>
<li class="calibre13"><strong class="bold">Logs and auditing</strong>: Cloud environments often offer extensive logging and auditing capabilities, providing more detailed activity information. However, accessing and interpreting these logs can <span>be complex.</span></li>
<li class="calibre13"><strong class="bold">Resource sharing</strong>: In cloud environments, multiple tenants might share the same physical hardware, impacting the isolation and preservation <span>of evidence.</span></li>
<li class="calibre13"><strong class="bold">Incident response tools</strong>: Traditional incident response tools may not fully translate to cloud environments due to<a id="_idIndexMarker732" class="pcalibre1 pcalibre2 pcalibre"/> architectural differences and the need for specialized tools tailored to <span>cloud </span><span><a id="_idIndexMarker733" class="pcalibre1 pcalibre2 pcalibre"/></span><span>forensics.</span></li>
<li class="calibre13"><strong class="bold">Legal and compliance</strong>: Cloud forensics can involve additional legal and compliance challenges due to<a id="_idIndexMarker734" class="pcalibre1 pcalibre2 pcalibre"/> jurisdictional issues, cross-border data transfers, and varying cloud service <span>provider terms.</span></li>
</ul>
<p class="calibre3">In summary, while traditional and cloud forensics share common principles regarding evidence collection and analysis, cloud forensics introduces complexities related to cloud environments’ virtualized and distributed nature (such as containerization and serverless architectures), shared resources, and legal considerations specific to the cloud. Incident responders and digital forensics experts must adapt their practices to effectively handle incidents in traditional and <span>cloud-based systems.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-160" class="calibre5"><a id="_idTextAnchor172" class="pcalibre1 pcalibre2 pcalibre"/>Summary</h1>
<p class="calibre3">As we’ve seen through this chapter, the basics of the incident response process and threat hunting largely remain the same, focusing on finding evil within the environment. Depending on the operating systems, investigators can customize what logs and artifacts should be collected and what must be investigated. We also saw how EDR deployments speed up the breach containment and incident response process. Remember, the incident response process is a discipline that investigators closely follow to ensure that all the investigative steps are performed. At the same time, the breach is contained, and there is no further risk to the organization and the endpoint under investigation. While this chapter aimed to introduce various elements of the breach investigation, it is undoubtedly encouraged that investigators stay up to date with the latest investigative tools <span>and techniques.</span></p>
<p class="calibre3">In the next chapter, we will look at collecting these artifacts from the cloud environment. Recognizing that collecting necessary artifacts from the cloud environment is difficult, we will look at various cloud service providers and their support for collecting forensic packages. We will also look at the options for exporting full disk images versus quick forensic collections in <span>the cloud.</span></p>

</div>
</div>


<div id="sbo-rt-content" class="calibre1">
<div id="_idContainer152" class="calibre2">
<h1 id="_idParaDest-161" class="calibre5"><a id="_idTextAnchor173" class="pcalibre1 pcalibre2 pcalibre"/>Further reading</h1>
<ul class="calibre12">
<li class="calibre13"><em class="italic">PE </em><span><em class="italic">block</em></span><span>: </span><a href="https://archive.org/details/windowsntfilesys00naga/page/129" class="pcalibre1 pcalibre2 pcalibre"><span>https://archive.org/details/windowsntfilesys00naga/page/129</span></a></li>
<li class="calibre13"><em class="italic">Overview of memory dump file options for </em><span><em class="italic">Windows</em></span><span>: </span><a href="https://learn.microsoft.com/en-us/troubleshoot/windows-server/performance/memory-dump-file-options" class="pcalibre1 pcalibre2 pcalibre"><span>https://learn.microsoft.com/en-us/troubleshoot/windows-server/performance/memory-dump-file-options</span></a></li>
<li class="calibre13"><em class="italic">Varieties of Kernel-Mode Dump Files</em>: <a href="https://blog.cloudflare.com/bpf-the-forgotten-bytecode/" class="pcalibre1 pcalibre2 pcalibre">Varieties of Kernel-Mode Dump Files - Windows drivers</a> | <a href="https://andreaskaris.github.io/blog/networking/bpf-and-tcpdump/" class="pcalibre1 pcalibre2 pcalibre"><span>Microsoft Learn</span></a></li>
<li class="calibre13"><em class="italic">Awesome Threat </em><span><em class="italic">Hunting</em></span><span>: </span><a href="https://github.com/0x4D31/awesome-threat-detection/tree/master" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/0x4D31/awesome-threat-detection/tree/master</span></a></li>
<li class="calibre13"><em class="italic">BPF – the forgotten </em><span><em class="italic">bytecode</em></span><span>: </span><a href="https://blog.cloudflare.com/bpf-the-forgotten-bytecode/" class="pcalibre1 pcalibre2 pcalibre"><span>https://blog.cloudflare.com/bpf-the-forgotten-bytecode/</span></a></li>
<li class="calibre13"><em class="italic">BPF and </em><span><em class="italic">tcpdump</em></span><span>: </span><a href="https://andreaskaris.github.io/blog/networking/bpf-and-tcpdump/" class="pcalibre1 pcalibre2 pcalibre"><span>https://andreaskaris.github.io/blog/networking/bpf-and-tcpdump/</span></a></li>
<li class="calibre13"><em class="italic">Transmission Control </em><span><em class="italic">Protocol</em></span><span>: </span><a href="https://datatracker.ietf.org/doc/html/rfc793" class="pcalibre1 pcalibre2 pcalibre"><span>https://datatracker.ietf.org/doc/html/rfc793</span></a></li>
<li class="calibre13"><em class="italic">Wireshark Cheat </em><span><em class="italic">Sheet</em></span><span>: </span><a href="https://cdn.comparitech.com/wp-content/uploads/2019/06/Wireshark-Cheat-Sheet-1.jpg.webp" class="pcalibre1 pcalibre2 pcalibre"><span>https://cdn.comparitech.com/wp-content/uploads/2019/06/Wireshark-Cheat-Sheet-1.jpg.webp</span></a></li>
<li class="calibre13"><span><em class="italic">CyberChef</em></span><span>: </span><a href="https://gchq.github.io/CyberChef/" class="pcalibre1 pcalibre2 pcalibre"><span>https://gchq.github.io/CyberChef/</span></a></li>
<li class="calibre13"><em class="italic">CyberChef </em><span><em class="italic">Recipes</em></span><span>: </span><a href="https://github.com/mattnotmax/cyberchef-recipes" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/mattnotmax/cyberchef-recipes</span></a></li>
<li class="calibre13"><em class="italic">Flare </em><span><em class="italic">VM</em></span><span>: </span><a href="https://github.com/mandiant/flare-vm" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/mandiant/flare-vm</span></a></li>
<li class="calibre13"><em class="italic">REMnux: A Linux Toolkit for Malware </em><span><em class="italic">Analysis</em></span><span>: </span><a href="https://remnux.org/" class="pcalibre1 pcalibre2 pcalibre"><span>https://remnux.org/</span></a></li>
<li class="calibre13"><em class="italic">SIFT </em><span><em class="italic">Workstation</em></span><span>: </span><a href="https://www.sans.org/tools/sift-workstation/" class="pcalibre1 pcalibre2 pcalibre"><span>https://www.sans.org/tools/sift-workstation/</span></a></li>
<li class="calibre13"><em class="italic">JPCERT Coordination Center-Memory </em><span><em class="italic">Forensics</em></span><span>: </span><a href="https://github.com/JPCERTCC/MemoryForensic-on-Cloud" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/JPCERTCC/MemoryForensic-on-Cloud</span></a></li>
<li class="calibre13"><em class="italic">Building a Custom Malware Analysis Lab </em><span><em class="italic">Environment</em></span><span>:</span> <a href="https://www.sentinelone.com/labs/building-a-custom-malware-analysis-lab-environment/" class="pcalibre1 pcalibre2 pcalibre"><span>https://www.sentinelone.com/labs/building-a-custom-malware-analysis-lab-environment/</span></a></li>
<li class="calibre13"><em class="italic">Get a Windows 11 development </em><span><em class="italic">environment</em></span><span>: </span><a href="https://developer.microsoft.com/en-us/windows/downloads/virtual-machines/" class="pcalibre1 pcalibre2 pcalibre"><span>https://developer.microsoft.com/en-us/windows/downloads/virtual-machines/</span></a></li>
<li class="calibre13"><em class="italic">Leveraging Microsoft Graph API for memory </em><span><em class="italic">forensics</em></span><span>: </span><a href="https://medium.com/comae/leveraging-microsoft-graph-api-for-memory-forensics-7ab7f9ea4d06" class="pcalibre1 pcalibre2 pcalibre"><span>https://medium.com/comae/leveraging-microsoft-graph-api-for-memory-forensics-7ab7f9ea4d06</span></a></li>
<li class="calibre13"><em class="italic">Digital Forensic – Most Commonly used </em><span><em class="italic">Tools</em></span><span>: </span><a href="mailto:https://medium.com/@KhalilAfridii/digital-forensic-most-commonly-used-tools-4a9dbb98f926" class="pcalibre1 pcalibre2 pcalibre"><span>https://medium.com/@KhalilAfridii/digital-forensic-most-commonly-used-tools-4a9dbb98f926</span></a></li>
<li class="calibre13"><span><em class="italic">CyLR</em></span><span>: </span><a href="https://github.com/orlikoski/CyLR" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/orlikoski/CyLR</span></a></li>
<li class="calibre13"><span><em class="italic">Kansa</em></span><span>: </span><a href="https://github.com/davehull/Kansa" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/davehull/Kansa</span></a></li>
<li class="calibre13"><span><em class="italic">PSHunt</em></span><span>: </span><a href="https://github.com/Infocyte/PSHunt" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/Infocyte/PSHunt</span></a></li>
<li class="calibre13"><span><em class="italic">PowerForensics</em></span><span>: </span><a href="https://github.com/Invoke-IR/PowerForensics" class="pcalibre1 pcalibre2 pcalibre"><span>https://github.com/Invoke-IR/PowerForensics</span></a></li>
</ul></div>
</div>
</body></html>