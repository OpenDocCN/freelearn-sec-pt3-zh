<html><head></head><body>
		<div><h1 id="_idParaDest-129" class="chapter-number"><a id="_idTextAnchor131"/>8</h1>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor132"/>Data Exposure and Sensitive Information Leakage</h1>
			<p>This chapter starts the fourth part of our book, which is about advanced API techniques. We will better understand the inherent problems of data exposure and sensitive information leakage that unpatched or badly configured API endpoints can suffer. We will tackle the nuances of how this can happen and ways of taking this in our favor as API pentesters.</p>
			<p>Either by digesting some data masses or by taking a ride on previous pentesting findings, we will learn how data or sensitive information can be detected among other garbage or less valuable assets. This can save you time not only when conducting a pentest but also when planning to hit the final target of a coordinated attack. Some testers establish the scope of their work on exfiltrating some data from the endpoint, whereas others work to get it down (by abusing their network, for example). You will learn the techniques and understand how such problems can be avoided when configuring or building an API.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Identifying sensitive data exposure</li>
				<li>Testing for information leakage</li>
				<li>Preventing data leakage</li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor133"/>Technical requirements</h1>
			<p>As we did in previous chapters, we’ll leverage the same environment as the one pointed out in previous chapters, such as an Ubuntu distro. Some other new relevant utilities will be mentioned in the corresponding sections.</p>
			<p>We will be especially occupied with handling vast amounts of data in this chapter. Hence, we will count on some data mining and curation tools that will do the hard work for us when analyzing huge-sized logs or other types of big data.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor134"/>Identifying sensitive data exposure</h1>
			<p>Identifying sensitive<a id="_idIndexMarker597"/> data exposure in APIs is a critical step in securing them. Regardless of their size, data breaches can cause severe and often irreparable damage to companies’ reputations. Hence, fully comprehending potential vulnerabilities on the API endpoints you own is paramount. The first step is defining what constitutes sensitive data. This goes beyond just <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) such as names and addresses. Here’s a breakdown of different types of sensitive data and how APIs might expose them:</p>
			<ul>
				<li><strong class="bold">PII</strong>: This corresponds to all <a id="_idIndexMarker598"/>kinds of data or information that can be used to identify a person or individual. This includes government ID numbers (such as social security numbers in the USA or Europe, or CPF in Brazil), passport information (such as passport numbers, as well as issue and expiry dates), and even health data. APIs that return user profiles without proper access control might expose PII.</li>
				<li><strong class="bold">Financial data</strong>: Credit card details, bank account numbers, and financial transaction history are highly sensitive. If an API endpoint needs to process any type of payment, even when simply redirecting data to and receiving data from a payment system, it must have strict security controls in place.</li>
				<li><strong class="bold">Authentication</strong> (<strong class="bold">AuthN</strong>) <strong class="bold">credentials</strong>: Usernames, passwords, and access tokens are fundamental for securing <a id="_idIndexMarker599"/>APIs. When such data leaks, access to the whole system behind an API endpoint can be compromised.</li>
				<li><strong class="bold">Proprietary information:</strong> Trade secrets, intellectual property documents, and internal configurations can all be considered sensitive data. APIs that interact with internal systems or databases could potentially leak such information if they are not properly secured.</li>
			</ul>
			<p>It’s not always straightforward to detect when sensitive data is available to be extracted from an output. This may require some sophistication on the toolbelt we use to parse file dumps such as logs. We will now dive into a mass of logs and combine a few tools and patterns to discover which sensitive data or information is available. Depending on the log volume you have at hand, you may need to delegate this to an external system with more computing power to process it.</p>
			<p>As true API endpoints with true sensitive data won’t be used during this exercise, we need a way to generate some log files to be analyzed. There’s a good open source project written in Golang called <code>go</code> command directly as a binary (including using <code>.tar.gz</code> packages) or run it as a Docker container.</p>
			<p>These lines will not contain any type of sensitive data we are looking for. Therefore, let’s boost the utility with some random data that we can further search in queries. The code that follows does that. The loop<a id="_idIndexMarker601"/> creates log entries and stores them in the file pointed to by the <code>LOG_FILE</code> variable. Observe that sensitive data is only inserted when the iterator variable (<code>i</code>) is divisible by 100. When <code>i</code> is not divisible by 100, <code>flog</code> generates a completely random line. Hence, we’ll have 1,000 lines with sensitive data and 9,000 lines with no sensitive data. This will make the output file a big mass of data with less interesting content. The echo command is in a single line:</p>
			<pre class="source-code">
LOG_FILE=dummy.log
for i in $(seq 1 10000); do
  if [ $((i % 100)) -eq 0 ]; then
    # Every 100th line contains sensitive data
    <strong class="bold">echo "192.168.1.$((RANDOM % 255)) - user_$RANDOM [$(date +'%d/%b/%Y:%H:%M:%S %z')] \"POST /api/submit HTTP/1.1\" 200 $((RANDOM % 5000 + 500)) \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\" Auth_Token=\"$(openssl rand -hex 16)\" Credit_Card=\"1234-5678-9012-$RANDOM\"" &gt;&gt; $LOG_FILE</strong>
  else
    # Other lines contain generic log data
    flog &gt;&gt; $LOG_FILE
  fi
done</pre>			<p>We are making use of BASH’s <code>$RANDOM</code> internal variable, which generates pseudorandom numbers when read. Observe that we need to have <code>openssl</code> available on the system to generate the random strings that correspond to fake tokens. Simply delete the <code>Auth_Token</code> part if you don’t want it to be included. The preceding code creates a file of around 1 GB in size.</p>
			<p>So, how can we digest this data mass and only extract the interesting parts? There are some ways to do it. Considering that we are using a Linux system, even the <code>grep</code> command could fulfill this task, accompanied <a id="_idIndexMarker602"/>by a few regular expressions to facilitate the search. This is not the solution with the best performance though. We need something else.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor135"/>Elasticsearch and more</h2>
			<p>When handling big masses of data, we require<a id="_idIndexMarker603"/> the right tool. OK, 1 GB is not that big nowadays, but imagining that you will have access to terabytes of log files, how would search them all with <code>grep</code> in a feasible timeframe? We will exercise one possible solution: the <strong class="bold">Elasticsearch, Logstash, and Kibana</strong> (<strong class="bold">ELK</strong>) stack. They are three separate products that can be combined to<a id="_idIndexMarker604"/> provide one of the best-in-class experiences for data analysis and visualization activities. Also, they can run as Docker containers.</p>
			<p>One drawback, however, is the substantial requirement for resources (computing, memory, and storage). I could not run them on the lab VM (with 8 GB of RAM). Elasticsearch alone required more memory than was available. As a matter of fact, on the version I tried while writing this chapter (8.13.2), it was specifically complaining about the maximum map count check, which is controlled by a Linux kernel parameter. Even after increasing it to the number recommended by the documentation (reference the <em class="italic">Further reading</em> section for this), Elasticsearch didn’t work. I’ve also done a few tests with another system running on top of macOS, but both the container versions and the standalone versions presented different problems that made it difficult to set them up.</p>
			<p>I finally decided to run this stack on Elastic’s cloud platform. They sell it as a <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>) with a 14-day<a id="_idIndexMarker605"/> trial period. You can use all the product’s features and ingest external sources. There’s a sequence of steps to set this platform up:</p>
			<ol>
				<li>You need to sign up for the platform or subscribe via AWS, Google, or Microsoft’s cloud marketplace. Access <a href="https://cloud.elastic.co/">https://cloud.elastic.co/</a> and click on <strong class="bold">Sign up</strong>. You may receive a verification email with a link to click. Do it and log in.</li>
				<li>The wizard will prompt you to answer a few questions about yourself, such as your full name, company name, and purpose for using the platform.</li>
				<li>Then, the wizard will suggest creating a deployment. By clicking on <strong class="bold">Edit settings</strong>, you can choose the public cloud provider, region, hardware profile, and Elastic version. The application automatically selects a combination that’s appropriate to your location. Type a name for this deployment and click <strong class="bold">Create deployment</strong>.</li>
				<li>The deployment takes just a couple of minutes to be created and you are then redirected to the landing page of the platform. A small note is important here: you won’t receive your deployment’s credentials as expected. Because of that, you will need to follow an additional step that we’ll explain later:</li>
			</ol>
			<div><div><img src="img/B19657_figure_08.1.jpg" alt="Figure 8.1 – Elastic Cloud Platform’s landing page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Elastic Cloud Platform’s landing page</p>
			<ol>
				<li value="5">The next step is to configure an input. You need to tell it how Elasticsearch and Kibana will receive data that you’ll further analyze. For that part, we will make use of Filebeat, which is both an external utility and a built-in integration. You can even stream logs directly to the platform. That’s very useful when you want to continuously send data to be analyzed. In our case, it will happen only once.</li>
				<li>There are specific installation instructions depending on the operating system you are using. Ubuntu, by default, does not have the repository that the application can be downloaded from. For your convenience, I put a link in the <em class="italic">Further reading</em> section with the steps you need to follow.</li>
				<li>You won’t start Filebeat’s service right away. First, you’ll have to configure it to send data to Elastic’s cloud. At least on Ubuntu, the <code>filebeat.yml</code> configuration file is located at <code>/etc/filebeat</code>. You must only worry about two sections: <strong class="bold">Filebeat inputs</strong> and <strong class="bold">Elastic Cloud</strong>. Make a backup copy of this file and edit it with <a id="_idIndexMarker606"/>your preferred editor. Locate the <strong class="bold">Filebeat </strong><strong class="bold">inputs</strong> section.</li>
				<li>You’ll see something like this (the comments were omitted for brevity):<pre class="source-code">
- type: filestream
  id: my-filestream-id
  enabled: false
  paths:
    - /var/log/*.log</pre></li>				<li>You’ll have to do the following:<ol><li class="upper-roman">Replace <code>filestream</code> with <code>log</code>. This is to instruct Filebeat that this is not a file being constantly changed, but rather a static one.</li><li class="upper-roman">Replace <code>my-filestream-id</code> with something more relevant, such as <code>sensitive-data-log</code>.</li><li class="upper-roman">Replace <code>false</code> with <code>true</code> to effectively activate the input.</li><li class="upper-roman">Replace <code>/var/log/*.log</code> with the full path of the file you generated on the code we used before (the one with the <code>flog</code> utility).</li></ol></li>
				<li>Locate the <strong class="bold">Elastic Cloud</strong> section. You’ll see something like this:<pre class="source-code">
# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:
# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `&lt;user&gt;:&lt;pass&gt;`.
#cloud.auth:</pre></li>				<li>At this point, you’ll have to get back to the web console and locate both parameters. The Cloud ID can be found using this sequence:<ol><li class="upper-roman">From the landing page in <em class="italic">Figure 8</em><em class="italic">.1</em>, click on the three horizontal lines located on the left to open the lateral menu and choose <strong class="bold">Manage </strong><strong class="bold">this deployment</strong>.</li><li class="upper-roman">There’s a clipboard <a id="_idIndexMarker607"/>button you can click to facilitate copying this data. Click to copy and save it in a temporary place.</li></ol></li>
			</ol>
			<div><div><img src="img/B19657_figure_08.2.jpg" alt="Figure 8.2 – Where to find the Cloud ID on Elastic’s console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Where to find the Cloud ID on Elastic’s console</p>
			<ol>
				<li value="12">The <code>Cloud Auth</code> parameter demands a few more steps:<ol><li class="upper-roman">On this screen, click on the <strong class="bold">Actions</strong> button and select <strong class="bold">Reset password</strong>. This will redirect you to the <strong class="bold">Security</strong> settings page, where you can make a few adjustments:</li></ol></li>
			</ol>
			<div><div><img src="img/B19657_figure_08.3.jpg" alt="Figure 8.3 – Resetting the deployment’s password"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Resetting the deployment’s password</p>
			<ol>
				<li class="upper-roman" value="2">Click on the <strong class="bold">Reset password</strong> button. The website will ask you for confirmation. Simply click on <strong class="bold">Reset</strong>.</li>
				<li class="upper-roman">Your new password <a id="_idIndexMarker608"/>will be defined. You are good to either copy it (using a similar clipboard button) or download a CSV file with the credentials. See <em class="italic">Figure 8</em><em class="italic">.4</em> for reference.</li>
			</ol>
			<div><div><img src="img/B19657_figure_08.4.jpg" alt="Figure 8.4 – The new Elastic password is defined, with the option to click to copy it or download the CSV file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – The new Elastic password is defined, with the option to click to copy it or download the CSV file</p>
			<ol>
				<li value="13">Now, go back to the <code>filebeat.yml</code> file.</li>
				<li>Uncomment the <code>cloud.id</code> and <code>cloud.auth</code> lines. Next, insert a blank space right after each of the colons in both lines.</li>
				<li>Paste the data that you previously copied on the corresponding lines. For the <code>cloud.auth</code> line, observe that the expected format is <code>username:password</code>. The username portion is usually <code>elastic</code>.</li>
				<li>Save and close the file. There <a id="_idIndexMarker609"/>are a few commands you can use to verify whether the config file is in good shape and whether Filebeat can contact the cloud deployment:<pre class="source-code">
<strong class="bold">$ sudo filebeat test config</strong>
<strong class="bold">Config OK</strong>
<strong class="bold">$ sudo filebeat test output</strong>
<strong class="bold">elasticsearch: https://&lt;a type of credential will show up here&gt;.us-central1.gcp.cloud.es.io:443...</strong>
<strong class="bold">  parse url... OK</strong>
<strong class="bold">  connection...</strong>
<strong class="bold">    parse host... OK</strong>
<strong class="bold">    dns lookup... OK</strong>
<strong class="bold">    addresses: 35.193.143.25</strong>
<strong class="bold">    dial up... OK</strong>
<strong class="bold">  TLS...</strong>
<strong class="bold">    security: server's certificate chain verification is enabled</strong>
<strong class="bold">    handshake... OK</strong>
<strong class="bold">    TLS version: TLSv1.3</strong>
<strong class="bold">    dial up... OK</strong>
<strong class="bold">  talk to server... OK</strong>
<strong class="bold">  version: 8.13.2</strong></pre></li>				<li>Note that you may need to run the commands as a superuser. This will depend on your operating system’s defaults. Now, you are good to either start the Filebeat service or run it interactively. I personally prefer the second way since you can watch its log output:<pre class="source-code">
<strong class="bold">$ sudo filebeat -e</strong>
<strong class="bold">{"log.level":"info","@timestamp":"2024-04-21T18:23:44.082+0200","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/cmd/instance.(*Beat).configure","file.name":"instance/beat.go","file.line":811},"message":"Home path: [/usr/share/filebeat] Config path: [/etc/filebeat] Data path: [/var/lib/filebeat] Logs path: [/var/log/filebeat]","service.name":"filebeat","ecs.version":"1.6.0"}</strong>
<strong class="bold">{"log.level":"info","@timestamp":"2024-04-21T18:23:44.083+0200","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/cmd/instance.(*Beat).configure","file.name":"instance/beat.go","file.line":819},"message":"Beat ID: 6e7f7876-f768-449b-b6b2-b74cd1d65e93","service.name":"filebeat","ecs.version":"1.6.0"}</strong>
<strong class="bold">The rest of the output was omitted for brevity.</strong></pre></li>			</ol>
			<p>At this stage, you can return to the console to check what it is receiving. Assuming that everything is working, to see the<a id="_idIndexMarker610"/> lines of <code>dummy.log</code> populated, click on the three horizontal lines on the lateral menu again and go to <strong class="bold">Observability</strong> | <strong class="bold">Logs</strong>. If nothing shows up, just click <strong class="bold">Refresh</strong>. By default, this view shows the last 15 minutes of activity. If you were doing something else while the data was already being sent, you may not see anything at all. If that happens, simply change the view control to show older data, such as <strong class="bold">Last </strong><strong class="bold">1 year</strong>:</p>
			<div><div><img src="img/B19657_figure_08.5.jpg" alt="Figure 8.5 – Changing the view control to display log data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Changing the view control to display log data</p>
			<p>The change to the view control takes effect immediately. The following screenshot shows the type of viewing you <a id="_idIndexMarker611"/>will have when browsing the log data sent by Filebeat.</p>
			<div><div><img src="img/B19657_figure_08.6.jpg" alt="Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform</p>
			<p>Observe that the lines start with an IP address. We can use this as an index. To be able to search patterns on this data, we can choose one of the options that follow:</p>
			<ul>
				<li>Simply type the data you are looking for into this search bar. For example, if you type <code>Credit_card</code>, or <code>Auth_Token</code>, all lines with these patterns will be displayed after you press <em class="italic">Enter</em>.</li>
				<li>Create a data view. Some<a id="_idIndexMarker612"/> literature will use the term <strong class="bold">Index pattern</strong> to refer to this, but it was renamed some time ago to data view.</li>
			</ul>
			<p>This is a Kibana feature. To create data views, it will be easier if you type <code>Data View</code> in the topmost search bar. This<a id="_idIndexMarker613"/> will cause a suggestion to show up along with the corresponding link. Click on it. You’ll be redirected to a blank page with the <strong class="bold">Create data view</strong> button. After clicking on it, all sources will be displayed. Some of them were created by the deployment and there will be a Filebeat one:</p>
			<div><div><img src="img/B19657_figure_08.7.jpg" alt="Figure 8.7 – Creating a data view in Kibana"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Creating a data view in Kibana</p>
			<p>In the <code>filebeat-*</code>. This will cause the right part of the screen to update and show only the Filebeat source under the <code>@timestamp</code>). Click on <strong class="bold">Save data view </strong><strong class="bold">to Kibana</strong>.</p>
			<p>When you do this, the previous blank page will be updated with the recently created data view. Now, the final step is to discover patterns. Go back to the topmost search bar and type <code>Discover</code>. You will now be redirected to the <code>message</code> keyword on KQL. We could build a query like this:</p>
			<pre class="source-code">
message: Credit_card OR Auth_Token</pre>			<p>You will end up with the filtered window displayed:</p>
			<div><div><img src="img/B19657_figure_08.8.jpg" alt="Figure 8.8 – Using KQL to look for sensitive data patterns"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Using KQL to look for sensitive data patterns</p>
			<p>This is far from being an introductory training to the ELK stack. I put other links in the <em class="italic">Further reading</em> section, where you can look at regular expressions on the platform as well as take a free training<a id="_idIndexMarker615"/> course on it. That’s cool, but what if you don’t want to use a browser or even leverage some cloud offering to do your sensitive search? We’ll cover that next.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor136"/>ripgrep</h2>
			<p>If you’re looking for a tool with a smaller<a id="_idIndexMarker616"/> footprint than ELK for searching <a id="_idIndexMarker617"/>through logs for sensitive data, <code>rg</code> is a line-oriented search tool that combines the usability of The Silver Searcher (link in the <em class="italic">Further reading</em> section) with the raw speed of grep. <code>rg</code> works very efficiently by default, ignoring binary files, respecting your <code>.gitignore</code> files to skip hidden and ignored files, and using memory efficiently.</p>
			<p><code>rg</code> has at least three advantages when compared to the ELK stack:</p>
			<ul>
				<li>It is extremely fast and performs well even on large files.</li>
				<li>It is a sole executable file, easy to install and use without complex configurations.</li>
				<li>Does not require running services or daemons and has minimal memory and CPU usage compared to ELK.</li>
			</ul>
			<p>Installing it on Ubuntu is as easy as installing any application available via <code>apt</code> or <code>apt-get</code>. There are also versions <a id="_idIndexMarker618"/>available for macOS and Windows. Let’s see how it behaves with our 1 GB dummy file when looking for credit card numbers:</p>
			<pre class="console">
$ time rg "\b\d{4}-\d{4}-\d{4}-\d{4}\b" dummy.log
594006:192.168.1.120 - user_12186 [19/Apr/2024:04:41:55 +0200] "POST /api/submit HTTP/1.1" 200 1633 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="e4e8be71c743f3273e22c43e1585282a" Credit_Card="1234-5678-9012-1975"
1188012:192.168.1.223 - user_22717 [19/Apr/2024:04:41:56 +0200] "POST /api/submit HTTP/1.1" 200 2929 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="7e204c483eb812251e2c219bbdda7c08" Credit_Card="1234-5678-9012-5180"
1485015:192.168.1.247 - user_28863 [19/Apr/2024:04:41:57 +0200] "POST /api/submit HTTP/1.1" 200 1585 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="a4de7124036ae0229ad43a75f972be69" Credit_Card="1234-5678-9012-6131"
...Output omitted for brevity...
real    0m2.276s
user    0m2.235s
sys    0m0.040s</pre>			<p>In around 2.5 seconds, <code>rg</code> could find 26 lines with credit card numbers in a file of almost 1 GB in size! That happened while it was running on an Ubuntu VM with 4 vCPUs and 8 GB of RAM. By the way, Filebeat was still up, and my browser instances were also disputing CPU and memory with it. Let’s <a id="_idIndexMarker619"/>check how it goes with AuthN tokens:</p>
			<pre class="console">
$ time rg "Auth_Token=[^ ]+" dummy.log
99001:192.168.1.209 - user_10741 [19/Apr/2024:04:41:53 +0200] "POST /api/submit HTTP/1.1" 200 2550 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="76358e1eaf10a2da25845535f6a2f8ca" Credit_Card="1234-5678-9012-685"
198002:192.168.1.31 - user_15060 [19/Apr/2024:04:41:53 +0200] "POST /api/submit HTTP/1.1" 200 4211 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="bfc4d56410a31f16e939559d1fd19011" Credit_Card="1234-5678-9012-30887"
297003:192.168.1.120 - user_1823 [19/Apr/2024:04:41:54 +0200] "POST /api/submit HTTP/1.1" 200 2612 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="56a3d397f23094f3517296ea35e8bf5e" Credit_Card="1234-5678-9012-10401"
...Output omitted for brevity...
real    0m0.216s
user    0m0.172s
sys    0m0.044s</pre>			<p>That was even more insane. Since the regular expression was simpler, it could find 100 lines with the pattern in less than 0.5 seconds! As is the case with the regular <code>grep</code>, <code>rg</code> is case-sensitive. The same switch (<code>-i</code>) can be used to turn this off. You can also combine regular expressions to look for multiple patterns at once:</p>
			<pre class="console">
$ time rg -e "\b\d{4}-\d{4}-\d{4}-\d{4}\b" -e "Auth_Token=[^ ]+" dummy.log
99001:192.168.1.209 - user_10741 [19/Apr/2024:04:41:53 +0200] "POST /api/submit HTTP/1.1" 200 2550 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="76358e1eaf10a2da25845535f6a2f8ca" Credit_Card="1234-5678-9012-685"
198002:192.168.1.31 - user_15060 [19/Apr/2024:04:41:53 +0200] "POST /api/submit HTTP/1.1" 200 4211 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="bfc4d56410a31f16e939559d1fd19011" Credit_Card="1234-5678-9012-30887"
297003:192.168.1.120 - user_1823 [19/Apr/2024:04:41:54 +0200] "POST /api/submit HTTP/1.1" 200 2612 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" Auth_Token="56a3d397f23094f3517296ea35e8bf5e" Credit_Card="1234-5678-9012-10401"
...Output omitted for brevity...
real    0m1.821s
user    0m1.788s
sys    0m0.033s</pre>			<p>Everything finished in less than 2 seconds. That’s a win! You can optionally integrate <code>rg</code> into automation scripts and <a id="_idIndexMarker620"/>redirect its output to log files. Carefully look at its man page to discover more about this fabulous tool. Next, we are going to learn how we can make some tests to detect information leakage.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor137"/>Testing for information leakage</h1>
			<p>Cool! So, you had access to a data mass, obtained via data exfiltration, social engineering, or any <a id="_idIndexMarker621"/>other pentesting technique, and you just learned how to extract data from such mass with a few rather nice tools. However, how can you possibly test an API endpoint to verify whether it is vulnerable to leaking something you’re looking for? That’s what we’re going to see here. It is not redundant to say that we are not testing real public API endpoints because we obviously do not have access for doing so. Consider the teachings here to be for educational and professional purposes only.</p>
			<p>We will use our controlled lab environment to put some API routes to run and play with them a bit to understand to which extent they can disclose data that is supposed to be protected. The first thing you need to have is the data itself, of course. You can either pick a file with dummy data you may already have or run the script that follows. This will create 1,000 lines of random data, again making use of the <code>$RANDOM</code> BASH variable. It will contain user IDs, email addresses, credit card numbers, and AuthN tokens:</p>
			<pre class="source-code">
# Generating dummy sensitive data
echo "id,name,email,credit_card,auth_token" &gt; sensitive_data.csv
for i in {1..1000}; do
  echo "$i,User_$i,user$i@example.com,\
  $RANDOM-$RANDOM-$RANDOM-$RANDOM,\
  $(openssl rand -hex 16)" &gt;&gt; sensitive_data.csv
done</pre>			<p>The created file will be a CSV and will look like the following:</p>
			<pre class="source-code">
id,name,email,credit_card,auth_token
1,User_1,user1@example.com,  10796-5693-25560-7313,  7fb3eb19f290e107a789c781a50e2ff3
2,User_2,user2@example.com,  16541-23368-7044-11673,  41715cd1bc94db51192e61d895a6fed6
3,User_3,user3@example.com,  433-32493-22646-29072,  03ac641fb0d669d18320b9806403ad4c
4,User_4,user4@example.com,  21120-26964-18866-19201,  9566b0809b3fe28b8e86b8f97961670a
5,User_5,user5@example.com,  24266-28815-8839-23803,  f345c6d3ef4a83433178d7b5431c8e47
6,User_6,user6@example.com,  32051-14393-2369-23011,  006e2fe5208e98c694318f099ecdbb62
7,User_7,user7@example.com,  2141-3195-31552-27733,  864a9c035fd0f3fd07383406c620192e
8,User_8,user8@example.com,  215-813-6840-24823,  36f2da15355593dcca987f570f331673
9,User_9,user9@example.com,  4015-30295-20623-27347,  fe59f7e5b7c6b02a7ff622848e7ff2dd
10,User_10,user10@example.com, 14783-2106-26501-22541, a8f56bf3720c74cb2d0859cfc071bbed
...Output omitted for brevity...</pre>			<p>Let’s now implement<a id="_idIndexMarker622"/> the API with five routes:</p>
			<ul>
				<li><code>/users</code>: An endpoint that exposes sensitive user information without AuthN.</li>
				<li><code>/login</code>: An endpoint that is vulnerable to SQL injection.</li>
				<li><code>/profile/&lt;user_id&gt;</code>: An endpoint with inadequate access control.</li>
				<li><code>/get_sensitive_data</code>: An endpoint that is vulnerable to data leakage.</li>
				<li><code>/cause_error</code>: An endpoint that triggers verbose error messages with stack traces.</li>
			</ul>
			<p>The code to implement this application is available at <a href="https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py">https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py</a>. It was written in Python since that’s one of the main languages we’ve been using in this book and since it’s quite trivial and straightforward to understand. The pandas framework is used to facilitate the reading of CSV files.</p>
			<p>As you already know, this code listens on port TCP/5000 by default. Set it to run and let’s play with the endpoints. As this application is vulnerable to some threats, you don’t necessarily have to authenticate first to be able to talk to the endpoints.</p>
			<p>Without having access to the code, you’d obviously have to apply the reconnaissance techniques that we covered in the second part of this book. However, since you do have access to the code, even in a sloppy analysis of it, you will discover how weakly this API was purposefully implemented. Going<a id="_idIndexMarker623"/> top to down, we can see that:</p>
			<ul>
				<li>There’s an endpoint that sends back the whole data mass without any previous AuthN and AuthZ.</li>
				<li>The login endpoint is <a id="_idIndexMarker624"/>vulnerable to SQL Injection, even in the simplest forms.</li>
				<li>The route that gives information about user profiles does not check whether the user is authorized to access such information.</li>
				<li>The penultimate route tries to do some control by looking for an AuthZ token, but it’s so simple that the value could be guessed after a few attempts.</li>
				<li>Finally, there’s even an endpoint that raises an internal exception, creating possibilities to disclose data about the internal infrastructure.</li>
			</ul>
			<p>Let’s try them one by one:</p>
			<pre class="console">
$ curl http://127.0.0.1:5000/users
[
  {
    "auth_token": "  7fb3eb19f290e107a789c781a50e2ff3",
    "credit_card": "  10796-5693-25560-7313",
    "email": "user1@example.com",
    "id": 1,
    "name": "User_1"
  },
  {
    "auth_token": "  41715cd1bc94db51192e61d895a6fed6",
    "credit_card": "  16541-23368-7044-11673",
    "email": "user2@example.com",
    "id": 2,
    "name": "User_2"
  },
  {
    "auth_token": "  03ac641fb0d669d18320b9806403ad4c",
    "credit_card": "  433-32493-22646-29072",
    "email": "user3@example.com",
    "id": 3,
    "name": "User_3"
  }
...Output omitted for brevity...</pre>			<p>You just got all the users, organized in a JSON format to facilitate being categorized afterward. The login<a id="_idIndexMarker625"/> endpoint does not actually interface with a SQL database. Hence, we won’t be able to simulate an injection attack here, but the spirit remains.</p>
			<pre class="console">
$ curl -X POST -H "Content-Type: application/json" \
-d '{"username": "admin", "password": "admin\' OR \'1\'=\'1"}' \
http://localhost:5000/login
{
  "message": "Invalid credentials!"
}</pre>			<p>What about the route that shows user profiles? It does not require any previous AuthZ to check a profile. Let’s try it:</p>
			<pre class="console">
$ curl http://localhost:5000/profile/10
{
  "auth_token": "  0f5832741bd997a963a2b1c10c7e3410",
  "credit_card": "  4904-20956-3479-12358",
  "email": "user10@example.com",
  "id": 10,
  "name": "User_10"
}</pre>			<p>You just got another API endpoint that discloses valid information without the correct AuthN or AuthZ. Let’s move on with the exercise and explore the one that tries to protect the application with an AuthZ token. In this case, we know that the token control is a simple Python condition that checks a trivial token content, but in a real-world scenario where some NoSQL or in-memory database would be in place, we could try a relevant injection attack to bypass the protection:</p>
			<pre class="console">
curl -H "Authorization: 12345" http://localhost:5000/get_sensitive_data
id,name,email,credit_card,auth_token
1,User_1,user1@example.com,  24280-22986-24153-30647, 1314d0dabf32fb00873d2af1df67104b
2,User_2,user2@example.com,  22724-31508-12727-13842,  0120956bf359ec6768e41451a4427360
3,User_3,user3@example.com,  19369-31798-14486-31982,  8be7e021287609dd9e274ccf26b7bbb5
…Output omitted for brevity…</pre>			<p>The final route is just there to<a id="_idIndexMarker626"/> push a detailed error message to reinforce the danger of not treating exceptions and errors when they happen. To know more about this, check <a href="B19657_06.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, where we have deep coverage of the subject:</p>
			<pre class="console">
$ curl http://localhost:5000/cause_error
&lt;!doctype html&gt;
&lt;html lang=en&gt;
  &lt;head&gt;
    &lt;title&gt;ZeroDivisionError: division by zero
 // Werkzeug Debugger&lt;/title&gt;
    &lt;link rel="stylesheet" href="?__debugger__=yes&amp;amp;cmd=resource&amp;amp;f=style.css"&gt;
    &lt;link rel="shortcut icon"
        href="?__debugger__=yes&amp;amp;cmd=resource&amp;amp;f=console.png"&gt;
    &lt;script src="img/?__debugger__=yes&amp;amp;cmd=resource&amp;amp;f=debugger.js"&gt;&lt;/script&gt;
    &lt;script&gt;
      var CONSOLE_MODE = false,
          EVALEX = true,
          EVALEX_TRUSTED = false,
          SECRET = "MN645GMVPd9f6W0ZSFTa";
    &lt;/script&gt;
  &lt;/head&gt;
...Output omitted for brevity...</pre>			<p>These are some ways to interact with APIs and get access to data that should not be directly accessible to a regular user. Moreover, unearthing unintentional information disclosure in an API involves a combination of passive and active probing methods. You can employ tools to craft diverse inquiries to the API and carefully examine the replies for potential leaks. This may encompass inspecting hidden data embedded within the response (metadata), error messages that might be overly revealing, or specific pieces of information that shouldn’t be readily available.</p>
			<p>In a live environment, tools such as Wireshark (or its command-line equivalent, <code>tshark</code>) may be useful to detect hidden fields or unprotected payloads that, once discovered, will most<a id="_idIndexMarker627"/> likely reveal what you are looking for. Burp Suite or OWASP ZAP also play a part here, and that’s especially true when the traffic to or from the API endpoints is encrypted with TLS. In such cases, if you are not able to replace the target’s TLS certificate with your own, which would allow you to completely see the packets’ contents, you could struggle more to dig into the findings. Next, we are going to understand which techniques we can use to reduce the chances of data leakage in the world of APIs.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor138"/>Preventing data leakage</h1>
			<p>To eliminate or at least reduce the chances of suffering data leakage on your API or the application behind it, a <a id="_idIndexMarker628"/>multi-layered approach is possibly one of the best options. This involves secure coding practices, robust AuthN, and careful handling of sensitive information.</p>
			<p>The first line of defense is secure API design – only create the interfaces you need. In other words, only expose the data your API requires to function. Avoid open queries that could allow unauthorized access. In GraphQL, tools such as query whitelisting act as bouncers, restricting data requests and preventing the over-fetching of sensitive information.</p>
			<p>Source code best practices are a vital topic too. When interacting with databases, one important point to keep in mind is to use parameterized queries instead of simply forwarding what the user provides as input to them. Think of these as pre-prepared invitations to the database – they prevent attackers from manipulating the query and potentially stealing data (often referred to as<a id="_idIndexMarker629"/> SQL injection attacks). An example of Python code implementing such queries is available here:</p>
			<pre class="source-code">
import sqlite3
def get_user_info(user_id):
    # Use parameterized query to prevent SQL injection
    connection = sqlite3.connect('my_database.db')
    cursor = connection.cursor()
    cursor.execute("SELECT * FROM users WHERE id = ?", (user_id,))
    user = cursor.fetchone()
    connection.close()
    return user</pre>			<p>Observe the user of a parameterized placeholder (<code>?</code>) for the <code>user_id</code> field. This prevents the possibility that input data provided by the API endpoint’s user affects the final SQL database, reducing the chances of injection attacks.</p>
			<p>The dynamic couple of<a id="_idIndexMarker630"/> AuthN and AuthZ must never be forgotten. APIs should use strong mechanisms such as OAuth 2.0 or OpenID Connect to ensure that only authorized users can<a id="_idIndexMarker631"/> access sensitive endpoints. <strong class="bold">JSON Web Tokens</strong> (<strong class="bold">JWTs</strong>) are like secure invitations – compact and protected, they allow developers to control who gets in. In the code block that follows, you can see an implementation of JWT in Python with the use of the Flask JWT Extended module:</p>
			<pre class="source-code">
from flask import Flask, jsonify, request
from flask_jwt_extended import JWTManager, create_access_token, jwt_required
app = Flask(__name__)
app.config['JWT_SECRET_KEY'] = 'type_a_secure_key_here'
jwt = JWTManager(app)
@app.route('/login', methods=['POST'])
def login():
    username = request.json.get("username", "")
    password = request.json.get("password", "")
    if username == "admin" and password == "admin123":
        access_token = create_access_token(identity=username)
        return jsonify(access_token=access_token)
    return jsonify({"message": "Invalid credentials!"}), 401
@app.route('/protected', methods=['GET'])
@jwt_required()
def protected():
    return jsonify({"message": "Access granted!"})</pre>			<p>To be able to access the <code>/protected</code> API route, users must present a valid JWT token, which is required by the <code>@</code><code>jwt_required()</code> decorator.</p>
			<p>Data encryption is like a crown jewel. You must apply TLS as much as possible in your communication. As a matter of fact, Red Hat OpenStack, which is a private cloud offering, uses a<a id="_idIndexMarker632"/> concept called <strong class="bold">TLS-e</strong> (the <strong class="bold">e</strong> stands for <strong class="bold">everywhere</strong>), which means that internal and public endpoints of the product have TLS enabled, guaranteeing traffic encryption. For data at rest, encryption algorithms such as AES (with strong key sizes) act as the vault door, safeguarding stored data.</p>
			<p>Input validation and sanitization offer a subtle yet absolutely inevitable shield. Do not simply accept what comes in as valid. When designing or writing an API, you should always, always, always think with the mind of a criminal: every single line of code or implemented endpoint can <a id="_idIndexMarker633"/>be explored in a malicious way. Sanitizing user input helps prevent attacks such as SQL injection and <strong class="bold">Cross-Site Scripting</strong> (<strong class="bold">XSS</strong>) that could lead to data leakage if left unchecked. In such scenarios, the OWASP <strong class="bold">Enterprise Security API</strong> (<strong class="bold">ESAPI</strong>) gives <a id="_idIndexMarker634"/>a helping hand in enforcing security checks.</p>
			<p>For GraphQL APIs, preventing<a id="_idIndexMarker635"/> the over-fetching of data is crucial. Techniques such as query whitelisting and query cost analysis act as portion control measures, ensuring that users only retrieve the data they need. The Apollo GraphQL platform offers additional security resources and tools for managing and analyzing queries.</p>
			<p>Correct error handling means that you shouldn’t disclose anything that’s not strictly necessary to display that an error has happened. Also, catching all possible exceptions to avoid an unmapped error can inadvertently disclose internal data to the public.</p>
			<p>Finally, logging and monitoring close our layered approach. Properly configured logging allows security teams to detect and respond to suspicious activity, while monitoring tools act as alarms, alerting administrators to potential breaches or unauthorized access. However, it’s important to ensure that logs don’t contain sensitive information. Rotate and encrypt them as needed.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor139"/>Summary</h1>
			<p>This chapter started the fourth part of the book, covering API advanced topics. We learned how to identify when sensitive data is exposed. We also discussed ways to test for information leakage on API endpoints (or routes) and finished the chapter with general recommendations on why and how such problems could be prevented.</p>
			<p>At the end of the day, it doesn’t matter whether an API uses a modern programming language, has just a few endpoints, and only does specific tasks if the data that this API services is not well protected. Data leakage is one of the (if not the number-one) most feared problems in cyber incidents when they hit companies, regardless of their size.</p>
			<p>In the next chapter, we will finish part four by talking about API abuse and general logic tests. It’s nothing less than better understanding the business logic behind an API implementation and how failures on it may lead to exploitations on the API itself. See you there!</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor140"/>Further reading</h1>
			<ul>
				<li>Flog: <a href="https://github.com/mingrammer/flog">https://github.com/mingrammer/flog</a></li>
				<li>The ELK stack: <a href="https://www.elastic.co/elastic-stack">https://www.elastic.co/elastic-stack</a></li>
				<li>The maximum map count check problem: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html">https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html</a></li>
				<li>Filebeat, an agent to send logs: <a href="https://www.elastic.co/beats/filebeat">https://www.elastic.co/beats/filebeat</a></li>
				<li>Installing Filebeat on Ubuntu: <a href="https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt">https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt</a></li>
				<li>KQL: <a href="https://www.elastic.co/guide/en/kibana/current/kuery-query.html">https://www.elastic.co/guide/en/kibana/current/kuery-query.html</a></li>
				<li>Apache Lucene, an open source search engine: <a href="https://lucene.apache.org/">https://lucene.apache.org/</a></li>
				<li>Exploring regular expressions on Elasticsearch: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html</a></li>
				<li>Free official Elastic training: <a href="https://www.elastic.co/training/free">https://www.elastic.co/training/free</a></li>
				<li><code>rg</code> tool: <a href="https://github.com/BurntSushi/ripgrep">https://github.com/BurntSushi/ripgrep</a></li>
				<li>The Silver Searcher tool: <a href="https://github.com/ggreer/the_silver_searcher">https://github.com/ggreer/the_silver_searcher</a></li>
				<li>Red Hat OpenStack TLS-e: <a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints">https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints</a></li>
				<li>OWASP ESAPI: <a href="https://owasp.org/www-project-enterprise-security-api/">https://owasp.org/www-project-enterprise-security-api/</a></li>
			</ul>
		</div>
	</body></html>