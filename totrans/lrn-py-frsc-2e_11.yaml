- en: Parsing Outlook PST Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Electronic mail** (**email**) continues to be one of the most common methods
    of communication in the workplace, surviving the number of new communication services
    present in today''s world. Emails can be sent from computers, websites, and the
    phones that''re in so many pockets across the globe. This medium allows for the
    transmission of information in the form of text, HTML, attachments, and more in
    a reliable fashion. It''s no wonder, then, that emails can play a large part in
    investigations, especially for cases involving the workplace. In this chapter,
    we''re going to work with a common email format, **Personal Storage Table** (**PST**),
    used by Microsoft Outlook to store email content in a single file.'
  prefs: []
  type: TYPE_NORMAL
- en: The script we'll develop in this chapter introduces us to a series of operations
    available through the `libpff` library developed by Joachim Metz. This library
    allows us to open PST file and explore its contents in a Pythonic manner. Additionally,
    the code we build demonstrates how to create dynamic, HTML-based, graphics to
    provide additional context to spreadsheet-based reports. For these reports, we'll
    leverage the Jinja2 module, introduced in [Chapter 5](a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml),
    *Databases in Python*, and the D3.js framework to generate our dynamic HTML-based
    charts.
  prefs: []
  type: TYPE_NORMAL
- en: The D3.js project is a JavaScript framework that allows us to design informative
    and dynamic charts without much effort. The charts used in this chapter are open
    source examples of the framework shared with the community at [https://github.com/d3/d3](https://github.com/d3/d3).
    Since this book doesn't focus on JavaScript, nor does it introduce the language,
    we won't cover the implementation details to create these charts. Instead, we'll
    demonstrate how to add our Python results to a pre-existing template.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll use a sample PST file, which has a large variety of data across
    time, to test our script. As always, we recommend running any code against test
    files before using it in casework to validate the logic and feature coverage.
    The library used in this chapter is in active development and is labeled experimental
    by the developer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the background of PST files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging `libpff` and its Python bindings, `pypff`, to parse PST files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating informative and professional charts using Jinja2 and D3.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter is developed and tested using Python 2.7.15.
  prefs: []
  type: TYPE_NORMAL
- en: The PST file format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PST format is a type of **Personal File Format** (**PFF**). Two other types
    of PFF file include the **Personal Address Book** (**PAB**) for storing contacts
    and the **Offline Storage Table** (**OST**), which stores offline email, calendars,
    and tasks. By default, Outlook stores cached email information in OST files, which
    can be found at the locations specified in the following table. Items in Outlook
    will be stored in a PST file if archived:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Windows version** | **Outlook version** | **OST location** |'
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | Outlook 2000/2003/2007 | `C:\Documents and Settings\USERPROFILE%\Local
    Settings\Application Data\Microsoft\Outlook` |'
  prefs: []
  type: TYPE_TB
- en: '| Windows Vista/7/8 | Outlook 2007 | `C:\Users\%USERPROFILE%\AppData\Local\Microsoft\Outlook`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | Outlook 2010 | `C:Documents and Settings\%USERPROFILE%\My Documents\Outlook
    Files` |'
  prefs: []
  type: TYPE_TB
- en: '| Windows Vista/7/8 | Outlook 2010/2013 | `C:\Users\%USERPROFILE%\Documents\Outlook
    Files`  |'
  prefs: []
  type: TYPE_TB
- en: 'From: [https://forensicswiki.org/wiki/Personal_Folder_File_(PAB,_PST,_OST)](https://forensicswiki.org/wiki/Personal_Folder_File_(PAB,_PST,_OST)).
    Location of OST files by default.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `%USERPROFILE%` field is dynamic and replaced with the user account name
    on the machine. PFF files can be identified through the hex file signature of
    `0x2142444E` or `!BDN` in ASCII. After the file signature, the type of PFF file
    is denoted by 2 bytes at offset 8:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Hex signature** | **ASCII signature** |'
  prefs: []
  type: TYPE_TB
- en: '| PST | 534D | SM |'
  prefs: []
  type: TYPE_TB
- en: '| OST | 534F | SO |'
  prefs: []
  type: TYPE_TB
- en: '| PAB | 4142 | AB |'
  prefs: []
  type: TYPE_TB
- en: From http://www.garykessler.net/library/file_sigs.html
  prefs: []
  type: TYPE_NORMAL
- en: 'The content type (such as 32-bit or 64-bit) is defined at byte offset 10\.
    The structure of the PFF file format has been described in detail by Joachim Metz
    in several papers that document the technical structure and how to manually parse
    these files on GitHub at the project''s code repository: [https://github.com/libyal/libpff](https://github.com/libyal/libpff).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll work only with PST files and we can ignore the differences
    in OST and PAB files. By default, PST archives have a root area containing a series
    of folders and messages depending on how the archives were created. For example,
    a user may archive all folders in their view or only a select few. All of the
    items within the selected content will be exported into the PST file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to users archiving content, Outlook has an automatic archiving
    feature that will store items in the PST files after a set time as defined in
    the following table. Once this expiration period has been reached, the items will
    be included in the next archive created. The automatic archive stores PSTs by
    default in `%USERPROFILE%\Documents\Outlook` in Windows 7, `%APPDATA%\Local\Microsoft\Outlook`
    in Vista, and `%APPDATA%\Local Settings\Microsoft\Outlook` in XP. These defaults
    could be set by the user or by group policy in a domain environment. This automatic
    archive functionality provides examiners with a great history of communication
    information that we can access and interpret in our investigations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Folder** | **Default aging period** |'
  prefs: []
  type: TYPE_TB
- en: '| Inbox and Drafts | 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| Sent Items and Deleted Items | 2 months |'
  prefs: []
  type: TYPE_TB
- en: '| Outbox | 3 months |'
  prefs: []
  type: TYPE_TB
- en: '| Calendar | 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| Tasks | 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| Notes | 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| Journal | 6 months |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11.1: Default aging of Outlook items (https://support.office.com/en-us/article/Automatically-move-or-delete-older-items-with-AutoArchive-e5ce650b-d129-49c3-898f-9cd517d79f8e)'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to libpff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `libpff` library allows us to reference and navigate through PST objects
    in a programmatic manner. The `root_folder()` function allows us to reference
    `RootFolder`, which is the base of the PST file and the starting point for our
    recursive analysis of email content. Within `RootFolder` are folders and messages.
    The folders can contain other sub-folders or messages. Folders have properties
    that include the name of the folder, the number of subfolders, and the number
    of submessages. Messages are objects representing messages and have attributes,
    including the subject line, the name of all participants, and several timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: How to install libpff and pypff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing some third-party libraries is more difficult than running `pip install
    <library_name>`. In the case of `libpff` and the `pypff` bindings, we need to
    take a few steps and follow the instructions outlined in the GitHub project repository.
    The `libpff` wiki (located at [https://github.com/libyal/libpff/wiki/Building](https://github.com/libyal/libpff/wiki/Building))
    describes the steps we need to take in order to build `libpff`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll briefly walk through how you would build this library on an Ubuntu 18.04
    system. After downloading and installing Ubuntu 18.04 (preferably in a virtual
    machine), you''ll want to install the dependencies by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install the required packages for both our script and the `pypff`
    bindings. We''ll then want to download our `libpff` code by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the `git clone` command completes, we''ll navigate into the new `libpff`
    directory and run the following commands to download additional dependencies,
    configure, and install the components we need for the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Additional build options are described further on the `libpff` wiki page.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should be able to run the following statements and get the
    same output, though your version may vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To make this process easier for you, we've prebuilt the `pypff` bindings and
    created a Dockerfile to run this entire setup for you. If you're unfamiliar with
    Docker, it's a virtualization environment that allows us to run virtual machines
    with minimal effort. While Docker is generally used to host applications, we'll
    use it more as a traditional virtual machine. What makes this advantageous for
    us is that we can distribute a configuration file that you can run on your system
    and generate the same environment that we've tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, please follow the instructions to install Docker on your system from [https://docs.docker.com/install/](https://docs.docker.com/install/).
    Once installed and running, navigate to the `Chapter 11` code folder on your system
    and run the `docker build` command. This command will generate a system following
    a series of preconfigured steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e560c17a-8832-4e57-bb38-a5163ddd3b28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will create a new image named `lpff-ch11` with the version number 20181130\.
    An image in Docker is what it sounds like: a base installation that you can use
    to create running machines. This way you can have multiple machines all based
    on the same image. Each machine is called a container, and to create a container
    from this image, we''ll use the `docker run` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e6dc4dd-69ab-4de0-a5ba-00efa49927a3.png)'
  prefs: []
  type: TYPE_IMG
- en: The `-it` flag in the `docker run` command asks Docker to connect to the bash
    shell once the container is created. The `-P` parameter asks Docker to provide
    us with networking to, in our case, the web server running on the container. Lastly,
    the `--name` argument allows us to assign a familiar name to our container. We
    then pass in the image name and version and run the command. As you can see, we're
    provided with a root shell as soon as the Docker instance finishes.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the previously mentioned web server, we've included `lighttpd` to
    allow us to serve our HTML-generated report as a web page. This isn't necessary,
    though we wanted to highlight how these reports could be made accessible on an
    internal system.
  prefs: []
  type: TYPE_NORMAL
- en: Please don't run this Docker container on a public network as it'll allow anyone
    with access to your machine's IP address to see your HTML reports.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we start this web server by running `server lighttpd
    start` and then list the contents of our current directory. As you can see, we
    have two files, our `pst_indexer.py` script that we're about to build and the
    `stats_template.html` that we'll use to generate our sharp report. Let's build
    our Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring PSTs – pst_indexer.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this script, we'll harvest information about the PST file, taking note of
    the messages in each folder and generating statistics for word usage, frequent
    senders, and a heat map for all email activity. Using these metrics, we can go
    beyond the initial collection and reporting of messages and explore trends in
    the language used or communication patterns with certain individuals. The statistics
    section highlights examples of how we can utilize the raw data and build informative
    graphics to assist the examiner. We recommend tailoring the logic to your specific
    investigation to provide the most informative report possible. For example, for
    the word count, we'll only be looking at the top ten words that're alphanumeric
    and longer than four characters, to help reduce common words and symbols. This
    might not provide the correct information for your investigation and might require
    tailoring to your specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: An overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter's script was built to work with Python 2.7.15 and requires the
    third-party libraries described in the previous section. Please consider using
    the Docker image alongside this script.
  prefs: []
  type: TYPE_NORMAL
- en: As with our other chapters, this script starts by importing libraries we use
    at the top. In this chapter, we use two new libraries, one of which is third-party.
    We've already introduced `pypff`, the Python bindings to the `libpff` library.
    The `pypff` module specifies the Python bindings that allow us access to the compiled
    code. On line 8, we introduce `unicodecsv`, a third-party library we've used previously
    in [Chapter 5](a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml), *Databases in Python*.
    This library allows us to write Unicode characters to CSV files as the native
    CSV library doesn't support Unicode characters as nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 6, we import a standard library called `collections` that provides
    a series of useful interfaces including `Counter`. The `Counter` module allows
    us to provide values to it and it handles the logic of counting and storing objects.
    In addition to this, the collections library provides `OrderedDict`, which is
    extremely useful when you need to create a dictionary with keys in a specified
    order. The `OrderedDict` module isn''t leveraged in this book though it does have
    its place in Python when you wish to use key-value pairs in an ordered list-like
    fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Following our license and script metadata, we''ll set up a few global variables.
    These variables will help us decrease the number of variables we must pass into
    functions. The first global variable is `output_directory`, defined on line 46,
    which will store a string path set by the user. The `date_dictionary`, defined
    on line 47, uses dictionary comprehension to create keys 1 through 24 and map
    them to the integer 0\. We then use list comprehension on line 48 to append seven
    instances of this dictionary to `date_list`. This list is leveraged to build a
    heat map to show information about activity within the PST file split within seven
    days'' worth of 24-hour columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This heat map will establish baseline trends and help identify anomalous activity.
    An example includes the ability to see a spike in activity at midnight on week
    nights or excessive activity on Wednesdays before the business day starts. The
    `date_list` has seven dictionaries, one for each day, each of which is identical
    and contains a key-value pair for the hour of the day with the default value of
    `0`.
  prefs: []
  type: TYPE_NORMAL
- en: The `date_dict.copy()` call on line 48 is required to ensure that we can update
    the hours within a single date. If we omit the `copy()` method, every day will
    be updated. This is because dictionaries are tied together by references to the
    original object, and we're generating a list of objects without the `copy()` method.
    When we do use this function, it allows us to create a copy of the values with
    a new object, so we can create a list of different objects.
  prefs: []
  type: TYPE_NORMAL
- en: With these variables built, we can reference and update their values throughout
    other functions without needing to pass them again. Global variables are read-only
    by default and require a special global command in order to be modified by a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following functions outline our script''s operation. As usual, we have
    our `main()` function to control behavior. The following is the `make_path()`
    function, which is a utility to assist us in gathering full paths for our output
    files. The `folder_traverse()` and `check_for_msgs()` functions are used to iterate
    through the available items and start processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Our remaining functions focus on processing and reporting data within PSTs.
    The `process_message()` function reads the message and returns the required attributes
    for our reports. The first reporting function is the `folder_report()` function.
    This code creates a CSV output for each folder found within the PST and describes
    the content found within each.
  prefs: []
  type: TYPE_NORMAL
- en: This function also processes data for the remaining reports by writing message
    bodies to a single text file, stores each set of dates, and preserves a list of
    the senders. By caching this information to a text file, the next function is
    easily able to read the file without a major impact on memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `word_stats()` function reads and ingests the information into a collection.
    The `Counter()` object is used in our `word_report()` function. When generating
    our word count report, we read the collection''s. `Counter()` object into a CSV
    file, which  will be read by our JavaScript code. The `sender_report()` and `date_report()`
    functions also flush data to delimited files for interpretation by JavaScript
    in the report. Finally, our `html_report()` function opens our report template
    and writes the custom report information into an HTML file in our output folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As with all of our scripts, we handle our arguments, logs, and the `main()`
    function call under the `if __name__ == "__main__":` conditional statement on
    line 302\. We define the required arguments, `PST_FILE` and `OUTPUT_DIR`, and
    the user can specify optional arguments, `--title` and `-l`, for a custom report
    title and log path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining our arguments, we begin processing them so that we can pass
    them to the `main()` function in a standardized and safe manner. On line 319,
    we convert the output location into an absolute path so that we can be sure about
    accessing the correct location throughout the script. Notice how we''re calling
    the `output_directory` global variable and assigning a new value to it. This is
    only possible because we''re not within a function. If we were modifying the global
    variable within a function, we would need to write `global output_directory` on
    line 318:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After we modify the `output_directory` variable, we make sure the path exists
    (and create it if it doesn''t) to avoid errors later in the code. Once complete,
    we then use our standard logging code snippet to configure logging for this script
    on lines 331 through 339\. On lines 341 through 345, we log debug information
    on the system executing the script prior to calling the `main()` function. On
    line 346, we call the `main()` function and pass the `args.PST_FILE` and `args.title`
    arguments. We don''t need to pass the `output_directory` value because we can
    reference it globally. Once we pass the arguments and the `main()` function completes
    execution, we log that the script has finished executing on line 347:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The following flowchart highlights how the functions interact with each other.
    This flowchart might seem a little complicated but encapsulates the basic structure
    of our script.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main()` function calls the recursive `folder_traverse()` function, which
    in turn finds, processes, and summarizes messages and folders from the root folder.
    After this, the `main()` function generates reports with the word, sender, and
    date reports, which get displayed in one HTML report generated by the `html_report()`
    function. As a note, the dashed lines represent functions that return a value,
    while the solid lines represent a function that returns no value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41b0ceae-68da-4acd-9cec-2fb60a9fd531.png)'
  prefs: []
  type: TYPE_IMG
- en: Developing the main() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `main()` function controls the primary operations of the script, from opening
    and initial processing of the file, traversing the PST, to generating our reports.
    On line 62, we split the name of the PST file from its path using the `os.path`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `pst_name` variable if a custom title isn''t supplied by the
    user. On the next line, we use the `pypff.open()` function to create a PST object.
    We use the `get_root_folder()` method to get the PST root folder so we can begin
    the iteration process and discover items within the folders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With the root folder extracted, we call the `folder_traverse()` function on
    line 67 to begin traversing the directories within the PST container. We''ll cover
    the nature of this function in the next section. After traversing the folders,
    we start generating our reports with the `word_stats()`, `sender_report()`, and
    `date_report()` functions. On line 74, we pass the name of the report, the PST
    name, and lists containing the most frequent words and senders to provide statistical
    data for our HTML dashboard, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the make_path() helper function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make life simpler, we've developed a helper function, `make_path()`, defined
    on line 78\. Helper functions allow us to reuse code that we might normally write
    out many times throughout our script in one function call. With this code, we
    take an input string representing a file name and return the absolute path of
    where the file should exist within the operating system based on the `output_directory`
    value supplied by the user. On line 85, two operations take place; first, we join
    the `file_name` to the `output_directory` value with the correct path delimiters
    using the `os.path.join()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, this value is processed by the `os.path.abspath()` method, which provides
    the full file path within the operating system environment. We then return this
    value to the function that originally called it. As we saw in the flow diagram,
    many functions will make calls to the `make_path()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Iteration with the folder_traverse() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function recursively walks through folders to parse message items and
    indirectly generates summary reports on the folder. This function, initially provided
    the root directory, is generically developed to be capable of handling any folder
    item passed to it. This allows us to reuse the function for each discovered subfolder.
    On line 97, we use a `for` loop to recurse through the `sub_folders` iterator
    generated from our `pypff.folder` object. On line 98, we check whether the folder
    object has any additional subfolders and, if it does, call the `folder_traverse()`
    function again before checking the current folder for any new messages. We only
    check for messages in the event that there are no new subfolders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a recursive function because we call the same function within itself
    (a loop of sorts). This loop could potentially run indefinitely, so we must make
    sure the data input will have an end to it. A PST should have a limited number
    of folders and will therefore eventually exit the recursive loop. This is essentially
    our PST specific `os.walk()` function, which iteratively walks through filesystem
    directories. Since we''re working with folders and messages within a file container,
    we have to create our own recursion. Recursion can be a tricky concept to understand;
    to guide you through it, please reference the following diagram when reading our
    explanation in the upcoming paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd3782c-1bd5-4025-a20d-94d7cefca744.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, there're five levels in this PST hierarchy, each containing
    a mixture of blue folders and green messages. On level **1**, we have `Root Folder`,
    which is the first iteration of the `folder_traverse()` loop. Since this folder
    has a single subfolder, `Top of Personal Folders`, as you can see on level **2**,
    we rerun the function before exploring the message contents. When we rerun the
    function, we now evaluate the `Top of Personal Folders` folder and find that it
    also has subfolders.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the `folder_traverse()` function again on each of the subfolders, we
    first process the Deleted Items folder on level **3**. Inside the `Deleted Items`
    folder on level 4, we find that we only have messages in this folder and call
    the `check_for_msgs()` function for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: After the `check_for_msgs()` function returns, we go back to the previous call
    of the `folder_traverse()` function on level 3 and evaluate the `Sent Items` folder.
    Since the `Sent Items` folder also doesn't have any subfolders, we process its
    messages before returning to level 3.
  prefs: []
  type: TYPE_NORMAL
- en: We then reach the `Inbox` folder on level 3 and call the `folder_traverse()`
    function on the `Completed Cases` subfolder on level 4\. Now that we're in level
    5, we process the two messages inside the `Completed Cases` folder. With these
    two messages processed, we step back to level 4 and process the two messages within
    the `Inbox` folder. Once these messages are processed, we've completed all items
    in levels 3, 4, and 5 and can finally move back to level 2\. Within `Root Folder`,
    we can process the three message items there before the function execution concludes.
    Our recursion, in this case, works from the bottom up.
  prefs: []
  type: TYPE_NORMAL
- en: These four lines of code allow us to navigate through the entire PST and call
    additional processing on every message in every folder. Though this is usually
    provided to us through methods such as `os.walk()`, some libraries don't natively
    support recursion and require the developer to do so using the existing functionality
    within the library.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying messages with the check_for_msgs() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is called for every discovered folder in the `folder_traverse()`
    function and handles the processing of messages. On line 110, we log the name
    of the folder to provide a record of what has been processed. Following this,
    we create a list to append messages on line 111 and begin iterating through the
    messages in the folder on line 112.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this loop, we call the `process_msg()` function to extract the relevant
    fields into a dictionary. After each message dictionary has been appended to the
    list, we call the `folder_report()` function, which will create a summary report
    of all of the messages within the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Processing messages in the process_msg() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is called the most as it runs for every discovered message. When
    you're considering how to improve the efficiency of your code base, these are
    the types of functions to look at. Even a minor efficiency improvement in function
    that're called frequently can have a large effect on your script.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the function is simple and exists mainly to remove clutter from
    another function. Additionally, it compartmentalizes message processing within
    a single function and will make it easier to troubleshoot bugs associated with
    message processing.
  prefs: []
  type: TYPE_NORMAL
- en: The return statement on line 126 passes a dictionary to the calling function.
    This dictionary contains a key-value pair for each of the `pypff.message` object
    attributes. Note that the `subject`, `sender`, `transport_headers`, and `plain_text_body`
    attributes are strings. The `creation_time`, `client_submit_time`, and `delivery_time`
    attributes are Python `datetime.datetime` objects and the `number_of_attachments`
    attribute is an integer.
  prefs: []
  type: TYPE_NORMAL
- en: The subject attribute contains the subject line found within the message and
    `sender_name` contains a single string of the name of the sender who sent the
    message. The sender name might reflect an email address or the contact name depending
    on whether the recipient resolved the name.
  prefs: []
  type: TYPE_NORMAL
- en: The `transport_headers` contains the email header data transmitted with any
    message. This data should be read from the bottom up, as new data is added to
    the top of the header as a message moves between mail servers. We can use this
    information to possibly track the movement of a message using hostnames and IP
    addresses. The `plain_text_body` attribute returns the body as plain text, though
    we could display the message in RTF or HTML format using the `rtf_body` and `html_body`
    attributes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `creation_times` and `delivery_times` are reflective of the creation of
    the message and delivery of a received message to the PST being examined. The
    `client_submit_time` value is the timestamp for when the message was sent. The
    last attribute shown here is the `number_of_attachments` attribute, which finds
    additional artifacts for extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: At this time, the `pypff` module doesn't support interaction with attachments,
    although the `libpff` library will extract artifacts using its `pffexport` and
    `pffinfo` tools. To build these tools, we must include the `--enable-static-executables`
    argument on the command line when running the `./configure` command while building.
  prefs: []
  type: TYPE_NORMAL
- en: Once built with these options, we can run the tools mentioned earlier to export
    the PST attachments in a structured directory. The developer has stated that he'll
    include `pypff` support for attachments in a future release. If made available,
    we'll be able to interface with message attachments and run additional processing
    on discovered files. If this functionality is needed for analysis, we could add
    support to call the `pffexport` tool via Python through the `os` or `subprocess`
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing data in the folder_report() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we've collected a fair amount of information about messages and
    folders. We use this code block to export that data into a simple report for review.
    To create this report, we require the `message_list` and `folder_name` variables.
    On line 146, we check whether there're any entries in the `message_list`; if not,
    we log a warning and return the function to prevent any of the remaining code
    from running.
  prefs: []
  type: TYPE_NORMAL
- en: If the `message_list` has content, we start to create a CSV report. We first
    generate the filename in the output directory by passing our desired filename
    into the `make_path()` function to get the absolute path of the file that we wish
    to write to. Using this file path, we open the file in `wb` mode to write our
    CSV file and to prevent a bug that would add an extra line between the rows of
    our reports on line 152\. In the following line, we define the list of headers
    for the output document.
  prefs: []
  type: TYPE_NORMAL
- en: This list should reflect an ordered list of columns we wish to report. Feel
    free to modify lines 153 and 154 to reflect a preferred order or additional rows.
    All of the additional rows must be valid keys from all dictionaries within the
    `message_list` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following our headers, we initiate the `csv.DictWriter` class on line 155\.
    If you recall from the start of our script, we imported the `unicodecsv` library
    to handle Unicode characters when writing to a CSV. During this import, we used
    the `as` keyword to rename the module from `unicodecsv` to `csv` within our script.
    This module provides the same methods as the standard library, so we can continue
    using the familiar function calls we have seen with the `csv` library. In this
    initialization of `DictWriter()`, we pass along the open file object, the field
    names, and an argument to tell the class what to do with unused information within
    the `message_list` dictionaries. Since we''re not using all of the keys within
    the dictionaries in the `message_list` list, we need to tell the `DictWriter()`
    class that we would like to ignore these values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With the `csv_fout` variable initialized and configured, we can begin writing
    our header data using the `writeheaders()` method call on line 157\. Next, we
    write the dictionary fields of interest to the file using the `writerows()` method.
    Upon writing all the rows, we close the `fout` file to write it to disk and release
    the handle on the object as seen on line 159.
  prefs: []
  type: TYPE_NORMAL
- en: On lines 119 through 141, we prepare the dictionaries from the `message_list`
    for use in generating HTML report statistics. We need to invoke the `global` statement
    as seen on line 162 to allow us to edit the `date_list` global variable. We then
    open two text files to record a raw list of all of the body content and sender
    names. These files will be used in a later section to generate our statistics
    and allow the collection of this data in a manner that doesn't consume large amounts
    of memory. These two text files, seen on lines 163 and 164, are opened in the
    `a` mode, which will create the file if it doesn't exist or append the data to
    the end of the file if it exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 165, we start a `for` loop to iterate through each message, `m`, in
    `message_list`. If the message body key has a value, then we write the value to
    the output file with two line breaks to separate this content. Following this,
    on lines 168 and 169, we perform a similar process on the sender key and its value.
    In this instance, we''ll only use one line break so that we can iterate through
    it easier in a later function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After collecting the message content and senders, we accumulate the date information.
    To generate our heat map, we'll combine all three dates of activity into a single
    count to form a single chart. After checking that a valid date value is available,
    we gather the day of the week to determine which of the dictionaries within the
    `date_list` list we wish to update.
  prefs: []
  type: TYPE_NORMAL
- en: The Python `datetime.datetime` library has a `weekday()` method and an `.hour`
    attribute, which allows us to access the values as integers and handles the messy
    conversions for us. The `weekday()` method returns an integer from 0 to 6, where
    0 represents Monday and 6 represents Sunday. The `.hour` attribute returns an
    integer between 0 and 23, representing time in a 24-hour fashion, though the JavaScript
    we're using for the heat map requires an integer between 1 and 24 to process correctly.
    Because of this, we add 1 to each of the hour values as seen on lines 175, 181,
    and 187.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the correct weekday and time of day keys we need to update the
    value in the `date_list`. Upon completing the loop, we can close the two file
    objects on lines 189 and 190:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the word_stats() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the message content written to a file, we can now use it to calculate a
    frequency of word usage. We use the `Counter` module we imported from the collections
    library to generate a word count in an efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize the `word_list` as a `Counter()` object, which allows us to call
    it and assign new words while keeping track of the overall count per word. After
    initialization, we start a `for` loop on line 200, open the file, and iterate
    through each line with the `readlines()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we need to `split()` the line into a list of individual words
    in order to generate a proper count. By not passing an argument to `split()`,
    we'll split on all whitespace characters, which, in this case, works to our advantage.
    Following the split on line 201, we use a conditional statement to ensure only
    a single word greater than four characters is included in our list, to eliminate
    common filler words or symbols. This logic may be tailored based on your environment,
    as you may, for example, wish to include words shorter than four letters or some
    other filtering criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the conditional evaluates to true, we add the word to our counter. On line
    204, we increment the value of the word in the list by one. After iterating through
    every line and word of the `message_body.txt` file, we pass this word list to
    the `word_report()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Creating the word_report() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once `word_list` is passed from the `word_stats()` function, we can generate
    our reports using the supplied data. In order to have more control over how our
    data is presented, we''re going to write a CSV report without the help of the
    `csv` module. First, on line 216, we need to ensure that `word_list` contains
    values. If it doesn''t, the function logs a warning and returns. On line 220,
    we open a new file object in `wb` mode to create our CSV report. On line 221,
    we write our `Count` and `Word` headers onto the first row with a newline character
    to ensure all other data is written in the rows below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use a `for` loop and the `most_common()` method to call out a tuple
    containing each word and the assigned count value. If the length of the tuple
    is greater than 1, we write the values into the CSV document in reverse order
    to properly align the columns with the values, followed by a newline character.
    After this loop completes, we close the file and flush the results to the disk
    as seen on line 225:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Following this loop, we then generate a list of the top 10 words. By passing
    the integer 10 into the `most_common()` method, we select only the top 10 most
    common entries in `Counter`. We append a dictionary of the results to a temporary
    list, which is returned to the `word_stats()` function and later used in our HTML
    report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Building the sender_report() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sender_report()` functions similarly to `word_report()` and generates a
    CSV and HTML report for individuals who sent emails. This function showcases another
    method for reading values into the `Counter()` method. On line 242, we open and
    read the lines of a file into the `Counter()` method.
  prefs: []
  type: TYPE_NORMAL
- en: We can implement it this way because each line of the input file represents
    a single sender. Counting the data in this manner simplifies the code and, by
    extension, saves us a few lines of writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This wasn''t a feasible option for the `word_stats()` function because we had
    to break each line into a separate word and then perform additional logic operations
    prior to counting the words. If we wanted to apply logic to the sender statistics,
    we would need to create a similar loop to that in `word_stats()`. For example,
    we might want to exclude all items from Gmail or that contain the word `noreply`
    in the sender''s name or address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After generating the sender count, we can open the CSV report and write our
    headers to it. At this point, we'll iterate through each of the most common in
    a `for` loop as seen on line 247, and if the tuple contains more than one element,
    we'll write it to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is another location where we could filter the values based on the sender''s
    name. After writing, the file is closed and flushed to the disk. On line 252,
    we generate statistics for the top five senders for the final report by generating
    a list of dictionaries containing the tuple values. To access it in our HTML report
    function, we return this list. See the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Refining the heat map with the date_report() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This report provides data to generate the activity heat map. For it to operate
    properly, it must have the same filename and path specified in the HTML template.
    The default template for the file is named `heatmap.tsv` and is located in the
    same directory as the output HTML report.
  prefs: []
  type: TYPE_NORMAL
- en: After opening this file with those defaults on line 267, we write the headers
    with a tab character delimiting the day, hour, and value columns and ending with
    a newline character. At this point, we can begin iterating through our list of
    dictionaries by using two `for` loops to access each list containing dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first `for` loop, we use the `enumerate()` method to capture the loop
    iteration number. This number conveniently corresponds to the date we''re processing,
    allowing us to use this value to write the day value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the second `for` loop, we iterate through each dictionary, gathering both
    the hour and count values separately by using the `items()` method to extract
    the key and value as a tuple. With these values, we can now assign the date, hour,
    and count to a tab-separated string and write it to the file.
  prefs: []
  type: TYPE_NORMAL
- en: On line 271, we add 1 to the date value as the heat map chart uses a 1 through
    7 range, whereas our list uses an index of 0 through 6 to count days of the week.
  prefs: []
  type: TYPE_NORMAL
- en: 'After iterating through the hours, we flush the data to the disk before moving
    forward to the next dictionary of hours. Once we''ve iterated through all of the
    seven days, we can close this document as it''s ready to be used with our heat
    map chart in the `html_report()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Writing the html_report() function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `html_report()` function is where we tie together all of the pieces of
    information gathered from the PST into a final report, with much anticipation.
    To generate this report, we require arguments specifying the report title, PST
    name, and counts of the top words and senders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: To begin with, we open the template file and read in the contents into a single
    variable as a string. This value is then passed into our `jinja2.Template` engine
    to be processed into a template object called `html_template` on line 290.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a dictionary of values to pass into the template's placeholders and
    use the `context` dictionary on line 292 to hold these values. With the dictionary
    in place, we then render the template on line 295 and provide the `context` dictionary.
    This rendered data is a string of HTML data, as you expect to see on a web page,
    with all of our placeholder logic evaluated and turned into a static HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: 'We write the rendered HTML data to an output file in the user-specified directory
    as seen on lines 297 through 299\. With the HTML report written to the output
    directory, the report is complete and ready to view in the output folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The HTML template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book focuses on the use of Python in forensics. Though Python provides
    many great methods for manipulating and applying logic to data, we still need
    to lean on other resources to support our scripts. In this chapter, we've built
    an HTML dashboard to present statistical information about these PST files.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll review sections of HTML, focusing on where our data is
    inserted into the template versus the intricacies of HTML, JavaScript, and other
    web languages. For more information in the use and implementation of HTML, JavaScript,
    D3.js, and other web resources, visit [http://packtpub.com](http://packtpub.com)
    for pertinent titles or [http://w3schools.com](http://w3schools.com) for introductory
    tutorials. Since we'll not be delving deeply into HTML, CSS, or other web design aspects,
    our focus will be primarily on the spaces where our Python script will interact.
  prefs: []
  type: TYPE_NORMAL
- en: This template leverages a couple of common frameworks that allow the rapid design
    of professional-looking web pages. The first is Bootstrap 3, a CSS styling framework
    that organizes and styles HTML to look uniform and clean no matter the device
    used to view the page. The second is the D3.js framework, which is a JavaScript
    framework for graphic visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen before, the template items into which we''ll insert our data
    are contained within double braces, `{{ }}`. We''ll insert the report title for
    our HTML dashboard on line 39 and 44\. Additionally, we''ll insert the name of
    the PST file on lines 48, 55, and 62\. The `div id` tags on lines 51, 58, and
    65 acts as a variable name for the charts that can be inserted by JavaScript in
    the later section of the template once the code processes the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `div` placeholder elements are in place, the JavaScript on lines
    69 through 305 processes the provided data into charts. The first location data
    is placed on line 92, where the `{{ word_frequency }}` phrase is replaced with
    the list of dictionaries. For example, this could be replaced with `[{''count'':
    ''175'', ''word'': ''message''}, {''count'': ''17'', ''word'': ''iPhone''}]`.
    This list of dictionaries is translated into chart values to form the vertical
    bar chart of the HTML report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 132, we insert the `percentage_by_sender` value from the context dictionary
    into the JavaScript. This replacement will occur in a similar example to the `word_frequency`
    insert. With this information, the donut chart generates on the HTML report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We'll use a new way to insert data for the heat map. By providing the filename
    discussed in the previous section, we can prompt the code to look for a `heatmap.tsv`
    file in the same directory as this HTML report. The upside to this is how we're
    able to generate a report once and use the TSV in a program such as Excel and
    within our dashboard, though the downside is that this file must travel with the
    HTML report for it to display properly, as the chart will regenerate on reload.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chart also has difficulty rendering on some browsers as JavaScript is
    interpreted differently by each browser. Testing found that Chrome, Firefox, and
    Safari were OK at viewing the graphic. Ensure that browser add-ons are not interfering
    with the JavaScript and that your browser doesn''t block JavaScript from interacting
    with local files. If your browser disallows this, consider running the script
    in the Docker instance, starting the `lighttpd` service, and placing your output
    in `/var/www/html`. When you visit the IP address of your Docker instance, you''ll
    be able to navigate to the report as the server will provide access to the resources
    for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The remainder of the template is available in the code repository and can easily
    be referenced and manipulated if web languages are your strong suit or worth further
    exploration. The D3.js library allows us to create additional informative graphics
    and adds another tool to our reporting toolbox that's relatively simple and portable.
    The following graphics represent examples of data for each of the three charts
    we've created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first chart represents the most used words in the PST file. The frequency
    is plotted on the *y* axis and the word on the *x* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea070c05-142b-467e-bee8-10ebe5b799bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following chart identifies the top five accounts that have sent an email
    to the user. Notice how the circle graph helps to identify which participants
    are most frequent in the dataset. In addition, the text labels provide the name
    of the address and the number of emails received by that address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b19b6e-c27f-47f5-a634-d963ffb450d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Lastly, the following heat map aggregates all emails into hour-long cells for
    each day. This is very useful in identifying trends in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in this case, we can quickly identify that most emails are sent
    or received early in the morning and particularly at 6 AM on Tuesdays. The bar
    at the bottom of the graphic indicates the number of emails. For example, the
    color of the cell for 6 AM Tuesdays indicates that more than 1,896 emails were
    sent or received during that time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17140a11-ba74-4359-8322-064e7b07c3f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our code complete, both the script and the HTML template, we're ready to
    execute the code! In our Ubuntu environment, we'll need to run the following command
    and provide our PST for analysis. If your Ubuntu machine has a configured web
    server, then the output could be placed in the web directory and served as a website
    for other users to view when visiting the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan on using the Docker container method to run this code, you''ll
    need to copy the PST file into your container using a command such as the one
    shown in the following. Please note that the following syntax  is `docker cp src_file
    container_name:/path/on/container` and additional functionality is described with
    `docker cp --help`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our PST is located within our container; we can run our script as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24182530-5859-41b7-a0f5-4ecf4ea41e9b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows us using `/var/www/html` as our output directory.
    This means that if we're running the `lighttpd` service on our Docker container,
    we'll be able to browse to the container's IP address and view the content in
    a browser on our system. You'll need to run `docker container ls pst_parser` to
    get the correct port that the web server can be found at.
  prefs: []
  type: TYPE_NORMAL
- en: Additional challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we invite you to implement some improvements that will make
    our script more versatile. As mentioned earlier in the chapter, `pypff` currently
    doesn't natively support the extraction or direct interaction with attachments.
    We can, however, call the `pffexport` and `pffinfo` tools within our Python script
    to do so. We recommend looking at the subprocess module to accomplish this. To
    extend this further, how can we connect this with the code covered in previous
    chapter ? What type of data might become available once we have access to attachments?
  prefs: []
  type: TYPE_NORMAL
- en: Consider methods that would allow a user to provide filtering options to collect
    specific messages of interest rather than the entire PST. A library that may assist
    in providing additional configuration options to the user is `ConfigParser` and
    can be installed with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, another challenge would be seeing improvements in the HTML report by
    adding additional charts and graphs. One example might be to parse `transit_headers`
    and extract the IP addresses. Using these IP addresses, you could geolocate them
    and plot them on a map with the D3.js library. This kind of information can increase
    the usefulness of our reports by squeezing out as much information as possible
    from all potential data points.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Email files contain a large amount of valuable information, allowing forensic
    examiners to gain greater insight into communications and the activity of users
    over time. Using open source libraries, we're able to explore PST files and extract
    information about the messages and folders within. We also examined the content
    and metadata of messages to gather additional information about frequent contacts,
    common words, and abnormal hot spots of activity. Through this automated process,
    we can gather a better understanding of the data we review and begin to identify
    hidden trends. The code for this project can be downloaded from GitHub or Packt,
    as described in the *Preface*.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying hidden information is very important in all investigations and is
    one of the many reasons that data recovery is an important cornerstone in the
    forensic investigation process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll cover how to recover data from a difficult source,
    databases. Using several Python libraries, we'll be able to recover data that
    might otherwise be lost and gain valuable insights into records that're no longer
    tracked by the database.
  prefs: []
  type: TYPE_NORMAL
