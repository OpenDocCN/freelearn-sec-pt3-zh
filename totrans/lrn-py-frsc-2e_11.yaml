- en: Parsing Outlook PST Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析Outlook PST容器
- en: '**Electronic mail** (**email**) continues to be one of the most common methods
    of communication in the workplace, surviving the number of new communication services
    present in today''s world. Emails can be sent from computers, websites, and the
    phones that''re in so many pockets across the globe. This medium allows for the
    transmission of information in the form of text, HTML, attachments, and more in
    a reliable fashion. It''s no wonder, then, that emails can play a large part in
    investigations, especially for cases involving the workplace. In this chapter,
    we''re going to work with a common email format, **Personal Storage Table** (**PST**),
    used by Microsoft Outlook to store email content in a single file.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**电子邮件**（**email**）继续是工作场所中最常见的通信方式之一，在当今世界的新通信服务中生存下来。电子邮件可以从计算机、网站和遍布全球口袋的手机发送。这种媒介可以可靠地以文本、HTML、附件等形式传输信息。因此，毫不奇怪，电子邮件在特别是涉及工作场所的调查中扮演了重要角色。在本章中，我们将处理一种常见的电子邮件格式，**个人存储表**（**PST**），由Microsoft
    Outlook用于将电子邮件内容存储在单个文件中。'
- en: The script we'll develop in this chapter introduces us to a series of operations
    available through the `libpff` library developed by Joachim Metz. This library
    allows us to open PST file and explore its contents in a Pythonic manner. Additionally,
    the code we build demonstrates how to create dynamic, HTML-based, graphics to
    provide additional context to spreadsheet-based reports. For these reports, we'll
    leverage the Jinja2 module, introduced in [Chapter 5](a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml),
    *Databases in Python*, and the D3.js framework to generate our dynamic HTML-based
    charts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中开发的脚本介绍了一系列通过Joachim Metz开发的`libpff`库可用的操作。这个库允许我们以Pythonic方式打开PST文件并探索其内容。此外，我们构建的代码演示了如何创建动态的基于HTML的图形，以提供电子表格报告的附加背景。对于这些报告，我们将利用第5章中引入的Jinja2模块，*Python中的数据库*，以及D3.js框架来生成我们的动态基于HTML的图表。
- en: The D3.js project is a JavaScript framework that allows us to design informative
    and dynamic charts without much effort. The charts used in this chapter are open
    source examples of the framework shared with the community at [https://github.com/d3/d3](https://github.com/d3/d3).
    Since this book doesn't focus on JavaScript, nor does it introduce the language,
    we won't cover the implementation details to create these charts. Instead, we'll
    demonstrate how to add our Python results to a pre-existing template.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: D3.js项目是一个JavaScript框架，允许我们设计信息丰富且动态的图表而不需要太多努力。本章使用的图表是框架的开源示例，与社区共享在[https://github.com/d3/d3](https://github.com/d3/d3)。由于本书不专注于JavaScript，也不介绍该语言，因此我们不会详细介绍创建这些图表的实现细节。相反，我们将演示如何将我们的Python结果添加到预先存在的模板中。
- en: Finally, we'll use a sample PST file, which has a large variety of data across
    time, to test our script. As always, we recommend running any code against test
    files before using it in casework to validate the logic and feature coverage.
    The library used in this chapter is in active development and is labeled experimental
    by the developer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用一个示例PST文件，该文件跨时间包含大量数据，用于测试我们的脚本。与往常一样，我们建议在案件中使用任何代码之前针对测试文件运行以验证逻辑和功能覆盖范围。本章使用的库处于活跃开发状态，并由开发者标记为实验性。
- en: 'The following are the topics covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Understanding the background of PST files
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PST文件的背景
- en: Leveraging `libpff` and its Python bindings, `pypff`, to parse PST files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用`libpff`及其Python绑定`pypff`来解析PST文件
- en: Creating informative and professional charts using Jinja2 and D3.js
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用Jinja2和D3.js创建信息丰富且专业的图表
- en: The code for this chapter is developed and tested using Python 2.7.15.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码是使用Python 2.7.15开发和测试的。
- en: The PST file format
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PST文件格式
- en: 'The PST format is a type of **Personal File Format** (**PFF**). Two other types
    of PFF file include the **Personal Address Book** (**PAB**) for storing contacts
    and the **Offline Storage Table** (**OST**), which stores offline email, calendars,
    and tasks. By default, Outlook stores cached email information in OST files, which
    can be found at the locations specified in the following table. Items in Outlook
    will be stored in a PST file if archived:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PST格式是一种**个人文件格式**（**PFF**）的类型。PFF文件的另外两种类型包括用于存储联系人的**个人通讯录**（**PAB**）和存储离线电子邮件、日历和任务的**脱机存储表**（**OST**）。默认情况下，Outlook将缓存的电子邮件信息存储在OST文件中，这些文件可以在下表中指定的位置找到。如果归档了Outlook中的项目，它们将存储在PST文件中：
- en: '| **Windows version** | **Outlook version** | **OST location** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | Outlook 2000/2003/2007 | `C:\Documents and Settings\USERPROFILE%\Local
    Settings\Application Data\Microsoft\Outlook` |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| Windows Vista/7/8 | Outlook 2007 | `C:\Users\%USERPROFILE%\AppData\Local\Microsoft\Outlook`
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | Outlook 2010 | `C:Documents and Settings\%USERPROFILE%\My Documents\Outlook
    Files` |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| Windows Vista/7/8 | Outlook 2010/2013 | `C:\Users\%USERPROFILE%\Documents\Outlook
    Files`  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: 'From: [https://forensicswiki.org/wiki/Personal_Folder_File_(PAB,_PST,_OST)](https://forensicswiki.org/wiki/Personal_Folder_File_(PAB,_PST,_OST)).
    Location of OST files by default.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The `%USERPROFILE%` field is dynamic and replaced with the user account name
    on the machine. PFF files can be identified through the hex file signature of
    `0x2142444E` or `!BDN` in ASCII. After the file signature, the type of PFF file
    is denoted by 2 bytes at offset 8:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Hex signature** | **ASCII signature** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| PST | 534D | SM |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| OST | 534F | SO |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| PAB | 4142 | AB |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: From http://www.garykessler.net/library/file_sigs.html
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The content type (such as 32-bit or 64-bit) is defined at byte offset 10\.
    The structure of the PFF file format has been described in detail by Joachim Metz
    in several papers that document the technical structure and how to manually parse
    these files on GitHub at the project''s code repository: [https://github.com/libyal/libpff](https://github.com/libyal/libpff).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll work only with PST files and we can ignore the differences
    in OST and PAB files. By default, PST archives have a root area containing a series
    of folders and messages depending on how the archives were created. For example,
    a user may archive all folders in their view or only a select few. All of the
    items within the selected content will be exported into the PST file.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to users archiving content, Outlook has an automatic archiving
    feature that will store items in the PST files after a set time as defined in
    the following table. Once this expiration period has been reached, the items will
    be included in the next archive created. The automatic archive stores PSTs by
    default in `%USERPROFILE%\Documents\Outlook` in Windows 7, `%APPDATA%\Local\Microsoft\Outlook`
    in Vista, and `%APPDATA%\Local Settings\Microsoft\Outlook` in XP. These defaults
    could be set by the user or by group policy in a domain environment. This automatic
    archive functionality provides examiners with a great history of communication
    information that we can access and interpret in our investigations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '| **Folder** | **Default aging period** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Inbox and Drafts | 6 months |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| Sent Items and Deleted Items | 2 months |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| Outbox | 3 months |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| Calendar | 6 months |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| Tasks | 6 months |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| Notes | 6 months |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| Journal | 6 months |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: 'Table 11.1: Default aging of Outlook items (https://support.office.com/en-us/article/Automatically-move-or-delete-older-items-with-AutoArchive-e5ce650b-d129-49c3-898f-9cd517d79f8e)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to libpff
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: libpff简介
- en: The `libpff` library allows us to reference and navigate through PST objects
    in a programmatic manner. The `root_folder()` function allows us to reference
    `RootFolder`, which is the base of the PST file and the starting point for our
    recursive analysis of email content. Within `RootFolder` are folders and messages.
    The folders can contain other sub-folders or messages. Folders have properties
    that include the name of the folder, the number of subfolders, and the number
    of submessages. Messages are objects representing messages and have attributes,
    including the subject line, the name of all participants, and several timestamps.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`libpff`库允许我们以编程方式引用和浏览PST对象。`root_folder()`函数允许我们引用`RootFolder`，它是PST文件的基础，也是我们递归分析电子邮件内容的起点。在`RootFolder`中包含文件夹和消息。文件夹可以包含其他子文件夹或消息。文件夹有一些属性，包括文件夹名称、子文件夹数量和子消息数量。消息是表示消息的对象，并具有包括主题行、所有参与者的名称以及若干时间戳等属性。'
- en: How to install libpff and pypff
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何安装libpff和pypff
- en: Installing some third-party libraries is more difficult than running `pip install
    <library_name>`. In the case of `libpff` and the `pypff` bindings, we need to
    take a few steps and follow the instructions outlined in the GitHub project repository.
    The `libpff` wiki (located at [https://github.com/libyal/libpff/wiki/Building](https://github.com/libyal/libpff/wiki/Building))
    describes the steps we need to take in order to build `libpff`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 安装一些第三方库比运行`pip install <library_name>`更为复杂。在`libpff`和`pypff`绑定的情况下，我们需要采取一些步骤并遵循GitHub项目仓库中列出的指示。`libpff`的wiki（位于[https://github.com/libyal/libpff/wiki/Building](https://github.com/libyal/libpff/wiki/Building)）描述了我们需要采取的步骤来构建`libpff`。
- en: 'We''ll briefly walk through how you would build this library on an Ubuntu 18.04
    system. After downloading and installing Ubuntu 18.04 (preferably in a virtual
    machine), you''ll want to install the dependencies by running the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍如何在Ubuntu 18.04系统上构建这个库。在下载并安装Ubuntu 18.04（最好是在虚拟机中）后，你需要通过运行以下命令来安装依赖项：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will install the required packages for both our script and the `pypff`
    bindings. We''ll then want to download our `libpff` code by running the following
    command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装我们脚本和`pypff`绑定所需的包。接下来，我们需要通过运行以下命令来下载`libpff`代码：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the `git clone` command completes, we''ll navigate into the new `libpff`
    directory and run the following commands to download additional dependencies,
    configure, and install the components we need for the library:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`git clone`命令完成，我们将进入新的`libpff`目录，并运行以下命令来下载其他依赖项，配置并安装我们需要的库组件：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Additional build options are described further on the `libpff` wiki page.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的构建选项在`libpff`的wiki页面中有更详细的描述。
- en: 'At this point, you should be able to run the following statements and get the
    same output, though your version may vary:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你应该能够运行以下语句并获得相同的输出，尽管你的版本可能会有所不同：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To make this process easier for you, we've prebuilt the `pypff` bindings and
    created a Dockerfile to run this entire setup for you. If you're unfamiliar with
    Docker, it's a virtualization environment that allows us to run virtual machines
    with minimal effort. While Docker is generally used to host applications, we'll
    use it more as a traditional virtual machine. What makes this advantageous for
    us is that we can distribute a configuration file that you can run on your system
    and generate the same environment that we've tested.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个过程，我们已经预构建了`pypff`绑定，并创建了一个Dockerfile来为你运行整个设置。如果你不熟悉Docker，它是一个虚拟化环境，可以让我们以最小的努力运行虚拟机。虽然Docker通常用于托管应用程序，但我们将更多地将它作为传统的虚拟机使用。对我们来说，这种方式的优势在于，我们可以分发一个配置文件，你可以在系统上运行它，从而生成与我们测试的环境相同的环境。
- en: 'To begin, please follow the instructions to install Docker on your system from [https://docs.docker.com/install/](https://docs.docker.com/install/).
    Once installed and running, navigate to the `Chapter 11` code folder on your system
    and run the `docker build` command. This command will generate a system following
    a series of preconfigured steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请按照[https://docs.docker.com/install/](https://docs.docker.com/install/)上的说明在你的系统上安装Docker。安装并运行后，导航到你系统上的`Chapter
    11`代码文件夹，并运行`docker build`命令。该命令将根据一系列预配置的步骤生成一个系统：
- en: '![](img/e560c17a-8832-4e57-bb38-a5163ddd3b28.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e560c17a-8832-4e57-bb38-a5163ddd3b28.png)'
- en: 'This will create a new image named `lpff-ch11` with the version number 20181130\.
    An image in Docker is what it sounds like: a base installation that you can use
    to create running machines. This way you can have multiple machines all based
    on the same image. Each machine is called a container, and to create a container
    from this image, we''ll use the `docker run` statement:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`lpff-ch11`、版本号为20181130的新镜像。在Docker中，镜像就是它的字面意思：一个基本安装，您可以用它来创建运行中的机器。这样，您可以拥有多个基于相同镜像的机器。每个机器称为容器，为了从这个镜像创建容器，我们将使用`docker
    run`语句：
- en: '![](img/1e6dc4dd-69ab-4de0-a5ba-00efa49927a3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e6dc4dd-69ab-4de0-a5ba-00efa49927a3.png)'
- en: The `-it` flag in the `docker run` command asks Docker to connect to the bash
    shell once the container is created. The `-P` parameter asks Docker to provide
    us with networking to, in our case, the web server running on the container. Lastly,
    the `--name` argument allows us to assign a familiar name to our container. We
    then pass in the image name and version and run the command. As you can see, we're
    provided with a root shell as soon as the Docker instance finishes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run`命令中的`-it`标志要求Docker在创建容器后连接到bash shell。`-P`参数要求Docker为我们提供网络连接，在我们的案例中，就是运行在容器中的Web服务器。最后，`--name`参数允许我们为容器指定一个熟悉的名称。然后，我们传入镜像名称和版本并运行该命令。如你所见，一旦Docker实例完成，我们就会获得一个root
    shell。'
- en: Regarding the previously mentioned web server, we've included `lighttpd` to
    allow us to serve our HTML-generated report as a web page. This isn't necessary,
    though we wanted to highlight how these reports could be made accessible on an
    internal system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于之前提到的Web服务器，我们已经包含了`lighttpd`，以便我们能够将HTML生成的报告作为网页提供。这不是必需的，不过我们希望强调如何使这些报告在内部系统上可访问。
- en: Please don't run this Docker container on a public network as it'll allow anyone
    with access to your machine's IP address to see your HTML reports.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要在公共网络上运行此Docker容器，因为它将允许任何能够访问您机器IP地址的人查看您的HTML报告。
- en: In the preceding screenshot, we start this web server by running `server lighttpd
    start` and then list the contents of our current directory. As you can see, we
    have two files, our `pst_indexer.py` script that we're about to build and the
    `stats_template.html` that we'll use to generate our sharp report. Let's build
    our Python script.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们通过运行`server lighttpd start`启动了Web服务器，然后列出了当前目录的内容。如您所见，我们有两个文件，一个是我们即将构建的`pst_indexer.py`脚本，另一个是我们将用来生成报告的`stats_template.html`。现在让我们开始构建Python脚本。
- en: Exploring PSTs – pst_indexer.py
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索PST文件 – pst_indexer.py
- en: In this script, we'll harvest information about the PST file, taking note of
    the messages in each folder and generating statistics for word usage, frequent
    senders, and a heat map for all email activity. Using these metrics, we can go
    beyond the initial collection and reporting of messages and explore trends in
    the language used or communication patterns with certain individuals. The statistics
    section highlights examples of how we can utilize the raw data and build informative
    graphics to assist the examiner. We recommend tailoring the logic to your specific
    investigation to provide the most informative report possible. For example, for
    the word count, we'll only be looking at the top ten words that're alphanumeric
    and longer than four characters, to help reduce common words and symbols. This
    might not provide the correct information for your investigation and might require
    tailoring to your specific situation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们将收集PST文件的信息，记录每个文件夹中的邮件，并生成关于词汇使用、频繁发送者以及所有邮件活动的热图统计数据。通过这些指标，我们可以超越初步的邮件收集和报告，探索使用的语言趋势或与特定人员的沟通模式。统计部分展示了如何利用原始数据并构建信息图表以帮助审查员。我们建议根据您的具体调查定制逻辑，以提供尽可能有用的报告。例如，对于词汇统计，我们只查看字母数字且长度大于四个字符的前十个词汇，以减少常见的词汇和符号。这可能不适用于您的调查，可能需要根据您的具体情况进行调整。
- en: An overview
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: This chapter's script was built to work with Python 2.7.15 and requires the
    third-party libraries described in the previous section. Please consider using
    the Docker image alongside this script.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的脚本是为Python 2.7.15版本编写的，并且需要上一节中提到的第三方库。请考虑在使用此脚本时同时使用Docker镜像。
- en: As with our other chapters, this script starts by importing libraries we use
    at the top. In this chapter, we use two new libraries, one of which is third-party.
    We've already introduced `pypff`, the Python bindings to the `libpff` library.
    The `pypff` module specifies the Python bindings that allow us access to the compiled
    code. On line 8, we introduce `unicodecsv`, a third-party library we've used previously
    in [Chapter 5](a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml), *Databases in Python*.
    This library allows us to write Unicode characters to CSV files as the native
    CSV library doesn't support Unicode characters as nicely.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们其他章节一样，本脚本通过导入我们在顶部使用的库开始。在本章中，我们使用了两个新的库，其中一个是第三方库。我们之前已经介绍过 `pypff`，它是
    `libpff` 库的 Python 绑定。`pypff` 模块指定了允许我们访问已编译代码的 Python 绑定。在第8行，我们引入了 `unicodecsv`，这是一个我们在[第五章](a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml)《Python中的数据库》中曾使用过的第三方库。这个库允许我们将
    Unicode 字符写入 CSV 文件，因为原生的 CSV 库对 Unicode 字符的支持并不理想。
- en: 'On line 6, we import a standard library called `collections` that provides
    a series of useful interfaces including `Counter`. The `Counter` module allows
    us to provide values to it and it handles the logic of counting and storing objects.
    In addition to this, the collections library provides `OrderedDict`, which is
    extremely useful when you need to create a dictionary with keys in a specified
    order. The `OrderedDict` module isn''t leveraged in this book though it does have
    its place in Python when you wish to use key-value pairs in an ordered list-like
    fashion:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6行，我们导入了一个名为 `collections` 的标准库，它提供了一系列有用的接口，包括 `Counter`。`Counter` 模块允许我们向其提供值，并处理计数和存储对象的逻辑。除此之外，collections
    库还提供了 `OrderedDict`，当你需要按指定顺序创建键的字典时，它非常有用。尽管在本书中没有利用 `OrderedDict` 模块，但当你希望以有序的方式使用键值对时，它在
    Python 中确实有其用武之地：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following our license and script metadata, we''ll set up a few global variables.
    These variables will help us decrease the number of variables we must pass into
    functions. The first global variable is `output_directory`, defined on line 46,
    which will store a string path set by the user. The `date_dictionary`, defined
    on line 47, uses dictionary comprehension to create keys 1 through 24 and map
    them to the integer 0\. We then use list comprehension on line 48 to append seven
    instances of this dictionary to `date_list`. This list is leveraged to build a
    heat map to show information about activity within the PST file split within seven
    days'' worth of 24-hour columns:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在设定了许可和脚本元数据后，我们将设置一些全局变量。这些变量将帮助我们减少需要传递到函数中的变量数量。第一个全局变量是第46行定义的 `output_directory`，它将存储用户设置的字符串路径。第47行定义的
    `date_dictionary` 使用字典推导式创建了键 1 到 24，并将它们映射到整数 0。然后，我们在第48行使用列表推导式将这个字典的七个实例附加到
    `date_list`。这个列表被用来构建热图，显示在PST文件中按七天24小时列划分的活动信息：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This heat map will establish baseline trends and help identify anomalous activity.
    An example includes the ability to see a spike in activity at midnight on week
    nights or excessive activity on Wednesdays before the business day starts. The
    `date_list` has seven dictionaries, one for each day, each of which is identical
    and contains a key-value pair for the hour of the day with the default value of
    `0`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个热图将建立基线趋势，并帮助识别异常活动。例如，它可以显示在工作日午夜时段活动的激增，或者在星期三业务日开始前的过度活动。`date_list` 包含七个字典，每个字典代表一天，它们是完全相同的，包含一个小时的键值对，默认值为
    `0`。
- en: The `date_dict.copy()` call on line 48 is required to ensure that we can update
    the hours within a single date. If we omit the `copy()` method, every day will
    be updated. This is because dictionaries are tied together by references to the
    original object, and we're generating a list of objects without the `copy()` method.
    When we do use this function, it allows us to create a copy of the values with
    a new object, so we can create a list of different objects.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`date_dict.copy()` 在第48行的调用是必需的，以确保我们可以在单个日期内更新小时数。如果省略了 `copy()` 方法，所有的日期都会被更新。这是因为字典通过对原始对象的引用相互关联，而在没有使用
    `copy()` 方法的情况下，我们生成的是对象的引用列表。当我们使用此函数时，它允许我们通过创建一个新对象来复制值，从而可以创建不同对象的列表。'
- en: With these variables built, we can reference and update their values throughout
    other functions without needing to pass them again. Global variables are read-only
    by default and require a special global command in order to be modified by a function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了这些变量后，我们可以在其他函数中引用并更新它们的值，而不需要再次传递它们。全局变量默认是只读的，必须使用特殊的 `global` 命令才能在函数中进行修改。
- en: 'The following functions outline our script''s operation. As usual, we have
    our `main()` function to control behavior. The following is the `make_path()`
    function, which is a utility to assist us in gathering full paths for our output
    files. The `folder_traverse()` and `check_for_msgs()` functions are used to iterate
    through the available items and start processing:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数概述了我们脚本的操作。像往常一样，我们有`main()`函数来控制行为。接下来是`make_path()`函数，这是一个帮助我们收集输出文件完整路径的工具。`folder_traverse()`和`check_for_msgs()`函数用于迭代可用项并开始处理：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our remaining functions focus on processing and reporting data within PSTs.
    The `process_message()` function reads the message and returns the required attributes
    for our reports. The first reporting function is the `folder_report()` function.
    This code creates a CSV output for each folder found within the PST and describes
    the content found within each.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的其余函数专注于处理PST中的数据并生成报告。`process_message()`函数读取消息并返回报告所需的属性。第一个报告函数是`folder_report()`函数。此代码为PST中找到的每个文件夹创建CSV输出，并描述每个文件夹中的内容。
- en: This function also processes data for the remaining reports by writing message
    bodies to a single text file, stores each set of dates, and preserves a list of
    the senders. By caching this information to a text file, the next function is
    easily able to read the file without a major impact on memory.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数还通过将消息主体写入单一文本文件来处理其余报告的数据，存储每组日期，并保存发送者列表。通过将这些信息缓存到文本文件中，接下来的函数可以轻松读取文件，而不会对内存产生重大影响。
- en: 'Our `word_stats()` function reads and ingests the information into a collection.
    The `Counter()` object is used in our `word_report()` function. When generating
    our word count report, we read the collection''s. `Counter()` object into a CSV
    file, which  will be read by our JavaScript code. The `sender_report()` and `date_report()`
    functions also flush data to delimited files for interpretation by JavaScript
    in the report. Finally, our `html_report()` function opens our report template
    and writes the custom report information into an HTML file in our output folder:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`word_stats()`函数读取并将信息导入到一个集合中。`Counter()`对象在我们的`word_report()`函数中使用。当生成单词计数报告时，我们将集合的`Counter()`对象读取到CSV文件中，该文件将被我们的JavaScript代码读取。`sender_report()`和`date_report()`函数也将数据刷新到分隔文件中，供JavaScript在报告中进行解释。最后，我们的`html_report()`函数打开报告模板，并将自定义报告信息写入输出文件夹中的HTML文件：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As with all of our scripts, we handle our arguments, logs, and the `main()`
    function call under the `if __name__ == "__main__":` conditional statement on
    line 302\. We define the required arguments, `PST_FILE` and `OUTPUT_DIR`, and
    the user can specify optional arguments, `--title` and `-l`, for a custom report
    title and log path:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们所有的脚本一样，我们在第302行的`if __name__ == "__main__":`条件语句下处理参数、日志和`main()`函数调用。我们定义了必需的参数`PST_FILE`和`OUTPUT_DIR`，用户可以指定可选参数`--title`和`-l`，用于自定义报告标题和日志路径：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After defining our arguments, we begin processing them so that we can pass
    them to the `main()` function in a standardized and safe manner. On line 319,
    we convert the output location into an absolute path so that we can be sure about
    accessing the correct location throughout the script. Notice how we''re calling
    the `output_directory` global variable and assigning a new value to it. This is
    only possible because we''re not within a function. If we were modifying the global
    variable within a function, we would need to write `global output_directory` on
    line 318:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了我们的参数后，我们开始处理它们，以便以标准化和安全的方式将它们传递给`main()`函数。在第319行，我们将输出位置转换为绝对路径，以确保在脚本中访问正确的位置。注意，我们正在调用`output_directory`全局变量并为其分配一个新值。这只有在我们不在函数内时才可能。如果我们在函数内部修改全局变量，就需要在第318行写上`global
    output_directory`：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After we modify the `output_directory` variable, we make sure the path exists
    (and create it if it doesn''t) to avoid errors later in the code. Once complete,
    we then use our standard logging code snippet to configure logging for this script
    on lines 331 through 339\. On lines 341 through 345, we log debug information
    on the system executing the script prior to calling the `main()` function. On
    line 346, we call the `main()` function and pass the `args.PST_FILE` and `args.title`
    arguments. We don''t need to pass the `output_directory` value because we can
    reference it globally. Once we pass the arguments and the `main()` function completes
    execution, we log that the script has finished executing on line 347:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在修改 `output_directory` 变量后，我们确保路径存在（如果不存在，则创建），以避免后续代码出现错误。完成后，我们在第331到339行使用标准的日志记录代码片段来配置脚本的日志记录。在第341到345行，我们记录执行脚本的系统的调试信息，然后再调用
    `main()` 函数。在第346行，我们调用 `main()` 函数，并传入 `args.PST_FILE` 和 `args.title` 参数。我们无需传递
    `output_directory` 值，因为可以全局引用它。在传递参数并且 `main()` 函数执行完成后，我们在第347行记录脚本已完成执行。
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The following flowchart highlights how the functions interact with each other.
    This flowchart might seem a little complicated but encapsulates the basic structure
    of our script.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程图展示了各个函数之间的交互方式。这个流程图可能看起来有些复杂，但它概括了我们脚本的基本结构。
- en: 'The `main()` function calls the recursive `folder_traverse()` function, which
    in turn finds, processes, and summarizes messages and folders from the root folder.
    After this, the `main()` function generates reports with the word, sender, and
    date reports, which get displayed in one HTML report generated by the `html_report()`
    function. As a note, the dashed lines represent functions that return a value,
    while the solid lines represent a function that returns no value:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 函数调用递归的 `folder_traverse()` 函数，该函数依次查找、处理并汇总根文件夹中的消息和文件夹。之后，`main()`
    函数生成包含单词、发送者和日期的报告，并通过 `html_report()` 函数生成一个 HTML 报告进行显示。需要注意的是，虚线代表返回值的函数，而实线代表没有返回值的函数：'
- en: '![](img/41b0ceae-68da-4acd-9cec-2fb60a9fd531.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41b0ceae-68da-4acd-9cec-2fb60a9fd531.png)'
- en: Developing the main() function
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发 `main()` 函数
- en: The `main()` function controls the primary operations of the script, from opening
    and initial processing of the file, traversing the PST, to generating our reports.
    On line 62, we split the name of the PST file from its path using the `os.path`
    module.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 函数控制脚本的主要操作，从打开和初步处理文件、遍历 PST 文件，到生成报告。在第62行，我们使用 `os.path` 模块从路径中分离出
    PST 文件名。'
- en: 'We''ll use the `pst_name` variable if a custom title isn''t supplied by the
    user. On the next line, we use the `pypff.open()` function to create a PST object.
    We use the `get_root_folder()` method to get the PST root folder so we can begin
    the iteration process and discover items within the folders:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户没有提供自定义标题，我们将使用 `pst_name` 变量。在下一行，我们使用 `pypff.open()` 函数创建一个 PST 对象。通过
    `get_root_folder()` 方法获取 PST 的根文件夹，从而开始迭代过程，发现文件夹中的项：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With the root folder extracted, we call the `folder_traverse()` function on
    line 67 to begin traversing the directories within the PST container. We''ll cover
    the nature of this function in the next section. After traversing the folders,
    we start generating our reports with the `word_stats()`, `sender_report()`, and
    `date_report()` functions. On line 74, we pass the name of the report, the PST
    name, and lists containing the most frequent words and senders to provide statistical
    data for our HTML dashboard, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提取根文件夹后，我们在第67行调用 `folder_traverse()` 函数，开始遍历 PST 容器中的目录。我们将在下一部分讨论该函数的具体内容。遍历文件夹后，我们开始使用
    `word_stats()`、`sender_report()` 和 `date_report()` 函数生成报告。在第74行，我们传入报告名称、PST 名称以及包含最常见单词和发送者的列表，为
    HTML 仪表板提供统计数据，如下所示：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Evaluating the make_path() helper function
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 `make_path()` 辅助函数
- en: To make life simpler, we've developed a helper function, `make_path()`, defined
    on line 78\. Helper functions allow us to reuse code that we might normally write
    out many times throughout our script in one function call. With this code, we
    take an input string representing a file name and return the absolute path of
    where the file should exist within the operating system based on the `output_directory`
    value supplied by the user. On line 85, two operations take place; first, we join
    the `file_name` to the `output_directory` value with the correct path delimiters
    using the `os.path.join()` method.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，我们开发了一个辅助函数`make_path()`，定义在第78行。辅助函数允许我们在脚本中重复利用通常需要多次编写的代码，只需一次函数调用即可。通过这段代码，我们接受一个表示文件名的输入字符串，并根据用户提供的`output_directory`值返回文件在操作系统中的绝对路径。在第85行，进行了两项操作；首先，我们使用`os.path.join()`方法将`file_name`与`output_directory`值按正确的路径分隔符连接起来。
- en: 'Next, this value is processed by the `os.path.abspath()` method, which provides
    the full file path within the operating system environment. We then return this
    value to the function that originally called it. As we saw in the flow diagram,
    many functions will make calls to the `make_path()` function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这个值将通过`os.path.abspath()`方法进行处理，该方法提供操作系统环境中的完整文件路径。然后我们将此值返回给最初调用它的函数。如我们在流程图中所见，许多函数会调用`make_path()`函数：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Iteration with the folder_traverse() function
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`folder_traverse()`函数进行迭代
- en: 'This function recursively walks through folders to parse message items and
    indirectly generates summary reports on the folder. This function, initially provided
    the root directory, is generically developed to be capable of handling any folder
    item passed to it. This allows us to reuse the function for each discovered subfolder.
    On line 97, we use a `for` loop to recurse through the `sub_folders` iterator
    generated from our `pypff.folder` object. On line 98, we check whether the folder
    object has any additional subfolders and, if it does, call the `folder_traverse()`
    function again before checking the current folder for any new messages. We only
    check for messages in the event that there are no new subfolders:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数递归地遍历文件夹，以解析消息项，并间接地生成文件夹的摘要报告。该函数最初通过根目录提供，经过通用开发，可以处理传递给它的任何文件夹项。这使得我们可以在每次发现子文件夹时重用该函数。在第97行，我们使用`for`循环递归遍历从我们的`pypff.folder`对象生成的`sub_folders`迭代器。在第98行，我们检查文件夹对象是否有任何额外的子文件夹，如果有，则在检查当前文件夹中的新消息之前再次调用`folder_traverse()`函数。只有在没有新子文件夹的情况下，我们才会检查是否有新消息：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is a recursive function because we call the same function within itself
    (a loop of sorts). This loop could potentially run indefinitely, so we must make
    sure the data input will have an end to it. A PST should have a limited number
    of folders and will therefore eventually exit the recursive loop. This is essentially
    our PST specific `os.walk()` function, which iteratively walks through filesystem
    directories. Since we''re working with folders and messages within a file container,
    we have to create our own recursion. Recursion can be a tricky concept to understand;
    to guide you through it, please reference the following diagram when reading our
    explanation in the upcoming paragraphs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个递归函数，因为我们在函数内部调用了相同的函数（某种形式的循环）。这个循环可能会无限运行，因此我们必须确保数据输入有一个结束点。PST应该有有限数量的文件夹，因此最终会退出递归循环。这基本上是我们PST特定的`os.walk()`函数，它遍历文件系统目录。由于我们处理的是文件容器中的文件夹和消息，我们必须自己实现递归。递归可能是一个难以理解的概念；为了帮助你理解，在阅读接下来的解释时，请参考以下图示：
- en: '![](img/fdd3782c-1bd5-4025-a20d-94d7cefca744.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdd3782c-1bd5-4025-a20d-94d7cefca744.png)'
- en: In the preceding diagram, there're five levels in this PST hierarchy, each containing
    a mixture of blue folders and green messages. On level **1**, we have `Root Folder`,
    which is the first iteration of the `folder_traverse()` loop. Since this folder
    has a single subfolder, `Top of Personal Folders`, as you can see on level **2**,
    we rerun the function before exploring the message contents. When we rerun the
    function, we now evaluate the `Top of Personal Folders` folder and find that it
    also has subfolders.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，PST层次结构中有五个级别，每个级别包含蓝色文件夹和绿色消息的混合。在**第1**级，我们有`根文件夹`，这是`folder_traverse()`循环的第一次迭代。由于此文件夹有一个子文件夹`个人文件夹顶部`，如**第2**级所示，我们在探索消息内容之前重新运行该函数。当我们重新运行该函数时，我们现在评估`个人文件夹顶部`文件夹，并发现它也有子文件夹。
- en: Calling the `folder_traverse()` function again on each of the subfolders, we
    first process the Deleted Items folder on level **3**. Inside the `Deleted Items`
    folder on level 4, we find that we only have messages in this folder and call
    the `check_for_msgs()` function for the first time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: After the `check_for_msgs()` function returns, we go back to the previous call
    of the `folder_traverse()` function on level 3 and evaluate the `Sent Items` folder.
    Since the `Sent Items` folder also doesn't have any subfolders, we process its
    messages before returning to level 3.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: We then reach the `Inbox` folder on level 3 and call the `folder_traverse()`
    function on the `Completed Cases` subfolder on level 4\. Now that we're in level
    5, we process the two messages inside the `Completed Cases` folder. With these
    two messages processed, we step back to level 4 and process the two messages within
    the `Inbox` folder. Once these messages are processed, we've completed all items
    in levels 3, 4, and 5 and can finally move back to level 2\. Within `Root Folder`,
    we can process the three message items there before the function execution concludes.
    Our recursion, in this case, works from the bottom up.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: These four lines of code allow us to navigate through the entire PST and call
    additional processing on every message in every folder. Though this is usually
    provided to us through methods such as `os.walk()`, some libraries don't natively
    support recursion and require the developer to do so using the existing functionality
    within the library.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Identifying messages with the check_for_msgs() function
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is called for every discovered folder in the `folder_traverse()`
    function and handles the processing of messages. On line 110, we log the name
    of the folder to provide a record of what has been processed. Following this,
    we create a list to append messages on line 111 and begin iterating through the
    messages in the folder on line 112.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this loop, we call the `process_msg()` function to extract the relevant
    fields into a dictionary. After each message dictionary has been appended to the
    list, we call the `folder_report()` function, which will create a summary report
    of all of the messages within the folder:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Processing messages in the process_msg() function
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is called the most as it runs for every discovered message. When
    you're considering how to improve the efficiency of your code base, these are
    the types of functions to look at. Even a minor efficiency improvement in function
    that're called frequently can have a large effect on your script.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the function is simple and exists mainly to remove clutter from
    another function. Additionally, it compartmentalizes message processing within
    a single function and will make it easier to troubleshoot bugs associated with
    message processing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The return statement on line 126 passes a dictionary to the calling function.
    This dictionary contains a key-value pair for each of the `pypff.message` object
    attributes. Note that the `subject`, `sender`, `transport_headers`, and `plain_text_body`
    attributes are strings. The `creation_time`, `client_submit_time`, and `delivery_time`
    attributes are Python `datetime.datetime` objects and the `number_of_attachments`
    attribute is an integer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The subject attribute contains the subject line found within the message and
    `sender_name` contains a single string of the name of the sender who sent the
    message. The sender name might reflect an email address or the contact name depending
    on whether the recipient resolved the name.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The `transport_headers` contains the email header data transmitted with any
    message. This data should be read from the bottom up, as new data is added to
    the top of the header as a message moves between mail servers. We can use this
    information to possibly track the movement of a message using hostnames and IP
    addresses. The `plain_text_body` attribute returns the body as plain text, though
    we could display the message in RTF or HTML format using the `rtf_body` and `html_body`
    attributes, respectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The `creation_times` and `delivery_times` are reflective of the creation of
    the message and delivery of a received message to the PST being examined. The
    `client_submit_time` value is the timestamp for when the message was sent. The
    last attribute shown here is the `number_of_attachments` attribute, which finds
    additional artifacts for extraction:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: At this time, the `pypff` module doesn't support interaction with attachments,
    although the `libpff` library will extract artifacts using its `pffexport` and
    `pffinfo` tools. To build these tools, we must include the `--enable-static-executables`
    argument on the command line when running the `./configure` command while building.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Once built with these options, we can run the tools mentioned earlier to export
    the PST attachments in a structured directory. The developer has stated that he'll
    include `pypff` support for attachments in a future release. If made available,
    we'll be able to interface with message attachments and run additional processing
    on discovered files. If this functionality is needed for analysis, we could add
    support to call the `pffexport` tool via Python through the `os` or `subprocess`
    libraries.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing data in the folder_report() function
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we've collected a fair amount of information about messages and
    folders. We use this code block to export that data into a simple report for review.
    To create this report, we require the `message_list` and `folder_name` variables.
    On line 146, we check whether there're any entries in the `message_list`; if not,
    we log a warning and return the function to prevent any of the remaining code
    from running.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: If the `message_list` has content, we start to create a CSV report. We first
    generate the filename in the output directory by passing our desired filename
    into the `make_path()` function to get the absolute path of the file that we wish
    to write to. Using this file path, we open the file in `wb` mode to write our
    CSV file and to prevent a bug that would add an extra line between the rows of
    our reports on line 152\. In the following line, we define the list of headers
    for the output document.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: This list should reflect an ordered list of columns we wish to report. Feel
    free to modify lines 153 and 154 to reflect a preferred order or additional rows.
    All of the additional rows must be valid keys from all dictionaries within the
    `message_list` variable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Following our headers, we initiate the `csv.DictWriter` class on line 155\.
    If you recall from the start of our script, we imported the `unicodecsv` library
    to handle Unicode characters when writing to a CSV. During this import, we used
    the `as` keyword to rename the module from `unicodecsv` to `csv` within our script.
    This module provides the same methods as the standard library, so we can continue
    using the familiar function calls we have seen with the `csv` library. In this
    initialization of `DictWriter()`, we pass along the open file object, the field
    names, and an argument to tell the class what to do with unused information within
    the `message_list` dictionaries. Since we''re not using all of the keys within
    the dictionaries in the `message_list` list, we need to tell the `DictWriter()`
    class that we would like to ignore these values, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With the `csv_fout` variable initialized and configured, we can begin writing
    our header data using the `writeheaders()` method call on line 157\. Next, we
    write the dictionary fields of interest to the file using the `writerows()` method.
    Upon writing all the rows, we close the `fout` file to write it to disk and release
    the handle on the object as seen on line 159.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: On lines 119 through 141, we prepare the dictionaries from the `message_list`
    for use in generating HTML report statistics. We need to invoke the `global` statement
    as seen on line 162 to allow us to edit the `date_list` global variable. We then
    open two text files to record a raw list of all of the body content and sender
    names. These files will be used in a later section to generate our statistics
    and allow the collection of this data in a manner that doesn't consume large amounts
    of memory. These two text files, seen on lines 163 and 164, are opened in the
    `a` mode, which will create the file if it doesn't exist or append the data to
    the end of the file if it exists.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 165, we start a `for` loop to iterate through each message, `m`, in
    `message_list`. If the message body key has a value, then we write the value to
    the output file with two line breaks to separate this content. Following this,
    on lines 168 and 169, we perform a similar process on the sender key and its value.
    In this instance, we''ll only use one line break so that we can iterate through
    it easier in a later function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After collecting the message content and senders, we accumulate the date information.
    To generate our heat map, we'll combine all three dates of activity into a single
    count to form a single chart. After checking that a valid date value is available,
    we gather the day of the week to determine which of the dictionaries within the
    `date_list` list we wish to update.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The Python `datetime.datetime` library has a `weekday()` method and an `.hour`
    attribute, which allows us to access the values as integers and handles the messy
    conversions for us. The `weekday()` method returns an integer from 0 to 6, where
    0 represents Monday and 6 represents Sunday. The `.hour` attribute returns an
    integer between 0 and 23, representing time in a 24-hour fashion, though the JavaScript
    we're using for the heat map requires an integer between 1 and 24 to process correctly.
    Because of this, we add 1 to each of the hour values as seen on lines 175, 181,
    and 187.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the correct weekday and time of day keys we need to update the
    value in the `date_list`. Upon completing the loop, we can close the two file
    objects on lines 189 and 190:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Understanding the word_stats() function
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the message content written to a file, we can now use it to calculate a
    frequency of word usage. We use the `Counter` module we imported from the collections
    library to generate a word count in an efficient manner.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize the `word_list` as a `Counter()` object, which allows us to call
    it and assign new words while keeping track of the overall count per word. After
    initialization, we start a `for` loop on line 200, open the file, and iterate
    through each line with the `readlines()` method:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At this point, we need to `split()` the line into a list of individual words
    in order to generate a proper count. By not passing an argument to `split()`,
    we'll split on all whitespace characters, which, in this case, works to our advantage.
    Following the split on line 201, we use a conditional statement to ensure only
    a single word greater than four characters is included in our list, to eliminate
    common filler words or symbols. This logic may be tailored based on your environment,
    as you may, for example, wish to include words shorter than four letters or some
    other filtering criteria.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'If the conditional evaluates to true, we add the word to our counter. On line
    204, we increment the value of the word in the list by one. After iterating through
    every line and word of the `message_body.txt` file, we pass this word list to
    the `word_report()` function:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Creating the word_report() function
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once `word_list` is passed from the `word_stats()` function, we can generate
    our reports using the supplied data. In order to have more control over how our
    data is presented, we''re going to write a CSV report without the help of the
    `csv` module. First, on line 216, we need to ensure that `word_list` contains
    values. If it doesn''t, the function logs a warning and returns. On line 220,
    we open a new file object in `wb` mode to create our CSV report. On line 221,
    we write our `Count` and `Word` headers onto the first row with a newline character
    to ensure all other data is written in the rows below:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then use a `for` loop and the `most_common()` method to call out a tuple
    containing each word and the assigned count value. If the length of the tuple
    is greater than 1, we write the values into the CSV document in reverse order
    to properly align the columns with the values, followed by a newline character.
    After this loop completes, we close the file and flush the results to the disk
    as seen on line 225:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Following this loop, we then generate a list of the top 10 words. By passing
    the integer 10 into the `most_common()` method, we select only the top 10 most
    common entries in `Counter`. We append a dictionary of the results to a temporary
    list, which is returned to the `word_stats()` function and later used in our HTML
    report:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Building the sender_report() function
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sender_report()` functions similarly to `word_report()` and generates a
    CSV and HTML report for individuals who sent emails. This function showcases another
    method for reading values into the `Counter()` method. On line 242, we open and
    read the lines of a file into the `Counter()` method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We can implement it this way because each line of the input file represents
    a single sender. Counting the data in this manner simplifies the code and, by
    extension, saves us a few lines of writing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'This wasn''t a feasible option for the `word_stats()` function because we had
    to break each line into a separate word and then perform additional logic operations
    prior to counting the words. If we wanted to apply logic to the sender statistics,
    we would need to create a similar loop to that in `word_stats()`. For example,
    we might want to exclude all items from Gmail or that contain the word `noreply`
    in the sender''s name or address:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After generating the sender count, we can open the CSV report and write our
    headers to it. At this point, we'll iterate through each of the most common in
    a `for` loop as seen on line 247, and if the tuple contains more than one element,
    we'll write it to the file.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'This is another location where we could filter the values based on the sender''s
    name. After writing, the file is closed and flushed to the disk. On line 252,
    we generate statistics for the top five senders for the final report by generating
    a list of dictionaries containing the tuple values. To access it in our HTML report
    function, we return this list. See the following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Refining the heat map with the date_report() function
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This report provides data to generate the activity heat map. For it to operate
    properly, it must have the same filename and path specified in the HTML template.
    The default template for the file is named `heatmap.tsv` and is located in the
    same directory as the output HTML report.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: After opening this file with those defaults on line 267, we write the headers
    with a tab character delimiting the day, hour, and value columns and ending with
    a newline character. At this point, we can begin iterating through our list of
    dictionaries by using two `for` loops to access each list containing dictionaries.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first `for` loop, we use the `enumerate()` method to capture the loop
    iteration number. This number conveniently corresponds to the date we''re processing,
    allowing us to use this value to write the day value:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the second `for` loop, we iterate through each dictionary, gathering both
    the hour and count values separately by using the `items()` method to extract
    the key and value as a tuple. With these values, we can now assign the date, hour,
    and count to a tab-separated string and write it to the file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: On line 271, we add 1 to the date value as the heat map chart uses a 1 through
    7 range, whereas our list uses an index of 0 through 6 to count days of the week.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'After iterating through the hours, we flush the data to the disk before moving
    forward to the next dictionary of hours. Once we''ve iterated through all of the
    seven days, we can close this document as it''s ready to be used with our heat
    map chart in the `html_report()` function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Writing the html_report() function
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `html_report()` function is where we tie together all of the pieces of
    information gathered from the PST into a final report, with much anticipation.
    To generate this report, we require arguments specifying the report title, PST
    name, and counts of the top words and senders:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To begin with, we open the template file and read in the contents into a single
    variable as a string. This value is then passed into our `jinja2.Template` engine
    to be processed into a template object called `html_template` on line 290.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a dictionary of values to pass into the template's placeholders and
    use the `context` dictionary on line 292 to hold these values. With the dictionary
    in place, we then render the template on line 295 and provide the `context` dictionary.
    This rendered data is a string of HTML data, as you expect to see on a web page,
    with all of our placeholder logic evaluated and turned into a static HTML page.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'We write the rendered HTML data to an output file in the user-specified directory
    as seen on lines 297 through 299\. With the HTML report written to the output
    directory, the report is complete and ready to view in the output folder:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The HTML template
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book focuses on the use of Python in forensics. Though Python provides
    many great methods for manipulating and applying logic to data, we still need
    to lean on other resources to support our scripts. In this chapter, we've built
    an HTML dashboard to present statistical information about these PST files.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll review sections of HTML, focusing on where our data is
    inserted into the template versus the intricacies of HTML, JavaScript, and other
    web languages. For more information in the use and implementation of HTML, JavaScript,
    D3.js, and other web resources, visit [http://packtpub.com](http://packtpub.com)
    for pertinent titles or [http://w3schools.com](http://w3schools.com) for introductory
    tutorials. Since we'll not be delving deeply into HTML, CSS, or other web design aspects,
    our focus will be primarily on the spaces where our Python script will interact.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This template leverages a couple of common frameworks that allow the rapid design
    of professional-looking web pages. The first is Bootstrap 3, a CSS styling framework
    that organizes and styles HTML to look uniform and clean no matter the device
    used to view the page. The second is the D3.js framework, which is a JavaScript
    framework for graphic visualizations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen before, the template items into which we''ll insert our data
    are contained within double braces, `{{ }}`. We''ll insert the report title for
    our HTML dashboard on line 39 and 44\. Additionally, we''ll insert the name of
    the PST file on lines 48, 55, and 62\. The `div id` tags on lines 51, 58, and
    65 acts as a variable name for the charts that can be inserted by JavaScript in
    the later section of the template once the code processes the input:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After the `div` placeholder elements are in place, the JavaScript on lines
    69 through 305 processes the provided data into charts. The first location data
    is placed on line 92, where the `{{ word_frequency }}` phrase is replaced with
    the list of dictionaries. For example, this could be replaced with `[{''count'':
    ''175'', ''word'': ''message''}, {''count'': ''17'', ''word'': ''iPhone''}]`.
    This list of dictionaries is translated into chart values to form the vertical
    bar chart of the HTML report:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'On line 132, we insert the `percentage_by_sender` value from the context dictionary
    into the JavaScript. This replacement will occur in a similar example to the `word_frequency`
    insert. With this information, the donut chart generates on the HTML report:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We'll use a new way to insert data for the heat map. By providing the filename
    discussed in the previous section, we can prompt the code to look for a `heatmap.tsv`
    file in the same directory as this HTML report. The upside to this is how we're
    able to generate a report once and use the TSV in a program such as Excel and
    within our dashboard, though the downside is that this file must travel with the
    HTML report for it to display properly, as the chart will regenerate on reload.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'This chart also has difficulty rendering on some browsers as JavaScript is
    interpreted differently by each browser. Testing found that Chrome, Firefox, and
    Safari were OK at viewing the graphic. Ensure that browser add-ons are not interfering
    with the JavaScript and that your browser doesn''t block JavaScript from interacting
    with local files. If your browser disallows this, consider running the script
    in the Docker instance, starting the `lighttpd` service, and placing your output
    in `/var/www/html`. When you visit the IP address of your Docker instance, you''ll
    be able to navigate to the report as the server will provide access to the resources
    for you:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The remainder of the template is available in the code repository and can easily
    be referenced and manipulated if web languages are your strong suit or worth further
    exploration. The D3.js library allows us to create additional informative graphics
    and adds another tool to our reporting toolbox that's relatively simple and portable.
    The following graphics represent examples of data for each of the three charts
    we've created.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The first chart represents the most used words in the PST file. The frequency
    is plotted on the *y* axis and the word on the *x* axis:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea070c05-142b-467e-bee8-10ebe5b799bd.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: 'The following chart identifies the top five accounts that have sent an email
    to the user. Notice how the circle graph helps to identify which participants
    are most frequent in the dataset. In addition, the text labels provide the name
    of the address and the number of emails received by that address:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b19b6e-c27f-47f5-a634-d963ffb450d0.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Lastly, the following heat map aggregates all emails into hour-long cells for
    each day. This is very useful in identifying trends in the dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in this case, we can quickly identify that most emails are sent
    or received early in the morning and particularly at 6 AM on Tuesdays. The bar
    at the bottom of the graphic indicates the number of emails. For example, the
    color of the cell for 6 AM Tuesdays indicates that more than 1,896 emails were
    sent or received during that time:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17140a11-ba74-4359-8322-064e7b07c3f4.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Running the script
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our code complete, both the script and the HTML template, we're ready to
    execute the code! In our Ubuntu environment, we'll need to run the following command
    and provide our PST for analysis. If your Ubuntu machine has a configured web
    server, then the output could be placed in the web directory and served as a website
    for other users to view when visiting the server.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan on using the Docker container method to run this code, you''ll
    need to copy the PST file into your container using a command such as the one
    shown in the following. Please note that the following syntax  is `docker cp src_file
    container_name:/path/on/container` and additional functionality is described with
    `docker cp --help`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that our PST is located within our container; we can run our script as
    follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24182530-5859-41b7-a0f5-4ecf4ea41e9b.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows us using `/var/www/html` as our output directory.
    This means that if we're running the `lighttpd` service on our Docker container,
    we'll be able to browse to the container's IP address and view the content in
    a browser on our system. You'll need to run `docker container ls pst_parser` to
    get the correct port that the web server can be found at.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Additional challenges
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we invite you to implement some improvements that will make
    our script more versatile. As mentioned earlier in the chapter, `pypff` currently
    doesn't natively support the extraction or direct interaction with attachments.
    We can, however, call the `pffexport` and `pffinfo` tools within our Python script
    to do so. We recommend looking at the subprocess module to accomplish this. To
    extend this further, how can we connect this with the code covered in previous
    chapter ? What type of data might become available once we have access to attachments?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Consider methods that would allow a user to provide filtering options to collect
    specific messages of interest rather than the entire PST. A library that may assist
    in providing additional configuration options to the user is `ConfigParser` and
    can be installed with `pip`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Finally, another challenge would be seeing improvements in the HTML report by
    adding additional charts and graphs. One example might be to parse `transit_headers`
    and extract the IP addresses. Using these IP addresses, you could geolocate them
    and plot them on a map with the D3.js library. This kind of information can increase
    the usefulness of our reports by squeezing out as much information as possible
    from all potential data points.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Email files contain a large amount of valuable information, allowing forensic
    examiners to gain greater insight into communications and the activity of users
    over time. Using open source libraries, we're able to explore PST files and extract
    information about the messages and folders within. We also examined the content
    and metadata of messages to gather additional information about frequent contacts,
    common words, and abnormal hot spots of activity. Through this automated process,
    we can gather a better understanding of the data we review and begin to identify
    hidden trends. The code for this project can be downloaded from GitHub or Packt,
    as described in the *Preface*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Identifying hidden information is very important in all investigations and is
    one of the many reasons that data recovery is an important cornerstone in the
    forensic investigation process.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll cover how to recover data from a difficult source,
    databases. Using several Python libraries, we'll be able to recover data that
    might otherwise be lost and gain valuable insights into records that're no longer
    tracked by the database.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
