<html><head></head><body>
        

                            
                    <h1 class="header-title">Fuzzy Hashing</h1>
                
            
            
                
<p>Hashing is one of the most common processes run in DFIR. This process allows us to summarize file content and assign a representative and repeatable signature that represents the file's content. We generally employ file and content hashes using algorithms such as MD5, SHA1, and SHA256. These hash algorithms are valuable as we can use them for integrity validation since a change to even one byte of a file's content will completely alter the resulting hash value. These hashes are also commonly used to form whitelists to exclude known or irrelevant content, or alert lists that quickly identify known interesting files. In some cases, though, we need to identify near matches—something that our MD5, SHA1, and SHA256 algorithms can't handle on their own. </p>
<p>One of the most common utilities that assists with similarity analysis is ssdeep, developed by Jessie Kornblum. This tool is an implementation of the spamsum algorithm, developed by Dr. Andrew Tridgell, which generates a base64 signature representing file content. These signatures can be used, independently of the file's content, to help to determine the confidence that two files are similar. This allows for a less computationally intense comparison of these two files and presents a relatively short signature that can be shared or stored easily. </p>
<p>In this chapter, we'll do the following:</p>
<ul>
<li>Hash data using MD5, SHA1, and SHA256 algorithms with Python</li>
<li>Discuss how to hash streams of data, files, and directories of files </li>
<li>Explore how the spamsum algorithm works and implement a version in Python</li>
<li>Leverage the compiled ssdeep library via Python bindings for increased performance and features</li>
</ul>
<p>The code for this chapter was developed and tested using Python 2.7.15 and Python 3.7.1.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Background on hashing</h1>
                
            
            
                
<p>Hashing data is a common technique in the forensics community to <kbd>fingerprint</kbd> a file. Normally, we create a hash of an entire file; however, in the script we'll build later in this chapter, we'll hash segments of a file to evaluate the similarity between two files. Before diving into the complexities of fuzzy hashing, let's walk through how Python can generate cryptographic hashes such as MD5 and SHA1 values.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hashing files in Python</h1>
                
            
            
                
<p>As previously discussed, there are multiple algorithms commonly used by the DFIR community and tools. Before generating a file hash, we must decide which algorithm we would like to use. This can be a tough question, as there are multiple factors to consider. The <strong>Message Digest Algorithm 5</strong> (<strong>MD5</strong>) produces a 128-bit hash and is one of the most commonly used cryptographic hash algorithms across forensic tools. The algorithm is relatively lightweight and the resulting hash is short in length, when compared with other algorithms. Since cryptographic hashes have a fixed length output, selecting an algorithm with a shorter length can help in reducing the impact on system resources.</p>
<p>However, the main issue with MD5 is the probability of hash collisions. A hash collision is where two different input values result in the same hash, an issue that is a product of having a fixed length hash value. This is an issue in forensics, as we rely on the hash algorithm to be a unique fingerprint to represent the integrity of data. If the algorithm has known collisions, the hash may no longer be unique and can't guarantee integrity. For this reason, MD5 isn't recommended for use as the primary hash algorithm in most forensic situations.</p>
<p>In addition to MD5, there are several other common cryptographic hash algorithms including the <strong>Secure Hash Algorithm</strong> (<strong>SHA</strong>) family. The SHA family consists of SHA-1 (160-bit), SHA-256 (256-bit), and SHA-512 (512-bit) to name a few of the more prominent algorithms used in forensics. The SHA-1 algorithm frequently accompanies the MD5 hash in most forensic tools. Recently a research group discovered collisions in the SHA-1 algorithm and shared their findings on their site, <a href="https://shattered.io/">https://shattered.io/</a>. Like MD5, SHA-1 is now losing popularity in the field.</p>
<p>Leveraging one of these hash algorithms is fairly straightforward in Python. In the following code block, we'll demonstrate the examples of hashing with the MD5, SHA-1, and SHA-256 algorithms in the interpreter.</p>
<p>To facilitate this, we'll need to import the standard library, <kbd>hashlib</kbd>, and provide data to generate a hash of. After importing <kbd>hashlib</kbd>, we create a hashing object using the <kbd>md5()</kbd> method. Once defined as <kbd>m</kbd>, we can use the <kbd>.update()</kbd> function to add data to the algorithm and the <kbd>hexdigest()</kbd> method to generate the hexadecimal hash we're accustomed to seeing from other tools. This process can be handled by a single line as demonstrated here:</p>
<pre><strong>&gt;&gt;&gt; import hashlib</strong><br/><strong>&gt;&gt;&gt; m = hashlib.md5()</strong><br/><strong>&gt;&gt;&gt; m.update('This will be hashed!')</strong><br/><strong>&gt;&gt;&gt; m.hexdigest()</strong><br/><strong>'0fc0cfd05cc543be3a2f7e7ed2fe51ea'</strong><br/><strong>&gt;&gt;&gt; hashlib.md5('This will be hashed!').hexdigest()</strong><br/><strong>'0fc0cfd05cc543be3a2f7e7ed2fe51ea'</strong><br/><strong>&gt;&gt;&gt; hashlib.sha1('This will be hashed!').hexdigest()</strong><br/><strong>'5166bd094f3f27762b81a7562d299d887dbd76e3'</strong><br/><strong>&gt;&gt;&gt; hashlib.sha256('This will be hashed!').hexdigest()</strong><br/><strong>'03bb6968581a6d6beb9d1d863b418bfdb9374a6ee23d077ef37df006142fd595'</strong> </pre>
<p>In the preceding example, we hashed a string object. But what about files? After all, that's what we're truly interested in doing.</p>
<p>To hash a file, we need to pass the contents of the file to the hash object. As seen in the code block, we begin by opening and writing to a file to generate some sample data that we can hash. After the setup, we close and then reopen the file for reading and use the <kbd>read()</kbd> method to read the full content of the file into the <kbd>buffer</kbd> variable. At this point, we provide the <kbd>buffer</kbd> value as the data to hash and generate our unique hash value. See the following code:</p>
<pre><strong>&gt;&gt;&gt; output_file = open('output_file.txt', 'w')</strong><br/><strong>&gt;&gt;&gt; output_file.write('TmV2ZXIgR29ubmEgR2l2ZSBZb3UgVXA=')</strong><br/><strong>&gt;&gt;&gt; output_file.close()</strong><br/><strong>&gt;&gt;&gt; input_file = open('output_file.txt', 'r')</strong><br/><strong>&gt;&gt;&gt; buffer = input_file.read()</strong><br/><strong>&gt;&gt;&gt; hashlib.sha1(buffer).hexdigest()</strong><br/><strong>'aa30b352231e2384888e9c78df1af47a9073c8dc'</strong><br/><strong>&gt;&gt;&gt; hashlib.md5(buffer).hexdigest()</strong><br/><strong>'1b49a6fb562870e916ae0c040ea52811'</strong><br/><strong>&gt;&gt;&gt; hashlib.sha256(buffer).hexdigest()</strong><br/><strong>'89446e08f985a9c201fa969163429de3dbc206bd7c7bb93e490631c308c653d7'</strong> </pre>
<p>The hashing method shown here is good for small files or streams of data. We need to adjust our approach some if we want to be able to more flexibly handle files.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hashing large files – hashing_example.py</h1>
                
            
            
                
<p>Our first script in this chapter is short and to the point; it'll allow us to hash a provided file's content with a specified cryptographic algorithm. This code will likely be more useful as a feature within a larger script, such as our file listing utility; we'll demonstrate a standalone example to walk through how to handle hashing files in a memory-efficient manner.</p>
<p>To begin, we only need two imports, <kbd>argparse</kbd> and <kbd>hashlib</kbd>. Using these two built-in libraries, we'll be able to generate hashes, as shown in the prior example. On line 33, we list out the supported hash algorithms. This list should only contain algorithms available as a module within <kbd>hashlib</kbd>, as we'll call (for example) <kbd>md5</kbd> from the list as <kbd>hashlib.md5()</kbd>. The second constant defined, on line 34, is <kbd>BUFFER_SIZE</kbd>, which is used to control how much of a file to read at a time. This value should be smaller, 1 MB in this instance, to preserve the amount of memory required per read, although we also want a number large enough to limit the number of reads we have to perform on the file. You may find this number is adjusted based on the system you choose to run it on. For this reason, you may consider specifying this as an argument instead of a constant:</p>
<pre>001 """Sample script to hash large files effiently."""<br/>002 import argparse<br/>003 import hashlib<br/>...<br/>033 HASH_LIBS = ['md5', 'sha1', 'sha256', 'sha512']<br/>034 BUFFER_SIZE = 1024**3</pre>
<p>Next, we define our arguments. This is very brief as we're only accepting a filename and an optional algorithm specification:</p>
<pre>036 parser = argparse.ArgumentParser()<br/>037 parser.add_argument("FILE", help="File to hash")<br/>038 parser.add_argument("-a", "--algorithm",<br/>039     help="Hash algorithm to use", choices=HASH_LIBS,<br/>040     default="sha512")<br/>041 args = parser.parse_args()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Once we know the specified arguments, we'll translate the selected algorithm from an argument into a function we can call. To do this, we use the <kbd>getattr()</kbd> method as shown on line 43. This built-in function allows us to retrieve functions and properties from an object (such as a method from a library, as shown in the following code). We end the line with <kbd>()</kbd> as we want to call the specified algorithm's initialization method and create an instance of the object as <kbd>alg</kbd> that we can use to generate the hash. This one-liner is the equivalent of calling <kbd>alg = hashlib.md5()</kbd> (for example), but performed in an argument-friendly fashion:</p>
<pre>043 alg = getattr(hashlib, args.algorithm)()</pre>
<p>On line 45, we open the file for reading, which we start on line 47 by reading the first buffer length into our <kbd>buffer_data</kbd> variable. We then enter a <kbd>while</kbd> loop where we update our hash algorithm object on line 49 before getting the next buffer of data on line 50. Luckily for us, Python will read all of the data from <kbd>input_file</kbd>, even if <kbd>BUFFER_SIZE</kbd> is greater than what remains in the file. Additionally, Python will exit the loop once we reach the end of the file and close it for us when exiting the <kbd>with</kbd> context. Lastly, on line 52, we print the <kbd>.hexdigest()</kbd> of the hash we calculated:</p>
<pre>045 with open(args.FILE, 'rb') as input_file:<br/>046 <br/>047     buffer_data = input_file.read(BUFFER_SIZE)<br/>048     while buffer_data:<br/>049         alg.update(buffer_data)<br/>050         buffer_data = input_file.read(BUFFER_SIZE)<br/>051 <br/>052 print(alg.hexdigest())</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating fuzzy hashes</h1>
                
            
            
                
<p>Now that we've mastered how to generate cryptographic hashes, let's work on generating fuzzy hashes. We'll discuss a few techniques we could employ for similarity analysis, and walk through a basic example of how ssdeep and spamsum employ rolling hashing to help generate more resilient signatures. </p>
<p>It may go without saying that our most accurate approach to similarity analysis is to compare the byte content of two files, side by side, and look for differences. While we may be able to accomplish this using command-line tools or a difference analysis tool (such as kdiff3), this only really works at a small scale. Once we move from comparing two small files to comparing many small files, or a few medium-sized files, we need a more efficient approach. This is where signature generation comes into play.</p>
<p>To generate a signature, we must have a few things figured out:</p>
<ul>
<li>What alphabet we want to use for our signature</li>
<li>How we want to segment the file into summarizable blocks</li>
<li>The technique for converting our block summary into a character from our alphabet</li>
</ul>
<p>While the alphabet is an optional component, it allows us humans to better review and understand the data. We can always store it as integers and save a tiny bit of computational resources. Base64 is a common choice for the alphabet and is used by both spamsum and ssdeep.</p>
<p>For the aforementioned second and third items, let's discuss a few techniques for slicing up our file and generating our hash value. For this example (and to keep things simple), let's use the following character sequence as our file content:</p>
<pre>abcdefghijklmnopqrstuvwxyz01</pre>
<p>Our first approach is to slice the file into equal sized blocks. The first row in the following example is our file content, and the second is the numeric ASCII value for each character in our first row. For this example, we've decided to split our file into 4-byte blocks with the vertical bars and color-coded numeric ASCII values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-497 image-border" src="img/bcd0cb6e-f1ee-49c5-a62b-9917512558d6.png" style="width:100.67em;height:9.17em;"/></p>
<p>We then summarize each of these 4-byte blocks by summing the ASCII value of the four characters, as shown in the third row of the table. We then convert this summarization of our file content into our base64 representation by taking 394 modulo 64 (<em>394 % 64</em>) which is 10, or K in the base64 alphabet. This base64 value, as you may have guessed, is on the fourth row.</p>
<p class="mce-root">The letter K becomes our summarization of the first block, a for the second, and it continues until we have our complete file signature of Kaq6KaU.</p>
<p class="mce-root">In the next diagram, there's a slightly modified version of our original file. As seen below, someone replaced jklmn with hello. We can now run our hashing algorithm against this file to get a sense of how much has changed between the two versions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-498 image-border" src="img/89201581-e5a0-496f-89e5-cd5cd7d50b39.png" style="width:100.58em;height:6.75em;"/></p>
<p class="mce-root">Using the same technique, we calculate the new hash value of Kai6KaU. If we wanted to compare the similarity of our two files, we should be able to use our signatures to facilitate our comparison, right? So in this case, we have one letter difference between our signatures, meaning our two file streams are largely similar!</p>
<p class="mce-root">As you may have spotted, there's an issue here: we've found a hash collision when using our algorithm. In the prior example, the fourth block of each file is different; the first is mnop and the second is loop. Since we're summing our file content to determine our signature value, we're bound to get an unhealthy amount of collisions. These collisions may cause us to think files are more similar when they aren't, and unfortunately are a product of summarizing file content without the use of a cryptographic hash algorithm. For this reason, we have to find a better balance between summarizing file content and encountering hash collisions.</p>
<p class="mce-root">Our next example demonstrates what happens when insertion occurs. As you can see in the following diagram, the letter h was inserted after mn, adding one byte to the file and causing the entire content to shift right by one. In this instance, our last block will just contain the number 1, though some implementations may handle this differently:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4e15eba7-e435-418c-ad0a-649fbaff8669.png"/></p>
<p class="mce-root">Using our same formula, we calculate a hash of KaqyGUbx. This hash is largely different than Kaq6KaU. In fact, once we reach the block containing the change, the hash is completely different even though we have similar content in the latter half of the file.</p>
<p class="mce-root">This is one of the main reasons that using a fixed block size isn't the best approach for similarity analysis. Any shift in content moves data across the boundaries and will cause us to calculate completely different hashes for similar content. To address that, we need to be able to set these boundaries in another way.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Context Triggered Piecewise Hashing (CTPH)</h1>
                
            
            
                
<p class="mce-root">As you probably guessed, this is where CTPH comes into play. Essentially, we're aiming to calculate reset points with this technique. Reset points, in this case, are boundaries similar to the 4-byte boundaries we used in the prior example, as we use these reset points to determine the amount of a file we want to summarize. The notable exception is that we pick the boundaries based on file content (our context triggering) versus fixed windows. What this means is we use a rolling hash, as employed by ssdeep and spamsum, to calculate values throughout the file; when this specific value is found, a boundary line is drawn and the content since the prior boundary is summarized (the piecewise hash). In the following example, we're using a simplified calculation to determine whether we've reached a reset point.</p>
<p class="mce-root">While both spamsum and ssdeep calculate the reset point number for each file, for our example, we'll use <em>7</em> to keep things simple. This means whenever our rolling hash has a value of <em>7</em>, we'll summarize the content between this boundary and the previous. As an additional note, this technique is meant for files with more than 28 bytes, so our hashes here will be really short and, therefore, less useful outside of our illustrative purposes.</p>
<p class="mce-root">Before jumping into the example, let's talk through what a rolling hash is. Once again, we'll use the same example file content we used previously. We then use what's known as a rolling hash to calculate our value for each byte of the file. A rolling hash works by calculating a hash value for all of the characters within a certain window of the file. In our case, we'll have a window size of three. The window movement in our file would look like this across the first four iterations:</p>
<ul>
<li class="mce-root"><kbd>['a', '', ''] = [97, 0, 0]</kbd></li>
<li class="mce-root"><kbd>['a', 'b', ''] = [97, 98, 0]</kbd></li>
<li class="mce-root"><kbd>['a', 'b', 'c'] = [97, 98, 99]</kbd></li>
<li class="mce-root"><kbd>['b', 'c', 'd'] = [98, 99, 100]</kbd></li>
</ul>
<p class="mce-root">As you can see, this rolling window would continue through the file, adding a new byte each iteration and removing the oldest byte, in FIFO style. To generate a hash of this window, we would then perform a series of further calculation against the values in the window.</p>
<p class="mce-root">For this example, as you likely guessed, we'll sum the ASCII values to keep things simple. This sum is shown in the first row of the following example. To keep the numbers smaller though, we'll then take our summed ASCII values (<em>S</em>) modulo 8 (<em>S % 8</em>) and use this integer to look for our boundaries in the file content. This number is found in the second row of the following screenshot. If <em>S % 8 == 7</em>, we've reached a reset point and can create a summarization of the prior block.</p>
<p class="mce-root">The ssdeep and spamsum algorithms handle this rolling window calculation differently, though the product of the calculation is used in the same manner. We have simplified the calculation to make this process easier to discuss.</p>
<p class="mce-root">Since our reset point is 7, as previously selected, we'll define a chunk of a file any time our rolling hash calculation returns a seven. This is represented in the following screenshot with horizontal lines showing the blocks we've set within the file.</p>
<p class="mce-root">For each block, we'll calculate our signature in the same way as before: summing up the ASCII integer values of the content within the entire block (as shown in the fourth row) and applying modulo 64 to get the character for the signature (as seen in the last row). Please remember that the only relationship between rows 2 and 4 in this example is that row 2 tells us when to set the reset point and calculate the number shown in row 4. These two hashes are algorithmically independent of one another by design. Row 4 is still the summation of the ASCII values for <em>a + b + c + d + e + f</em> and not the summation of our rolling hash output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-499 image-border" src="img/ee120ac7-f0a4-4081-9c02-a9fc71583a47.png" style="width:98.33em;height:10.33em;"/></p>
<p class="mce-root">This produces the signature VUUD. While much shorter, we now have context triggered hashes. As previously described, we've accomplished this by using the rolling hash to define our boundaries (the context triggering), and the summation of our block (piecewise hashing) to identify common chunks of the file that we can compare to files with similar reset point sizes (or other files with a reset point of 7).</p>
<p class="mce-root">For our final example, let's revisit what happens when we perform the same insertion of the letter h. Using our rolling hash to calculate our context-based blocks (as shown in the first row), we can calculate the summarization of blocks using the same algorithm and generate the signature VUH1D:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-500 image-border" src="img/bf27cd4e-684f-4f06-a801-7bcf23102561.png" style="width:100.75em;height:8.75em;"/></p>
<p class="mce-root">As you can see, this technique is more resilient to insertions and allows us to more accurately compare differences in files than using the fixed blocks. In this case, our signatures are showing that the two files are more different than they are, though this technique is more accurate than our fixed block calculation as it understands that the tail of our file is the same between our two versions.</p>
<p class="mce-root">Obviously, this technique requires files larger than 28 bytes in order to produce accurate results, though hopefully this simplification can help depict how these fuzzy hashes are formed. With this understanding, let's start working on our script.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing fuzzy_hasher.py</h1>
                
            
            
                
<p>This script was tested with both Python versions 2.7.15 and 3.7.1 and doesn't leverage any third-party libraries.</p>
<p>While we'll get to the internals of the fuzzy hashing algorithm, let's start our script as we have the others. We begin with our imports, all standard libraries that we've used before as shown in the following. We also define a set of constants on lines 36 through 47. Lines 37 and 38 define our signature alphabet, in this case all of the base64 characters. The next set of constants are used in the spamsum algorithm to generate the hash. <kbd>CONTEXT_WINDOW</kbd> defines the amount of the file we'll read for our rolling hash. <kbd>FNV_PRIME</kbd> is used to calculate the hash while <kbd>HASH_INIT</kbd> sets a starting value for our hash. We then have <kbd>SIGNATURE_LEN</kbd>, which defines how long our fuzzy hash signature should be. Lastly, the <kbd>OUTPUT_OPTS</kbd> list is used with our argument parsing to show supported output formats—more on that later:</p>
<pre>001 """Spamsum hash generator."""<br/>002 import argparse<br/>003 import logging<br/>004 import json<br/>005 import os<br/>006 import sys<br/>007<br/>008 """ The original spamsum algorithm carries the following license:<br/>009 Copyright (C) 2002 Andrew Tridgell &lt;tridge@samba.org&gt;<br/>010 <br/>011 This program is free software; you can redistribute it and/or<br/>012 modify it under the terms of the GNU General Public License<br/>013 as published by the Free Software Foundation; either version 2<br/>014 of the License, or (at your option) any later version.<br/>015 <br/>016 This program is distributed in the hope that it will be useful,<br/>017 but WITHOUT ANY WARRANTY; without even the implied warranty of<br/>018 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<br/>019 GNU General Public License for more details.<br/>020 <br/>021 You should have received a copy of the GNU General Public License<br/>022 along with this program; if not, write to the Free Software<br/>023 Foundation, Inc.,<br/>024 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.<br/>025 <br/>026 CHANGELOG:<br/>027 Implemented in Python as shown below by Chapin Bryce &amp;<br/>028 Preston Miller<br/>029 """<br/>030<br/>031 __authors__ = ["Chapin Bryce", "Preston Miller"]<br/>032 __date__ = 20181027<br/>033 __description__ = '''Generate file signatures using<br/>034     the spamsum algorithm.'''<br/>035 <br/>036 # Base64 Alphabet<br/>037 ALPHABET = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'<br/>038 ALPHABET += 'abcdefghijklmnopqrstuvwxyz0123456789+/'<br/>039 <br/>040 # Constants for use with signature calculation<br/>041 CONTEXT_WINDOW = 7<br/>042 FNV_PRIME = 0x01000193<br/>043 HASH_INIT = 0x28021967<br/>044 SIGNATURE_LEN = 64<br/>045 <br/>046 # Argument handling constants<br/>047 OUTPUT_OPTS = ['txt', 'json', 'csv']<br/>048 logger = logging.getLogger(__file__)</pre>
<p>This script has three functions: <kbd>main()</kbd>, <kbd>fuzz_file()</kbd>, and <kbd>output()</kbd>. The <kbd>main()</kbd> function acts as our primary controller, handling the processing of directories versus single files and calling the <kbd>output()</kbd> function to display the result of the hashing. The <kbd>fuzz_file()</kbd> function accepts a file path and generates a spamsum hash value. The <kbd>output()</kbd> function then takes the hash and filename and displays the values in the specified format:</p>
<pre>051 def main(file_path, output_type):<br/>...<br/>087 def fuzz_file(file_path):<br/>...<br/>188 def output(sigval, filename, output_type='txt'):</pre>
<p>The structure of our script is fairly straightforward, as emphasized by the following diagram. As illustrated by the dashed line, the <kbd>fuzz_file()</kbd> function is the only function that returns a value. This is true as our <kbd>output()</kbd> function displays content on the console instead of returning it to <kbd>main()</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-501 image-border" src="img/ec8fd59d-9b8c-4fbf-ad5c-8ae4d19a960b.png" style="width:74.58em;height:32.42em;"/></p>
<p>Finally, our script ends with argument handling and log initiation. For command-line arguments, we're accepting a path to a file or folder to process and the format of our output. Our output will be written to the console, with current options for text, CSV, and JSON output types. Our logging parameter is standard and looks very similar to our other implementations, with the notable difference that we're writing the log messages to <kbd>sys.stderr</kbd> instead so that the user can still interact with the output generated by <kbd>sys.stdout</kbd>:</p>
<pre>204 if __name__ == '__main__':<br/>205     parser = argparse.ArgumentParser(<br/>206         description=__description__,<br/>207         epilog='Built by {}. Version {}'.format(<br/>208             ", ".join(__authors__), __date__),<br/>209         formatter_class=argparse.ArgumentDefaultsHelpFormatter<br/>210     )<br/>211     parser.add_argument('PATH',<br/>212         help='Path to file or folder to generate hashes for. '<br/>213              'Will run recursively.')<br/>214     parser.add_argument('-o', '--output-type',<br/>215         help='Format of output.', choices=OUTPUT_OPTS,<br/>216         default="txt")<br/>217     parser.add_argument('-l', help='specify log file path',<br/>218         default="./")<br/>219 <br/>220     args = parser.parse_args()<br/>221 <br/>222     if args.l:<br/>223         if not os.path.exists(args.l):<br/>224             os.makedirs(args.l) # create log directory path<br/>225             log_path = os.path.join(args.l, 'fuzzy_hasher.log')<br/>226     else:<br/>227         log_path = 'fuzzy_hasher.log'<br/>228 <br/>229     logger.setLevel(logging.DEBUG)<br/>230     msg_fmt = logging.Formatter("%(asctime)-15s %(funcName)-20s"<br/>231         "%(levelname)-8s %(message)s")<br/>232     strhndl = logging.StreamHandler(sys.stderr) # Set to stderr<br/>233     strhndl.setFormatter(fmt=msg_fmt)<br/>234     fhndl = logging.FileHandler(log_path, mode='a')<br/>235     fhndl.setFormatter(fmt=msg_fmt)<br/>236     logger.addHandler(strhndl)<br/>237     logger.addHandler(fhndl)<br/>238 <br/>239     logger.info('Starting Fuzzy Hasher v. {}'.format(__date__))<br/>240     logger.debug('System ' + sys.platform)<br/>241     logger.debug('Version ' + sys.version.replace("\n", " "))<br/>242 <br/>243     logger.info('Script Starting')<br/>244     main(args.PATH, args.output_type)<br/>245     logger.info('Script Completed')</pre>
<p>With this framework, let's explore how our <kbd>main()</kbd> function is implemented.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Starting with the main() function</h1>
                
            
            
                
<p>Our main function accepts two parameters: the file path and output type. We first check the output type to ensure it's in the <kbd>OUTPUT_OPTS</kbd> list, just in case the function was called from other code that did not validate. If it is an unknown output format, we'll raise an error and exit the script:</p>
<pre>051 def main(file_path, output_type):<br/>052     """<br/>053     The main function handles the main operations of the script<br/>054     :param file_path: path to generate signatures for<br/>055     :param output_type: type of output to provide<br/>056     :return: None<br/>057     """<br/>058 <br/>059     # Check output formats<br/>060     if output_type not in OUTPUT_OPTS:<br/>061         logger.error(<br/>062             "Unsupported output format '{}' selected. Please "<br/>063             "use one of {}".format(<br/>064                 output_type, ", ".join(OUTPUT_OPTS)))<br/>065         sys.exit(2)</pre>
<p>We then start working with the file path, getting its absolute file path on line 67, and checking whether it's a directory on line 69. If so, we begin to iterate over the directory and subdirectories to find and process all files within. The code on lines 71 through 73 should look familiar from <a href="a4ae250a-8aa8-49b9-8fd6-0cac51975f11.xhtml" target="_blank">Chapter 5</a>, <em>Databases in Python</em>. On line 74, we call the <kbd>fuzz_file()</kbd> function to generate our hash value, <kbd>sigval</kbd>. This <kbd>sigval</kbd> value is then provided, along with the filename and output format, to our <kbd>output()</kbd> function:</p>
<pre>067     # Check provided file path<br/>068     file_path = os.path.abspath(file_path)<br/>069     if os.path.isdir(file_path):<br/>070         # Process files in folders<br/>071         for root, _, files in os.walk(file_path):<br/>072             for f in files:<br/>073                 file_entry = os.path.join(root, f)<br/>074                 sigval = fuzz_file(file_entry)<br/>075                 output(sigval, file_entry, output_type)</pre>
<p>The remainder of our <kbd>main()</kbd> function handles single file processing and error handling for invalid paths. If, as seen on lines 76 through 79, the path is a file, we'll process it the same as we did before, generating the hash with <kbd>fuzz_file()</kbd> and passing the values to our <kbd>output()</kbd> function. Lastly, on lines 80 through 84, we handle errors with accessing the specified file or folder path:</p>
<pre>076     elif os.path.isfile(file_path):<br/>077         # Process a single file<br/>078         sigval = fuzz_file(file_path)<br/>079         output(sigval, file_path, output_type)<br/>080     else:<br/>081         # Handle an error<br/>082         logger.error("Error - path {} not found".format(<br/>083             file_path))<br/>084         sys.exit(1)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating our fuzzy hashes</h1>
                
            
            
                
<p>Before we dive into the code for our <kbd>fuzz_file()</kbd> function, let's talk briefly about the moving parts here:</p>
<ul>
<li>A rolling hash</li>
<li>A calculated reset point that is derived from the file's size</li>
<li>Two traditional hashes, in this case leveraging the FNV algorithm</li>
</ul>
<p>The rolling hash is similar to our earlier example in that it's used to identify the boundaries that we'll summarize using our traditional hashes. In the case of ssdeep and spamsum, the reset point that the rolling hash is compared to (set to <kbd>7</kbd> in our prior example) is calculated based on the file's size. We'll show the exact function for determining this value in a bit, though we wanted to highlight that this means only files with the same block size can be compared. While there is more to talk about conceptually, let's start working through the code and applying these concepts.</p>
<p>We now move to the fun function: <kbd>fuzz_file()</kbd>. This function accepts a file path and uses the constants found at the beginning of the file to handle the calculation of the signature:</p>
<pre>087 def fuzz_file(file_path):<br/>088     """<br/>089     The fuzz_file function creates a fuzzy hash of a file<br/>090     :param file_path (str): file to read.<br/>091     :return (str): spamsum hash<br/>092     """</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Generating our rolling hash</h1>
                
            
            
                
<p>The following code block is our rolling hash function. Now, it may seem odd to have a function within a function, though this design has a few advantages. First, it's useful for organization. This rolling hash code block is only used by our <kbd>fuzz_file()</kbd> function and, by nesting it inside this function, we can inform the next person who reads our code that this is the case. Secondly, by placing this function within <kbd>fuzz_file()</kbd>, we can assure anyone who imports our code as a module doesn't misuse the rolling hash function. And while there are multiple other efficiencies and management reasons for selecting this design, we wanted to incorporate this feature into this script to introduce you to the concept. As you see in our other scripts, this isn't always used for specialized functions but is a tool that you can employ in your scripts to refine their design.</p>
<p>This nested function takes two arguments, shortened to <kbd>nb</kbd> for <kbd>new_byte</kbd> and <kbd>rh</kbd> for our rolling hash tracking dictionary. In our prior example, to calculate the rolling hash, we added the ASCII values of the entire window together. In this function, we'll perform a series of calculations to help us generate a rolling hash of a larger 7-byte window:</p>
<pre>095     def update_rolling_hash(nb, rh):<br/>096         """<br/>097         Update the rolling hash value with the new byte<br/>098         :param nb (int): new_byte as read from file<br/>099         :param rh (dict): rolling hash tracking dictionary<br/>100         :return: computed hash value to compare to reset_point<br/>101         """</pre>
<p>The <kbd>rh</kbd> rolling hash tracking dictionary is used to keep an eye on the moving parts within this rolling hash. There are three numbers that are stored as <kbd>r1</kbd>, <kbd>r2</kbd>, and <kbd>r3</kbd>. These numbers face additional calculations, as shown in the following code block, and the sum of the three are returned as the integer representing the rolling hash for that frame of the file.</p>
<p>The other two elements tracked by the dictionary are <kbd>rn</kbd> and <kbd>rw</kbd>. The <kbd>rn</kbd> key holds the offset the rolling hash is at within the file and is used to determine what character in the window is replaced by the <kbd>nb</kbd>, <kbd>new_byte</kbd>, value. This window, as you may have guessed, is stored in <kbd>rw</kbd>. Unlike our prior example where each character in the window was shifted left for each calculation of the rolling hash, this implementation just replaces the oldest character in the array. This improves efficiency as it results in one operation instead of eight:</p>
<pre>102         # Calculate R2<br/>103         rh['r2'] -= rh['r1']<br/>104         rh['r2'] += (CONTEXT_WINDOW * nb)<br/>105 <br/>106         # Calculate R1<br/>107         rh['r1'] += nb<br/>108         rh['r1'] -= rh['rw'][rh['rn'] % CONTEXT_WINDOW]<br/>109 <br/>110         # Update RW and RN<br/>111         rh['rw'][rh['rn'] % CONTEXT_WINDOW] = nb<br/>112         rh['rn'] += 1<br/>113 <br/>114         # Calculate R3<br/>115         rh['r3'] = (rh['r3'] &lt;&lt; 5) &amp; 0xFFFFFFFF<br/>116         rh['r3'] = rh['r3'] ^ nb<br/>117 <br/>118         # Return the sum of R1 + R2 + R3<br/>119         return rh['r1'] + rh['r2'] + rh['r3']</pre>
<p>This logic is computationally the same as that used by ssdeep and spamsum. To start, we compute the <kbd>r2</kbd> value by subtracting <kbd>r1</kbd> and adding the product of <kbd>CONTEXT_WINDOW</kbd> and <kbd>new_byte</kbd>. We then update the <kbd>r1</kbd> value by adding <kbd>new_byte</kbd> and subtracting the oldest byte within the window. This means that <kbd>r1</kbd> stores the sum of the entire window, similarly to our entire rolling hash algorithm in the earlier example.</p>
<p>On line 111, we start updating our window, replacing the oldest byte with our <kbd>new_byte</kbd> character. After this, we increment the <kbd>rn</kbd> value so that it accurately tracks the offset within the file.</p>
<p>Finally, we calculate our <kbd>r3</kbd> value, which uses some operations we haven't introduced at this point. The <kbd>&lt;&lt;</kbd> operator is a bitwise operator that shifts our value to the left, in this case by five places. This is effectively the same as us multiplying our value by 2**5. The second new bitwise operator on line 115 is <kbd>&amp;</kbd>,which in Python is a bitwise <kbd>AND</kbd> statement. This operator evaluates each bit for the values on either side of the operation, position by position, and if they're both equal to <kbd>1</kbd>, they're enabled in that position of the output; otherwise, they're disabled. As a note, two <kbd>0</kbd> values in the same position do not result in <kbd>1</kbd> when using a bitwise <kbd>AND</kbd> statement. The third new bitwise operator is on line 116 and is <kbd>^</kbd>, or the exclusive <kbd>OR</kbd> operator, also called an XOR operation. This works mostly as the opposite of our bitwise <kbd>AND</kbd> statement, where if the bits between the two values, position by position, are different, <kbd>1</kbd> is returned for that position and if they're the same, <kbd>0</kbd> is returned. </p>
<p>More information on bitwise operations in Python is available at <a href="https://wiki.python.org/moin/BitwiseOperators">https://wiki.python.org/moin/BitwiseOperators</a>.</p>
<p>With our bitwise operations out of the way, we return the sum of <kbd>r1</kbd>, <kbd>r2</kbd>, and <kbd>r3</kbd> for further use in our fuzzy hash calculation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing signature generation</h1>
                
            
            
                
<p>Moving back into our <kbd>fuzz_file()</kbd> function, we evaluate the provided file to see whether it has any content, and if so, open the file. We store this file size for later use:</p>
<pre>122     fsize = os.stat(file_path).st_size<br/>123     if fsize == 0:<br/>124         logger.warning("File is 0-bytes. Skipping...")<br/>125         return ""<br/>126     open_file = open(file_path, 'rb')</pre>
<p>We now start with our first factor in the hashing algorithm, the <strong>reset point</strong>. This value is noted as the first value in a signature, as it's used to determine what hashes can be compared. To calculate this number, we start with <kbd>3</kbd>, as selected in the spamsum algorithm as a minimum reset point. We then double the reset point, as shown on line 130, until it's larger than the <kbd>filesize / 64</kbd>:</p>
<pre>129     reset_point = 3<br/>130     while reset_point * 64 &lt; fsize:<br/>131         reset_point *= 2</pre>
<p>Once we have our initial reset point, we read our file into memory as <kbd>bytearray</kbd> since we want to read each character as a byte that we can interpret. We then set up our <kbd>while</kbd> loop, which we'll use to adjust the <kbd>reset_point</kbd> size if we need to—more on that later on:</p>
<pre>134     complete_file = bytearray(open_file.read())<br/>135     done = False<br/>136     while not done:</pre>
<p>Once within our <kbd>while</kbd> loop, we'll initiate our hashing objects. The first object is <kbd>rolling_hash</kbd>, a dictionary with five keys. The <kbd>r1</kbd>, <kbd>r2</kbd>, and <kbd>r3</kbd> keys are used to compute the hash; the <kbd>rn</kbd> key tracks the position of the cursor in the file; the <kbd>rw</kbd> key holds a list the size of the <kbd>CONTEXT_WINDOW</kbd> constant. This is the dictionary that's referenced heavily in our <kbd>update_rolling_hash()</kbd> function. It may be helpful to re-read that section now that you've seen what the <kbd>rolling_hash</kbd> dictionary looks like.</p>
<p>Following this dictionary, we have <kbd>trad_hash1</kbd> and <kbd>trad_hash2</kbd> initialized with the <kbd>HASH_INIT</kbd> constant. Lastly, we initialize the two signatures, <kbd>sig1</kbd> and <kbd>sig2</kbd>. The variable <kbd>trad_hash1</kbd> is used to populate the <kbd>sig1</kbd> value, and similarly, <kbd>trad_hash2</kbd> is used to populate the <kbd>sig2</kbd> value. We'll show how we calculate these traditional hashes and update our signatures shortly:</p>
<pre>138         rolling_hash = {<br/>139             'r1': 0,<br/>140             'r2': 0,<br/>141             'r3': 0,<br/>142             'rn': 0,<br/>143             'rw': [0 for _ in range(CONTEXT_WINDOW)]<br/>144         }<br/>145         trad_hash1 = HASH_INIT<br/>146         trad_hash2 = HASH_INIT<br/>147         sig1 = ""<br/>148         sig2 = ""</pre>
<p>Once we've initialized our hash values, we can start iterating through the file as seen on line 151. On line 153, we calculate the rolling hash using the latest byte from the file and the <kbd>rolling_hash</kbd> dictionary. Remember that dictionaries can be passed into a function and updated, and can retain their updated values outside of the function without needing to be returned. This allows a simpler interface with our rolling hash function. This function simply returns the calculated rolling hash, which is in the form of an integer as previously discussed. This rolling hash allows us to hash a moving (or rolling) window of data through a byte stream and is used to identify when in our file we should add a character to our signature:</p>
<pre>151         for new_byte in complete_file:<br/>152             # Calculate our rolling hash<br/>153             rh = update_rolling_hash(new_byte, rolling_hash)</pre>
<p>After calculating the rolling hash value, we need to update our traditional hashes. These hashes use the <strong>Fowler–Noll–Vo</strong> (<strong>FNV</strong>) hash, where we multiply the former value of the hash against the fixed prime, defined as one of our constants, before being XOR'd (<kbd>^</kbd> as previously discussed) against the new byte of data. Unlike the rolling hash, these hash values continue to increment with each new byte and grow in size until we reach one of our boundaries:</p>
<pre>156             trad_hash1 = (trad_hash1 * FNV_PRIME) ^ new_byte<br/>157             trad_hash2 = (trad_hash2 * FNV_PRIME) ^ new_byte</pre>
<p>These boundaries are evaluated by two conditionals, one for each of our hash/signature pairs. Lines 161 through 164 are functionally equivalent to lines 165 through 168, with the exception of which traditional hash and signature is in use. For simplicity, we'll walk through the first.</p>
<p>On lines 161 and 162 (due to line wrapping), we have our first conditional statement, which evaluates whether the product of our rolling hash modulo <kbd>reset_point</kbd>, is equal to <kbd>reset_point - 1</kbd>. We also ensure that our overall signature length is less than the maximum signature length minus 1. If these conditions are met, we've reached a boundary and will convert our traditional hash into a character of our signature, as shown on line 163. After adding a character to our signature, we then reset our traditional hash back to the initial value, meaning the next block of data will have a hash value starting from the same point as the prior block.</p>
<p>As mentioned, this is repeated for the second signature, with the notable exception that the second signature is modifying <kbd>reset_point</kbd> (by multiplying it by two) and the maximum signature length (by dividing it by two). This second reset point was added to address the desire for the spamsum signature to be short—64 characters by default. This means that the primary signature may be cut off and the tail of the file may represent one character of the signature. To combat this, spamsum added the second signature to generate a value that represents more, if not all, of the file. This second signature effectively has a <kbd>reset_point</kbd> twice as large as the first signature:</p>
<pre>159             # Check if our rolling hash reaches a reset point<br/>160             # If so, update sig and reset trad_hash<br/>161            if (rh % reset_point == reset_point - 1<br/>162                     and len(sig1) &lt; SIGNATURE_LEN - 1):<br/>163                 sig1 += ALPHABET[trad_hash1 % 64]<br/>164                 trad_hash1 = HASH_INIT<br/>165             if (rh % (reset_point * 2) == (reset_point * 2) - 1<br/>166                     and len(sig2) &lt; (SIGNATURE_LEN / 2) - 1):<br/>167                 sig2 += ALPHABET[trad_hash2 % 64]<br/>168                 trad_hash2 = HASH_INIT</pre>
<p>This is the end of our for loop; this logic will repeat until we've reached the end of the file, though the signatures will only grow to 63 and 31 characters in length, respectively. After our <kbd>for</kbd> loop exists, we evaluate whether we should start the <kbd>while</kbd> loop (beginning on line 136) over again. We would want to do this if our first signature was less than 32 characters and our <kbd>reset_point</kbd> wasn't the default value of <kbd>3</kbd>. If we have too short a signature, we halve our <kbd>reset_point</kbd> value and re-run our entire calculation again. This means that we need every efficiency possible within this <kbd>while</kbd> loop, as we could be re-processing content over and over:</p>
<pre>170         # If sig1 is too short, change block size and recalculate<br/>171         if len(sig1) &lt; SIGNATURE_LEN / 2 and reset_point &gt; 3:<br/>172             reset_point = reset_point // 2<br/>173             logger.debug("Shortening block size to {}".format(<br/>174                 reset_point))<br/>175         else:<br/>176             done = True</pre>
<p>If our signature length is greater than 32 characters, we exit our <kbd>while</kbd> loop and generate the last character for our signature. If the product of our rolling hash isn't equal to zero, we add the last character to each signature, as shown on lines 180 and 181:</p>
<pre>178     # Add any values from the tail to our hash<br/>179     if rh != 0:<br/>180         sig1 += ALPHABET[trad_hash1 % 64]<br/>181         sig2 += ALPHABET[trad_hash2 % 64]<br/>182 <br/>183     # Close the file and return our new signature<br/>184     open_file.close()<br/>185     return "{}:{}:{}".format(reset_point, sig1, sig2)</pre>
<p>At this point, we can close the file and return our full spamsum/ssdeep signature. This signature has three, hopefully recognizable, parts:</p>
<ul>
<li>Our <kbd>reset_point</kbd> value</li>
<li>The primary signature</li>
<li>The secondary signature</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Providing the output</h1>
                
            
            
                
<p>Our last function, luckily, is a whole lot simpler than the previous one. This function provides output of the signature and filename in one of the supported formats. In the past, we've written separate functions to handle separate formats, though in this case we've opted to place them all in the same function. This design decision is because we want to provide results in near-real time, especially if the user is processing a number of files. Since our logs are redirected to <kbd>STDERR</kbd>, we can use the <kbd>print()</kbd> function to provide results on <kbd>STDOUT</kbd>. This allows flexibility to our users, who can pipe the output into another program (such as grep) and perform additional processing on the results:</p>
<pre>188 def output(sigval, filename, output_type='txt'):<br/>189     """Write the output of the script in the specified format<br/>190     :param sigval (str): Calculated hash<br/>191     :param filename (str): name of the file processed<br/>192     :param output_type (str): Formatter to use for output<br/>193     """<br/>194     if output_type == 'txt':<br/>195         print("{} {}".format(sigval, filename))<br/>196     elif output_type == 'json':<br/>197         print(json.dumps({"sig": sigval, "file": filename}))<br/>198     elif output_type == 'csv':<br/>199         print("{},\"{}\"".format(sigval, filename))<br/>200     else:<br/>201         raise NotImplementedError(<br/>202             "Unsupported output type: {}".format(output_type))</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Running fuzzy_hasher.py</h1>
                
            
            
                
<p>The following screenshot shows us how we can generate our fuzzy hashing on a set of files within a directory and perform post-processing on the output. In this case, we're hiding the log messages by sending <kbd>STDERR</kbd> to <kbd>/dev/null</kbd>. Then, we pipe our output into <kbd>jq</kbd>, a utility that formats and queries JSON data, to present our output in a nicely formatted manner:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-506 image-border" src="img/4e98a7a3-2022-44aa-a1f5-95ca7c1e6af6.png" style="width:96.50em;height:47.92em;"/></p>
<p>There are a few things you may have identified in this output. The first we'll highlight is that the files aren't in alphabetical order. This is because our <kbd>os.walk</kbd> function doesn't preserve alphabetical order by default when it iterates through a path. The second thing is, even though all of these files are identical in size, they vary in block size. What this means is that some of these files (containing random content) didn't have enough blocks and therefore the signatures were too short. This means we needed to halve the block size and try again, so when we move to the comparison component, we can compare files with enough similar blocks. On the other hand, the second signature in the files with the 3,072 blocks (<kbd>file_2</kbd> and <kbd>file_4</kbd>) can be compared in part to the first signature of the other files with block sizes of 6,144. </p>
<p>We've provided these test files for your use and comparison to confirm our implementation matches yours and the output of the next script.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using ssdeep in Python – ssdeep_python.py</h1>
                
            
            
                
<p>This script was tested with both Python 2.7.15 and 3.7.1, and requires the ssdeep version 3.3 third-party library.</p>
<p>As you may have noticed, the prior implementation is almost prohibitively slow. In situations like this, it's best to leverage a language, such as C, that can perform this operation much faster. Luckily for us, spamsum was originally written in C, then further expanded by the ssdeep project, also in C. One of the expansions the ssdeep project provides us with is Python bindings. These bindings allow us to still have our familiar Python function calls while offloading the heavy calculations to our compiled C code. Our next script covers the implementation of the ssdeep library in a Python module to produce the same signatures and handle comparison operations.</p>
<p>In this second example of fuzzy hashing, we're going to implement a similar script using the ssdeep Python library. This allows us to leverage the ssdeep tool and the spamsum algorithm, which has been widely used and accepted in the fields of digital forensics and information security. This code will be the preferred method for fuzzy hashing in most scenarios as it's more efficient with resources and produces more accurate results. This tool has seen wide support in the community, and many ssdeep signatures are available online. For example, the VirusShare and VirusTotal websites host hashes from ssdeep on their sites. This public information can be used to check for known malicious files that match or are similar to executable files on a host machine, without the need to download the malicious files.</p>
<p>One weakness of ssdeep is that it doesn't provide information beyond the matching percentage and can't compare files with significantly different block sizes. This can be an issue as ssdeep automatically generates the block size based on the size of the input file. The process allows ssdeep to run more efficiently and accommodates scaling much better than our script; however, it doesn't provide a manual solution to specify a block size. We could take our prior script and hardcode our block size, though that introduces other (previously discussed) issues.</p>
<p>This script starts the same as the other, with the addition of the new import of the ssdeep library. To install this library, run <kbd>pip install ssdeep==3.3</kbd>, or if that fails, you can run <kbd>BUILD_LIB=1 pip install ssdeep==3.3</kbd> as per the documentation at <a href="https://pypi.python.org/pypi/ssdeep">https://pypi.python.org/pypi/ssdeep</a>. This library wasn't built by the developer of ssdeep, but another member of the community who created the bindings Python needs to communicate with the C-based library. Once installed, it can be imported as seen on line 7:</p>
<pre>001 """Example script that uses the ssdeep python bindings."""<br/>002 import argparse<br/>003 import logging<br/>004 import os<br/>005 import sys<br/>006 <br/>007 import ssdeep</pre>
<p>This iteration has a similar structure to our previous one, though we hand off all of our calculations to the <kbd>ssdeep</kbd> library. Though we may be missing our hashing and comparison functions, we're still using our main and output functions in a very similar manner:</p>
<pre>047 def main():
...
104 def output(): </pre>
<p>Our program flow has also remained similar to our prior iteration, though it's missing the internal hashing function we developed in our prior iteration. As seen in the flow diagram, we still make calls to the <kbd>output()</kbd> function in the <kbd>main()</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-507 image-border" src="img/54931c15-04a4-4b6a-b142-6f5b246441e5.png" style="width:76.42em;height:32.75em;"/></p>
<p>Our argument parsing and logging configurations are nearly identical to the prior script. The major difference is that we've introduced one new file path argument and renamed our argument that accepted files or folders. On line 134, we once more create the <kbd>argparse</kbd> object to handle our two positional arguments and two optional output format and logging flags. The remainder of this code block is consistent with the prior script, with the exception of renaming our log files:</p>
<pre>134 if __name__ == '__main__':<br/>135     parser = argparse.ArgumentParser(<br/>136         description=__description__,<br/>137         epilog='Built by {}. Version {}'.format(<br/>138             ", ".join(__authors__), __date__),<br/>139         formatter_class=argparse.ArgumentDefaultsHelpFormatter<br/>140     )<br/>141     parser.add_argument('KNOWN',<br/>142         help='Path to known file to use to compare')<br/>143     parser.add_argument('COMPARISON',<br/>144         help='Path to file or directory to compare to known. '<br/>145         'Will recurse through all sub directories')</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Revisiting the main() function</h1>
                
            
            
                
<p>This <kbd>main()</kbd> function is very similar to the prior script, though it has a few additional lines of code as we've added some functionality. This script starts the same with checking that the output type is a valid format, as shown on lines 56 through 62. We then add another conditional on line 63 that allows us to print the CSV header row since this output is more complicated than the last iteration:</p>
<pre>047 def main(known_file, comparison, output_type):<br/>048     """<br/>049     The main function handles the main operations of the script<br/>050     :param known_file: path to known file<br/>051     :param comparison: path to look for similar files<br/>052     :param output_type: type of output to provide<br/>053     :return: None<br/>054     """<br/>055 <br/>056     # Check output formats<br/>057     if output_type not in OUTPUT_OPTS:<br/>058         logger.error(<br/>059             "Unsupported output format '{}' selected. Please "<br/>060             "use one of {}".format(<br/>061                 output_type, ", ".join(OUTPUT_OPTS)))<br/>062         sys.exit(2)<br/>063     elif output_type == 'csv':<br/>064         # Special handling for CSV headers<br/>065         print('"similarity","known_file","known_hash",'<br/>066               '"comp_file","comp_hash"')</pre>
<div><p>Now that we've handled the output format validation, let's pivot to our files for comparison. To start, we'll get the absolute path for both our known file and comparison path for consistency to our prior script. Then, on line 73, we check to ensure our known file exists. If it does, we then calculate the ssdeep hash on line 78. This calculation is completely handled by ssdeep; all we need to do is provide a valid file path to the <kbd>hash_from_file()</kbd> method. This method returns a string value containing our ssdeep hash, the same product as our <kbd>fuzz_file()</kbd> function in our prior script. The big difference here is speed improvements through the use of efficient C code running in the <kbd>ssdeep</kbd> module:</p>
<pre>068     # Check provided file paths<br/>069     known_file = os.path.abspath(known_file)<br/>070     comparison = os.path.abspath(comparison)<br/>071<br/>072     # Generate ssdeep signature for known file<br/>073     if not os.path.exists(known_file):<br/>074         logger.error("Error - path {} not found".format(<br/>075             comparison))<br/>076         sys.exit(1)<br/>077<br/>078     known_hash = ssdeep.hash_from_file(known_file)</pre></div>
<p>Now that we have our known hash value, we can evaluate our comparison path. In case this path is a directory, as shown on line 81, we'll walk through the folder and it's subfolders looking for files to process. On line 86, we generate a hash of this comparison file as we had for the known file. The next line introduces the <kbd>compare()</kbd> method, allowing us to provide two hashes for evaluation. This compare method returns an integer between (and including) 0 and 100, representing the confidence that these two files have similar content. We then take all of our parts, including the filenames, hashes, and resulting similarity, and provide them to our output function along with our formatting specification. This logic continues until we've recursively processed all of our files:</p>
<pre>080     # Generate and test ssdeep signature for comparison file(s)<br/>081     if os.path.isdir(comparison):<br/>082         # Process files in folders<br/>083         for root, _, files in os.walk(comparison):<br/>084             for f in files:<br/>085                 file_entry = os.path.join(root, f)<br/>086                 comp_hash = ssdeep.hash_from_file(file_entry)<br/>087                 comp_val = ssdeep.compare(known_hash, comp_hash)<br/>088                 output(known_file, known_hash,<br/>089                        file_entry, comp_hash,<br/>090                        comp_val, output_type)</pre>
<p>Our next conditional handles the same operations, but for a single file. As you can see, it uses the same <kbd>hash_from_file()</kbd> and <kbd>compare()</kbd> functions as in the directory operation. Once all of our values are assigned, we pass them in the same manner to our <kbd>output()</kbd> function. Our final conditional handles the case where an error on input is found, notifying the user and exiting:</p>
<pre><br/>092     elif os.path.isfile(comparison):<br/>093         # Process a single file<br/>094         comp_hash = ssdeep.hash_from_file(comparison)<br/>095         comp_val = ssdeep.compare(known_hash, comp_hash)<br/>096         output(known_file, known_hash, file_entry, comp_hash,<br/>097                comp_val, output_type)<br/>098     else:<br/>099         logger.error("Error - path {} not found".format(<br/>100             comparison))<br/>101         sys.exit(1)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Redesigning our output() function</h1>
                
            
            
                
<p class="mce-root">Our last function is <kbd>output()</kbd>; this function takes our many values and presents them cleanly to the user. Just like our prior script, we'll support TXT, CSV, and JSON output formats. To show a different design for this type of function, we'll use our format specific conditionals to build out a template. This template will then be used to print our contents in a formatted manner. This technique is useful if we plan on changing our output function (in this case <kbd>print()</kbd>) to another output function down the road:</p>
<pre>104 def output(known_file, known_hash, comp_file, comp_hash, comp_val,<br/>105            output_type='txt'):<br/>106     """Write the output of the script in the specified format<br/>107     :param sigval (str): Calculated hash<br/>108     :param filename (str): name of the file processed<br/>109     :param output_type (str): Formatter to use for output<br/>110     """</pre>
<p class="mce-root">To begin, we need to convert our one integer value, <kbd>comp_val</kbd>, into a string for compatibility with our templates. With this complete on line 112, we'll build our template for the text format. The text format gives us the freedom to display the data in a way that we find useful for visual review. The following is one option, though feel free to make modifications.</p>
<p class="mce-root">On lines 113 and 114, we're able to build our template with named placeholders by using the curly braces surrounding our placeholder identifier. Skipping ahead to lines 127 to 132, you can see that when we call <kbd>msg.format()</kbd>, we provide our values as arguments using the same names as our placeholders. This tells the <kbd>format()</kbd> method which placeholder to fill with which value. The main advantage of naming our placeholders is that we can arrange the values however we want when we call the <kbd>format()</kbd> method, and even have the elements in different positions between our template formats:</p>
<pre class="mce-root">111     comp_val = str(comp_val)<br/>112     if output_type == 'txt':<br/>113         msg = "{similarity} - {known_file} {known_hash} | "<br/>114         msg += "{comp_file} {comp_hash}"</pre>
<p class="mce-root">Next is our JSON formatting. The <kbd>json.dumps()</kbd> method is the preferred way to output dictionaries as JSON content, though in this case we'll explore how you can accomplish a similar goal. Using our same templating method, we build out a dictionary where the keys are fixed strings and the values are the placeholders. Since the templating syntax uses a single curly brace to indicate a placeholder, we must escape the single curly brace with a second curly brace. This means our entire JSON object it wrapped in an extra curly brace—don't fear, only one of the two curly braces will display on print:</p>
<pre class="mce-root">115     elif output_type == 'json':<br/>116         msg = '{{"similarity": {similarity}, "known_file": '<br/>117         msg += '"{known_file}", "known_hash": "{known_hash}", '<br/>118         msg += '"comparison_file": "{comp_file}", '<br/>119         msg += '"comparison_hash": "{comp_hash}"}}'</pre>
<p class="mce-root">Lastly, we have our CSV output, which uses the named placeholder templating again. As you may have noticed, we wrapped each value in double quotes to ensure that any commas within the values don't cause formatting issues down the line:</p>
<pre class="mce-root">120     elif output_type == 'csv':<br/>121         msg = '"{similarity}","{known_file}","{known_hash}"'<br/>122         msg += '"{comp_file}","{comp_hash}"'</pre>
<p class="mce-root">The only reason we have our <kbd>msg</kbd> variable on multiple lines here is for word wrapping. There's nothing else stopping you from having one long string as a format template. Lastly, we have our <kbd>else</kbd> conditional, which will catch any unsupported output type:</p>
<pre class="mce-root">123     else:<br/>124         raise NotImplementedError(<br/>125             "Unsupported output type: {}".format(output_type))</pre>
<p class="mce-root">After our conditional, we print out the template with the applied values in place of the placeholders. If we wanted to support a new or alternate format, we could add a new conditional above and create the desired template without needing to re-implement this <kbd>print()</kbd> function:</p>
<pre class="mce-root">127     print(msg.format(<br/>128         similarity=comp_val,<br/>129         known_file=known_file,<br/>130         known_hash=known_hash,<br/>131         comp_file=comp_file,<br/>132         comp_hash=comp_hash))</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Running ssdeep_python.py</h1>
                
            
            
                
<p class="mce-root">We can now run our script, providing, for example, <kbd>test_data/file_3</kbd> as our known file and the whole <kbd>test_data/</kbd> folder as our comparison set. Using the JSON output again, we can see the result of our templating in the two following screenshots:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-508 image-border" src="img/34112c19-7d80-433e-a0c4-c0140a7d8d8a.png" style="width:99.33em;height:55.67em;"/></p>
<p>The following is our continued output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-509 image-border" src="img/125e09b2-8cf2-414b-bdef-fb5f225d80f0.png" style="width:100.00em;height:55.75em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">You'll also notice that this script, using the <kbd>ssdeep</kbd> library, produces the same signatures as our prior implementation! One thing to notice is the speed difference between these two scripts. Using the tool time, we ran our two scripts against the same folder of these six files. As seen in the following screenshot, there's a significant performance boost in using our <kbd>ssdeep</kbd> imported module:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-510 image-border" src="img/25a60288-e966-45e6-b4ea-4e190365c2b0.png" style="width:83.58em;height:23.08em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Additional challenges</h1>
                
            
            
                
<p class="mce-root">You've created a script that implements the spamsum algorithm to generate ssdeep compatible hashes! With this, there are a few additional challenges to pursue.</p>
<p class="mce-root">First, we're providing six sample files, found in the previously mentioned <kbd>test_data/</kbd> directory. These files are available to confirm you're getting the same hashes as those printed and to allow you to perform some additional testing. The <kbd>file_1</kbd>, <kbd>file_2</kbd>, and <kbd>file_3</kbd> files are our originals, whereas the instances with an appended <kbd>a</kbd> are a modified version of the original. The accompanying <kbd>README.md</kbd> file contains a description of the alterations we performed, though in short, we have the following:</p>
<ul>
<li class="mce-root"><kbd>file_1</kbd> with a relocation of some file content to a later portion of the file</li>
<li class="mce-root"><kbd>file_2</kbd> with an insertion in the early portion of the file</li>
<li class="mce-root"><kbd>file_3</kbd> with a removal of the start of the file</li>
</ul>
<p class="mce-root">We encourage you to perform additional testing to learn about how ssdeep responds to different types of alterations. Feel free to further alter the original files and share your findings with the community!</p>
<p class="mce-root">Another challenge is to study the ssdeep or spamsum code and learn how it handles the comparison component with the goal of adding it into the first script.</p>
<p class="mce-root">We can also explore developing code to expose the content of, for example, word documents and generate ssdeep hashes of the document's content instead of the binary file. This can be applied to other file types and doesn't have to be limited to text content. For example, if we discover that an executable is packed, we may also want to generate a fuzzy hash of the unpacked byte content.</p>
<p class="mce-root">Lastly, there are other similarity analysis utilities out there. To name one, the <kbd>sdhash</kbd> utility takes a different approach to identifying similarities between two files. We recommend you spend some time with this utility, running it against your and our provided test data to see how it performs with different types of modifications and alterations. More information on <kbd>sdhash</kbd> is available on the website: <a href="http://roussev.net/sdhash/sdhash.html">http://roussev.net/sdhash/sdhash.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<ul>
<li class="mce-root">Kornblum, J. (2006). <em>Identifying Almost Identical Files Using Context Triggered Piecewise Hashing</em>, Digital Investigation, 91-97. Retrieved October 31, 2015, from <a href="http://dfrws.org/2006/proceedings/12-Kornblum.pdf">http://dfrws.org/2006/proceedings/12-Kornblum.pdf</a></li>
<li class="mce-root">Stevens, M. Karpmanm P. Peyrin, T. (2015), <em>RESEARCHERS URGE: INDUSTRY STANDARD SHA-1 SHOULD BE RETRACTED SOONER</em>, retrieved October 31, 2015, from <a href="https://ee788fc4-a-62cb3a1a-s-sites.googlegroups.com/site/itstheshappening/shappening_PR.pdf">https://ee788fc4-a-62cb3a1a-s-sites.googlegroups.com/site/itstheshappening/shappening_PR.pdf</a></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">Hashing is a critical component of the DFIR workflow. While most use cases of hashing are focused on integrity checking, the use of similarity analysis allows us to learn more about near matches and file relations. This process can provide insight for malware detection, identification of restricted documents in unauthorized locations, and discovery of closely related items based on content only. Through the use of third-party libraries, we're able to lean on the power behind the C languages with the flexibility of the Python interpreter and build powerful tools that are user and developer friendly. The code for this project can be downloaded from GitHub or Packt, as described in the <em>Preface</em>.</p>
<p class="mce-root">A fuzzy hash is a form of metadata, or data about data. Metadata also includes embedded attributes such as document editing time, image geolocation information, and source application. In the next chapter, you'll learn how to extract embedded metadata from various files including images, audio files, and office documents.</p>


            

            
        
    </body></html>