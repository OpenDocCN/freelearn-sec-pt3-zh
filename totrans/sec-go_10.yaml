- en: Web Scraping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information gathering from the web can be useful for many situations. Websites
    can provide a wealth of information. The information can be used to help when
    performing a social engineering attack or a phishing attack. You can find names
    and emails for potential targets, or collect keywords and headers that can help
    to quickly understand the topic or business of a website. You can also potentially
    learn the location of the business, find images and documents, and analyze other
    aspects of a website using web scraping techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the target allows you to create a believable pretext. Pretexting
    is a common technique attackers use to trick an unsuspecting victim into complying
    with a request that compromises the user, their account, or their machine in some
    kind of way. For example, someone researches a company and finds out that it is
    a large company with a centralized IT support department in a specific city. They
    can call or email people at the company, pretending to be a support technician,
    and ask them to perform actions or provide their password. Information from a
    company's public website can contain many details used to set up a pretexting
    situation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Web crawling is another aspect of scraping, which involves following hyperlinks
    to other pages. Breadth-first crawling refers to finding as many different websites
    as you can and following them to find more sites. Depth-first crawling refers
    to crawling a single site to find all pages possible before moving on to the next
    site.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover web scraping and web crawling. We will walk you
    through examples of basic tasks such as finding links, documents, and images,
    looking for hidden files and information, and using a powerful third-party package
    named `goquery`. We will also discuss techniques for mitigating scraping of your
    own websites.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will specifically cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping fundamentals
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String matching
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting HTTP headers from a response
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cookies
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting HTML comments from a page
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for unlisted files on a web server
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying your user agent
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fingerprinting web applications and servers
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the goquery package
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all links in a page
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all document links in a page
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing title and headings of a page
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating most common words used on a page
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all external JavaScript sources of a page
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth-first crawling
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Breadth-first crawling
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting against web scraping
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping fundamentals
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping, as used in this book, is the process of extracting information
    from an HTML-structured page that is intended to be viewed by a human and not
    consumed programmatically. Some services provide an API that is efficient for
    programmatic use, but some websites only provide their information in HTML pages.
    These web scraping examples demonstrate various ways of extracting information
    from HTML. We'll look at basic string matching, then regular expressions, and
    then a powerful package named `goquery`, for web scraping.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所说的网页抓取，是指从HTML结构化页面中提取信息的过程，这些页面是供人类查看的，而非供程序消费。一些服务提供了适合程序化使用的API，但有些网站只提供HTML页面中的信息。这些网页抓取示例展示了从HTML中提取信息的不同方法。我们将从基本的字符串匹配开始，然后是正则表达式，最后是一个强大的名为`goquery`的网页抓取包。
- en: Finding strings in HTTP responses with the strings package
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用strings包在HTTP响应中查找字符串
- en: To get started, let's look at making a basic HTTP request and searching for
    a string using the standard library. First, we will create `http.Client` and set
    any custom variables; for example, whether or not the client should follow redirects,
    what set of cookies it should use, or what transport to use.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们先看一下如何使用标准库发起一个基本的HTTP请求，并查找字符串。首先，我们将创建`http.Client`并设置任何自定义变量；例如，客户端是否应该跟随重定向，应该使用哪些cookie，或者使用什么传输。
- en: The `http.Transport` type implements the network request operations to perform
    the HTTP request and get a response. By default, `http.RoundTripper` is used,
    and this executes a single HTTP request. For the majority of use cases, the default
    transport is just fine. By default, the HTTP proxy from the environment is used,
    but the proxy can also be specified in the transport. This might be useful if
    you want to use multiple proxies. This example does not use a custom `http.Transport`
    type, but I wanted to highlight how `http.Transport` is an embedded type within
    `http.Client`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`http.Transport`类型实现了执行HTTP请求并获取响应的网络请求操作。默认情况下，使用`http.RoundTripper`，它执行单个HTTP请求。对于大多数使用场景，默认的传输就足够了。默认情况下，环境中的HTTP代理会被使用，但代理也可以在传输中指定。如果你想使用多个代理，这可能会很有用。此示例未使用自定义的`http.Transport`类型，但我想强调的是，`http.Transport`是`http.Client`中的一个嵌入类型。'
- en: We are creating a custom `http.Client` type, but only to override the `Timeout`
    field. By default, there is no timeout and an application could hang forever.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个自定义的`http.Client`类型，但仅仅是为了重写`Timeout`字段。默认情况下，没有超时设置，应用程序可能会永远挂起。
- en: 'Another embedded type that can be overridden within `http.Client` is the `http.CookieJar`
    type. Two functions the `http.CookieJar` interface requires are: `SetCookies()`
    and `Cookies()`. The standard library comes with the `net/http/cookiejar` package,
    and it contains a default implementation of `CookieJar`. One use case for multiple
    cookie jars is to log in and store multiple sessions with a website. You can log
    in as many users, and store each session in a cookie jar and use each one, as
    needed. This example does not use a custom cookie jar.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以在`http.Client`中重写的嵌入类型是`http.CookieJar`类型。`http.CookieJar`接口要求的两个函数是：`SetCookies()`和`Cookies()`。标准库中包含了`net/http/cookiejar`包，并且它包含了`CookieJar`的默认实现。多个cookie
    jar的一个使用场景是登录并存储与网站的多个会话。你可以登录多个用户，将每个会话存储在一个cookie jar中，并按需使用它们。此示例未使用自定义cookie
    jar。
- en: HTTP responses contain the body as a reader interface. We can extract the data
    from the reader using any function that accepts a reader interface. This includes
    functions such as the `io.Copy()`, `io.ReadAtLeast()`, `io.ReadlAll()`, and `bufio`
    buffered readers. In this example, `ioutil.ReadAll()` is used to quickly store
    the full contents of the HTTP response into a byte-slice variable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP响应包含一个作为读取器接口的主体。我们可以使用任何接受读取器接口的函数从读取器中提取数据。这些函数包括`io.Copy()`、`io.ReadAtLeast()`、`io.ReadAll()`以及`bufio`缓冲读取器。在此示例中，使用`ioutil.ReadAll()`快速将HTTP响应的完整内容存储到字节切片变量中。
- en: 'The following is the code implementation of this example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using regular expressions to find email addresses in a page
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式在页面中查找电子邮件地址
- en: A regular expression, or regex, is actually a form of language in its own right.
    Essentially, it is a special string that expresses a text search pattern. You
    may be familiar with the asterisk (`*`) when using a shell. Commands such as `ls
    *.txt` use a simple regular expression. The asterisk in this case represents *anything*;
    so any string would match as long as it ended with `.txt`. Regular expressions
    have other symbols besides the asterisk, like the period (`.`), which matches
    any single character as opposed to the asterisk, which will match a string of
    any length. There are even more powerful expressions that can be crafted with
    the handful of symbols that are available.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions have a reputation for being slow. The implementation used
    is guaranteed to run in linear time, as opposed to exponential time, based on
    the input length. This means it will run faster than many other implementations
    of regular expressions that do not provide that guarantee, such as Perl. Russ
    Cox, one of Go's authors, published a deep comparison of the two different approaches
    in 2007, which is available at [https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html).
    This is very important for our use case of searching the contents of an HTML page.
    If the regular expression ran in exponential time, based on the input length,
    it could take quite literally years to perform a search of certain expressions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Learn more about regular expressions in general from [https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
    and the relevant Go documentation at [https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a regular expression that searches for email address links
    embedded in HTML. It will search for any `mailto` links and extract the email
    address. We'll use the default HTTP client and call `http.Get()` instead of creating
    a custom client to modify the timeout.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical email link looks like one of these:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The regular expression used is in this example is this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '`"mailto:.*?["?]`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break this down and examine each part:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '`"mailto:`: This whole piece is just a string literal. The first character
    is a quotation mark (`"`) and has no special meaning in the regular expression.
    It is treated like as a regular character. This means that the regex will begin
    by searching for a quotation mark character first. After the quotation mark is
    the text `mailto` with a colon (`:`). The colon has no special meaning either.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.*?`: The period (`.`) means match any character except a newline. The asterisk
    means continue matching based on the previous symbol (the period) for zero or
    more characters. Directly after the asterisk, is a question mark (`?`). This question
    mark tells the asterisk to be non-greedy. It will match the shortest string possible.
    Without it, the asterisk will continue to match as long as possible, while still
    satisfying the full regular expression. We only want the email address itself
    and not any query parameters such as `?subject`, so we are telling it to do a
    non-greedy or short match.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`["?]`: The last piece of the regular expression is the `["?]` set. The brackets
    tell the regex to match any character encapsulated by the brackets. We only have
    two characters: the quotation mark and the question mark. The question mark here
    has no special meaning and is treated as a regular character. The two characters
    inside the brackets are the two possible characters that deliminate the end of
    the email address. By default, the regex would go with whichever one came last
    and return the longest string possible because the asterisk that preceded it would
    have been greedy. However, because we added the other question mark in the previous
    section directly after the asterisk, it will perform a non-greedy search and stop
    at the first thing that matches a character inside the brackets.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this technique means that we will only find emails that are explicitly
    linked using an `<a>` tag in the HTML. It will not find emails that are just written
    as plaintext in the page. Creating a regular expression to search for an email
    string based on a pattern such as `<word>@<word>.<word>` may seem simple, but
    the nuances between different regular expression implementations and the complex
    variations that emails can have make it difficult to craft a regular expression
    that catches all valid email combinations. If you do a quick search online for
    an example, you will see how many variations there are and how complex they get.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: If you are creating some kind of web service it is important to verify a person's
    email account by sending them an email and having them respond or verify with
    a link in some way. I do not recommend that you ever rely solely on a regular
    expression to determine if an email is valid, and I also recommend that you be
    extremely careful about using regular expressions to perform client-side email
    validation. A user may have a weird email address that is technically valid and
    you may prevent them from signing up to your service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of email addresses that are actually valid according
    to *RFC 822* from 1982:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`*.*@example.com`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$what^the.#!$%@example.com`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`!#$%^&*=()@example.com`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"!@#$%{}^&~*()|/="@example.com`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"hello@example.com"@example.com`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2001, *RFC 2822* replaced *RFC 822*. Out of all the preceding examples, only
    the last two containing an at (`@`) symbol are considered invalid by the newer
    *RFC 2822*. All of the other examples are still valid. Read the original RFCs
    at [https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)
    and [https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Extracting HTTP headers from an HTTP response
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HTTP headers contain metadata and descriptive information about the request
    and response. You can potentially learn a lot about a server by inspecting the
    HTTP headers it serves with a response. You can learn the following things about
    the server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Caching system
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web server
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response type
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framework or content management system
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming language
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spoken language
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security headers
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cookies
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not every web server will return all of those headers, but it is helpful to
    learn as much as you can from the headers. Popular frameworks such as WordPress
    and Drupal will return an `X-Powered-By` header telling you whether it is WordPress
    or Drupal and what version.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The session cookie can give away a lot of information too. A cookie named `PHPSESSID`
    tells you it is most likely a PHP application. Django's default session cookie
    is named `sessionid`, that of Java is `JSESSIONID`, and the session cookie of
    Ruby on Rail follows the `_APPNAME_session` pattern. You can use these clues to
    fingerprint web servers. If you only want the headers and don't need the whole
    body of a page, you can always use the HTTP `HEAD` method instead of HTTP `GET`.
    The `HEAD` method will return only headers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'This example makes a `HEAD` request to a URL and prints out all of its headers.
    The `http.Response` type contains a map of strings to strings named `Header`,
    which contain the key-value pair for each HTTP header:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Setting cookies with an HTTP client
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cookies are an essential component of modern web applications. Cookies are sent
    back and forth between the client and server as HTTP headers. Cookies are just
    text key-value pairs that are stored by the browser client. They are used to store
    persistent data on the client. They can be used to store any text value, but are
    commonly used to store preferences, tokens, and session information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Session cookies usually store a token that matches the token the server has.
    When a user logs in, the server creates a session with an identifying token tied
    to that user. The server then sends the token back to the user in the form of
    a cookie. When the client sends the session token in the form of a cookie, the
    server looks and finds a matching token in the session store, which may be a database,
    a file, or in memory. The session token requires sufficient entropy to ensure
    that it is unique and attackers cannot guess it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: If a user is on a public Wi-Fi network and visits a website that does not use
    SSL, anyone nearby can see the HTTP requests in plaintext. An attacker could steal
    the session cookie and use it in their own requests. When a cookie is sidejacked
    in this fashion, the attacker can impersonate the victim. The server will treat
    them as the already logged in user. The attacker may never learn the password
    and does not need to.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户在公共 Wi-Fi 网络上，并访问一个没有使用 SSL 的网站，附近的任何人都可以看到明文的 HTTP 请求。攻击者可能会窃取会话 cookie，并在自己的请求中使用它。当
    cookie 以这种方式被侧面劫持时，攻击者可以冒充受害者。服务器会将其视为已经登录的用户。攻击者可能永远无法知道密码，而且也不需要知道。
- en: For this reason, it can be useful to log out of websites occasionally and destroy
    any active sessions. Some websites allow you to manually destroy all active sessions.
    If you run a web service, I recommend that you set a reasonable expiration time
    for sessions. Bank websites do a good job of this usually enforcing a short 10-15
    minute expiration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偶尔退出网站并销毁所有活动会话是有用的。有些网站允许你手动销毁所有活动会话。如果你运行一个网络服务，我建议你为会话设置合理的过期时间。银行网站通常会做得很好，强制执行短时间（10-15分钟）的过期时间。
- en: There is a `Set-Cookie` header that a server sends to the client when creating
    a new cookie. The client then sends the cookies back to the server using the `Cookie`
    header.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器在创建新 cookie 时，会发送一个`Set-Cookie`头部到客户端。客户端随后会使用`Cookie`头部将 cookie 发送回服务器。
- en: 'Here is a simple example of cookie headers sent from a server:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务器发送的一个简单的 cookie 头部示例：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is an example header from a client:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自客户的一个示例标题：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are other attributes that a cookie can contain, such as the `Secure` and
    `HttpOnly` flags discussed in [Chapter 9](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml),
    *Web Applications*. Other attributes include an expiration date, a domain, and
    a path. This example is only presenting the simplest application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: cookie 还可以包含其他属性，例如在[第9章](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml)《Web 应用程序》中讨论的`Secure`和`HttpOnly`标志。其他属性包括过期日期、域名和路径。这个示例仅展示了最简单的应用。
- en: In this example, a simple request is made with a custom session cookie. The
    session cookie is what allows you to be *logged in* when making a request to a
    website. This example should serve as a reference for how to make a request with
    a cookie and not a standalone tool. First, the URL is defined just before the
    `main` function. Then, the HTTP request is created first with the HTTP `GET` method
    specified. A nil body is provided since `GET` requests generally don't require
    a body. The new request is then updated with a new header, the cookie. In this
    example, `session_id` is the name of the session cookie, but that will vary depending
    on the web application being interacted with.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，发起了一个带有自定义会话 cookie 的简单请求。会话 cookie 使你在访问网站时能够保持*登录状态*。这个示例应该作为如何使用 cookie
    发起请求的参考，而不是一个独立的工具。首先，在`main`函数之前定义 URL。然后，创建 HTTP 请求，首先指定 HTTP `GET` 方法。由于`GET`请求通常不需要正文，所以提供一个空正文。接着，更新新请求，添加新的头部信息——cookie。在这个示例中，`session_id`
    是会话 cookie 的名称，但这会根据所交互的 web 应用而有所不同。
- en: Once the request is prepared, an HTTP client is created to actually make the
    request and process the response. Note that the HTTP request and the HTTP client
    are separate and independent entities. For example, you can reuse a request multiple
    times, use a request with different clients, and use multiple requests with a
    single client. This allows you to create multiple request objects with different
    session cookies if you need to manage multiple client sessions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求准备好，就创建一个 HTTP 客户端来实际发起请求并处理响应。请注意，HTTP 请求和 HTTP 客户端是独立的实体。例如，你可以多次重用一个请求，使用不同的客户端发送请求，或者使用一个客户端发起多个请求。这使得你可以在需要管理多个客户端会话时创建多个带有不同会话
    cookie 的请求对象。
- en: 'The following is the code implementation of this example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该示例的代码实现：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finding HTML comments in a web page
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中查找 HTML 注释
- en: HTML comments can sometimes hold amazing pieces of information. I have personally
    seen websites with the admin username and password in HTML comments. I have also
    seen an entire menu commented out, but the links still worked and could be reached
    directly. You never know what kind of information a careless developer might leave
    behind.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HTML 注释有时包含一些惊人的信息。我曾亲眼见过一些网站在 HTML 注释中暴露了管理员的用户名和密码。我也曾见过整个菜单被注释掉，但链接仍然有效并且可以直接访问。你永远不知道一个粗心的开发者可能会留下什么信息。
- en: If you are going to leave comments in your code, it is always ideal to leave
    them in the server-side code and not in the client-facing HTML and JavaScript.
    Comment in the PHP, Ruby, Python, or whatever backend code you have. You never
    want to give the client more information than they need in the code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The regular expression used in this program consists of a few special sequences.
    Here is the full regular expression. It essentially says, "match anything between
    the `<!--` and `-->` strings." Let''s examine it piece by piece:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '`<!--(.|\n)*?-->`: The beginning and the end start with `<!--` and `-->`, which
    are the designations for opening and closing an HTML comment. Those are plain
    characters and not special characters to the regular expression.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.|\n)*?`: This can be broken down into two pieces:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.|\n)`: The first part has a few special characters. The parentheses, `()`,
    enclose a set of options. The pipe, `|`, separates the options. The options themselves
    are the dot, `.`, and the newline character, `\n`. The dot means match any character,
    except a newline. Because an HTML comment can span multiple lines, we want to
    match any character, including a newline character. The whole piece, `(.|\n)`
    means match the dot or a newline character.'
  id: totrans-95
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*?`: The asterisk means continue matching the previous character or expression
    zero or more times. Immediately preceding the asterisk is the set of parentheses,
    so it will continue trying to match `(.|\n)`. The question mark tells the asterisk
    to be non-greedy, or return the smallest match possible. Without the question
    mark, to designate it as non-greedy; it will match the largest thing possible,
    which means it will start at the beginning of the first comment in the page, and
    end at the ending of the very last comment in the page, including everything in
    between.'
  id: totrans-96
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Try running this program against some websites and see what kind of HTML comments
    you find. You might be surprised at what kind of information you can uncover.
    For example, the MailChimp signup forms come with an HTML comment that actually
    gives you tips on bypassing the bot signup prevention. The MailChimp signup form
    uses a honeypot field that should not be filled out or it assumes the form was
    submitted by a bot. See what you can find.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will first fetch the URL provided, then use the regular expression
    we walked through earlier to search for HTML comments. Every match found is then
    printed out to standard output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finding unlisted files on a web server
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a popular program called DirBuster, which penetration testers use for
    finding unlisted files. DirBuster is an OWASP project that comes preinstalled
    on Kali, the popular penetration testing Linux distribution. With nothing but
    the standard library, we can create a fast, concurrent, and simple clone of DirBuster
    with just a few lines. More information about DirBuster is available at [https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个流行的程序叫做DirBuster，渗透测试人员用它来查找未列出的文件。DirBuster是一个OWASP项目，预装在Kali上，Kali是流行的渗透测试Linux发行版。仅凭标准库，我们就能用几行代码快速、并发且简单地克隆DirBuster。有关DirBuster的更多信息，请访问[https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project)。
- en: This program is a simple clone of DirBuster that searches for unlisted files
    based on a word list. You will have to create your own word list. A small list
    of example filenames will be provided here to give you some ideas and to use as
    a starting list. Build your list of files based on your own experience and based
    on the source code. Some web applications have files with specific names that
    will allow you to fingerprint which framework is being used. Also look for backup
    files, configuration files, version control files, changelog files, private keys,
    application logs, and anything else that is not intended to be public. You can
    also find prebuilt word lists on the internet, including DirBuster's lists.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序是一个简单的DirBuster克隆，基于单词列表搜索未列出的文件。你需要自己创建单词列表。这里会提供一个小的示例文件名列表，以便给你一些想法并作为起始列表。根据你的经验和源代码来构建文件列表。某些Web应用程序具有特定名称的文件，能让你指纹识别使用的框架。此外，还要查找备份文件、配置文件、版本控制文件、更新日志文件、私钥、应用程序日志以及任何不应公开的文件。你也可以在互联网上找到现成的单词列表，包括DirBuster的列表。
- en: 'Here is a sample list of files that you could search for:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个你可以搜索的文件示例列表：
- en: '`.gitignore`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.gitignore`'
- en: '`.git/HEAD`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.git/HEAD`'
- en: '`id_rsa`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id_rsa`'
- en: '`debug.log`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug.log`'
- en: '`database.sql`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database.sql`'
- en: '`index-old.html`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index-old.html`'
- en: '`backup.zip`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backup.zip`'
- en: '`config.ini`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config.ini`'
- en: '`settings.ini`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.ini`'
- en: '`settings.php.bak`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.php.bak`'
- en: '`CHANGELOG.txt`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CHANGELOG.txt`'
- en: This program will search a domain with the provided word list and report any
    files that do not return a 404 NOT FOUND response. The word list should have filenames
    separated with a newline and have one filename per line. When providing the domain
    name as a parameter, the trailing slash is optional, and the program will behave
    properly with or without the trailing slash on the domain name. The protocol must
    be specified though, so that the request knows whether to use HTTP or HTTPS.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序将使用提供的单词列表搜索域名，并报告任何没有返回404 NOT FOUND响应的文件。单词列表应该以换行符分隔文件名，每行一个文件名。提供域名作为参数时，尾部的斜杠是可选的，程序无论有无尾部斜杠都会正确运行。然而，协议必须被指定，这样请求才能知道是使用HTTP还是HTTPS。
- en: The `url.Parse()` function is used to create a proper URL object. With the URL
    type, you can independently modify `Path` without modifying `Host` or `Scheme`.
    This provides an easy way to update the URL without resorting to manual string
    manipulation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`url.Parse()`函数用于创建一个正确的URL对象。使用URL类型，你可以独立地修改`Path`，而无需修改`Host`或`Scheme`。这提供了一种便捷的方式来更新URL，而无需手动处理字符串。'
- en: 'To read the file line by line, a scanner is used. By default, scanners split
    by newlines, but they can be overridden by calling `scanner.Split()` and providing
    a custom split function. We use the default behavior since the words are expected
    to be provided on separate lines:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要逐行读取文件，使用了扫描器。默认情况下，扫描器按换行符拆分，但可以通过调用`scanner.Split()`并提供自定义拆分函数来覆盖此行为。由于预计单词将单独一行提供，因此我们使用默认行为：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Changing the user agent of a request
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改请求的用户代理
- en: A common technique to block scrapers and crawlers is to block certain user agents.
    Some services will blacklist certain user agents that contain keywords such as `curl`
    and `python`. You can get around most of these by simply changing your user agent
    to `firefox`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的阻止爬虫和抓取器的技术是阻止特定的用户代理。一些服务会将包含如`curl`和`python`等关键词的用户代理列入黑名单。你可以通过简单地将用户代理更改为`firefox`来绕过大多数这些限制。
- en: To set the user agent, you must first create the HTTP request object. The header
    must be set before making the actual request. This means that you can't use the
    shortcut convenience functions such as `http.Get()`. We have to create the client
    and then create a request, and then use the client to `client.Do()` the request.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: This example creates an HTTP request with `http.NewRequest()`, and then modifies
    the request headers to override the `User-Agent` header. You can use this to hide,
    fake, or be honest. To be a good web citizen, I recommend that you create a unique
    user agent for your crawler so that webmasters can throttle or block your bot.
    I also recommend that you include a website or email address in the user agent
    so that webmasters can request to be skipped by your scraper.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fingerprinting web application technology stacks
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fingerprinting a web application is when you try to identify the technology
    that is being used to serve a web application. Fingerprinting can be done at several
    levels. At a lower level, HTTP headers can give clues to what operating system,
    such as Windows or Linux, and what web server, such as Apache or nginx, is running.
    The headers may also give information about the programming language or framework
    being used at the application level. At a higher level, the web application can
    be fingerprinted to identify which JavaScript libraries are being used, whether
    any analytics platforms are being included, any ad networks are being displayed,
    the caching layers in use, and other information. We will first look at the HTTP
    headers, and then cover more complex methods of fingerprinting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting is a critical step in an attack or penetration test because it
    helps narrow down options and determine which paths to take. Identifying what
    technologies are being used also lets you search for known vulnerabilities. If
    a web application is not kept up to date, a simple fingerprinting and vulnerability
    search may be all that is needed for finding and exploiting an already-known vulnerability.
    If nothing else, it helps you learn about the target.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting based on HTTP response headers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I recommend that you inspect the HTTP headers first since they are simple key-value
    pairs, and generally, there are only a few returned with each request. It doesn't
    take very long to go through the headers manually, so you can inspect them first
    before moving on to the application. Fingerprinting at the application level is
    more complicated and we'll talk about that in a moment. Earlier in this chapter,
    there was a section about extracting HTTP headers and printing them out for inspection
    (the *Extracting HTTP headers from an HTTP response* section). You can use that
    program to dump the headers of different web pages and see what you can find.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is simple. Look for keywords. Some headers in particular contain
    the most obvious clues, such as the `X-Powered-By`, `Server`, and `X-Generator`
    headers. The `X-Powered-By` header can contain the name of the framework or **Content
    Management System** (**CMS**) being used, such as WordPress or Drupal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic steps to examining the headers. First, you need to get the
    headers. Use the example provided earlier in this chapter for extracting HTTP
    headers. The second step is to do a string search to look for the keywords. You
    can use `strings.ToUpper()` and `strings.Contains()` to search directly for keywords,
    or use regular expressions. Refer to the earlier examples in this chapter that
    explain how to use regular expressions. Once you are able to search through the
    headers, you just need to be able to generate the list of keywords to search for.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many keywords you can look for. What you search for will depend on
    what you are looking for. I''ll try to cover several broad categories to give
    you ideas on what to look for. The first thing you can try to identify is the
    operating system that the host is running. Here is a sample list of keywords that
    you can find in HTTP headers to indicate the operating system:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '`Linux`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Debian`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fedora`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Red Hat`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CentOS`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ubuntu`'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FreeBSD`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Win32`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Win64`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Darwin`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a list of keywords that will help you determine which web server is
    being used. This is by no means an exhaustive list, but does cover several keywords
    that will yield results if you search the internet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '`Apache`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Nginx`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Microsoft-IIS`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tomcat`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WEBrick`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lighttpd`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IBM HTTP Server`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining which programming language is being used can make a big difference
    in your attack choices. Scripted languages such as PHP are vulnerable to different
    things than a Java server or an ASP.NET application. Here are a few sample keywords
    you can use to search in HTTP headers to identify which language is powering an
    application:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`Python`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ruby`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Perl`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PHP`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASP.NET`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Session cookies are also big giveaways as to what framework or language is
    being used. For example, `PHPSESSID` indicates PHP and `JSESSIONID` indicates
    Java. Here are a few session cookies you can search for:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '`PHPSESSID`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JSESSIONID`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sessionid`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CFID/CFTOKEN`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASP.NET_SessionId`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fingerprinting web applications
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fingerprinting web applications in general covers a much broader scope than
    just looking at the HTTP headers. You can do basic keyword searches in the HTTP
    headers, as just discussed, and learn a lot, but there is also a wealth of information
    in the HTML source code and the contents, or simply the existence, of other files
    on the server.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In the HTML source code, you can look for clues such as the structure of pages
    themselves and the names of classes and IDs of HTML elements. AngularJS applications
    have distinct HTML attributes, such as `ng-app`, that can be used as keywords
    for fingerprinting. Angular is also generally included with a `script` tag, the
    same way other frameworks such as jQuery are included. The `script` tags can also
    be inspected for other clues. Look for things such as Google Analytics, AdSense,
    Yahoo ads, Facebook, Disqus, Twitter, and other third-party JavaScript embedded.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Simply looking at the file extensions in URLs can tell you what language is
    being used. For example, `.php`, `.jsp`, and `.asp` indicate that PHP, Java, and
    ASP are being used, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at a program that finds HTML comments in a web page. Some frameworks
    and CMSes leave an identifiable footer or hidden HTML comment. Sometimes the marker
    is in the form of a small image.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Directory structure can also be another giveaway. It requires familiarity with
    different frameworks first. For example, Drupal stores site information in a directory
    called `/sites/default`. If you attempt to visit that URL and you get a 403 FORBIDDEN
    response and not a 404 NOT FOUND error, you likely found a Drupal-based website.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Look for files such as `wp-cron.php`. In the *Finding unlisted files on a web
    server* section, we looked at finding unlisted files with the DirBuster clone.
    Find a list of unique files that you can use to fingerprint web applications and
    add them to your word list. You can figure out which files to look for by inspecting
    the code bases for different web frameworks. For example, the source code for
    WordPress and Drupal are publicly available. Use the program discussed earlier
    in this chapter for finding unlisted files to search for files. Other unlisted
    files that you can search for are related to documentation, such as `CHANGELOG.txt`,
    `readme.txt`, `readme.md`, `readme.html`, `LICENSE.txt`, `install.txt`, or `install.php`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to get even more detail out of a web application by fingerprinting
    the version of an application that is running. It is much easier if you have access
    to the source code. I will use WordPress as an example since is it so ubiquitous
    and the source is available on GitHub at [https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to find the differences between versions. WordPress is a good example
    because they all come with the `/wp-admin/` directory that contains all the administrative
    interfaces. Inside `/wp-admin/`, there are the `css` and `js` folders with style
    sheets and scripts in them, respectively. These files are publicly accessible
    when a site is hosted on a server. Use the `diff` command on these folders to
    identify which versions introduce new files, which versions remove files, and
    which versions modify existing files. With all that information combined, you
    can generally narrow down applications to a specific version or to at least a
    small range of versions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'As a contrived example, let''s say version 1.0 contains only one file: `main.js`.
    Version 1.1 introduces a second file: `utility.js`. Version 1.3 removes both of
    those and replaces them with a single file: `master.js`. You can make HTTP requests
    to the web server for all three files: `main.js`, `utility.js`, and `master.js`.
    Based on which files are found with a 200 OK error and which files return a 404
    NOT FOUND error, you can determine which version is running.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If the same files are present across multiple versions, you can inspect deeper
    into the contents of the files. Either do a byte-by-byte comparison or hash the
    files and compare the checksums. Hashing and examples of hashing are covered in [Chapter
    6](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml), *Cryptography*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, identifying the version can be much simpler than that whole process
    just described. Sometimes there is a `CHANGELOG.txt` or `readme.html` file that
    will tell you exactly which version is running without having to do any work.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: How to prevent fingerprinting of your applications
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As demonstrated earlier, there are multiple ways to fingerprint applications
    at many different levels of the technology stack. The first question you should
    really ask yourself is, "Do I need to prevent fingerprinting?" In general, trying
    to prevent fingerprinting is a form of obfuscation. Obfuscation is a bit controversial,
    but I think everyone does agree that obfuscation is not security in the same way
    that encoding is not encryption. It may slow down, limit information, or confuse
    an attacker temporarily, but it does not truly prevent any vulnerability from
    being exploited. Now, I'm not saying that there is no benefit at all from obfuscation,
    but it can never be relied on by itself. Obfuscation is simply a thin layer of
    concealment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, you don't want to give away too much information about your application,
    such as debug output or configuration settings, but some information is going
    to be available no matter what when a service is available on the network. You
    will have to make a choice about how much time and effort you want to put into
    hiding information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Some people go as far as outputting false information to mislead attackers.
    Personally, putting out fake headers is not on my checklist of things to do when
    hardening a server. One thing I recommend that you do is to remove any extra files
    as mentioned earlier. Files such as changelog files, default setting files, installation
    files, and documentation files should all be removed before deployment. Don't
    publicly serve the files that are not required for the application to work.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscation is a topic that warrants its own chapter or even its own book. There
    are obfuscation competitions dedicated to awarding the most creative and bizarre
    forms of obfuscation. There are some tools that help you obfuscate JavaScript
    code, but on the flip side, there are also deobfuscation tools.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Using the goquery package for web scraping
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `goquery` package is not part of the standard library, but is available
    on GitHub. It is intended to work similar to jQuery—a popular JavaScript framework
    for interacting with the HTML DOM. As demonstrated in the previous sections, trying
    to search with string matching and regular expressions is both tedious and complicated.
    The `goquery` package makes it much easier to work with HTML content and search
    for specific elements. The reason I suggest this package is because it is modelled
    after the very popular jQuery framework that many people are already familiar
    with.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the `goquery` package with the `go get` command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The documentation is available at [https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Listing all hyperlinks in a page
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the introduction to the `goquery` package, we''ll look at a common and
    simple task. We will find all hyperlinks in a page and print them out. A typical
    link looks something like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In HTML, the `a` tag stands for **anchor** and the `href` attribute stands
    for **hyperlink reference**. It is possible to have an anchor tag with no `href`
    attribute but only a `name` attribute. These are called bookmarks, or named anchors,
    and are used to jump to a location on the same page. We will ignore these since
    they only link within the same page. The `target` attribute is just an optional
    one specifying which window or tab to open the link in. We are only interested
    in the `href` value for this example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finding documents in a web page
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documents are also points of interest. You might want to scrape a web page and
    look for documents. Word processor documents, spreadsheets, slideshow decks, CSV,
    text, and other files can contain useful information for a variety of purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The following example will search through a URL and search for documents based
    on the file extensions in the links. A global variable is defined at the top for
    convenience with the list of all extensions that should be searched for. Customize
    the list of extensions to search for your target file types. Consider extending
    the application to take a list of file extensions in from a file instead of being
    hardcoded. What other file extensions would you look for when trying to find sensitive
    information?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Listing page title and headings
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Headings are the primary structural elements that define the hierarchy of a
    web page, with `<h1>` being the top level and `<h6>` being the lowest or deepest
    level of the hierarchy. The title, defined in the `<title>` tag, of an HTML page
    is what gets displayed in the browser title bar, and it is not part of the rendered
    page.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: By listing the title and headings, you can quickly get an idea of what the topic
    of the page is, assuming that they properly formatted their HTML. There is only
    supposed to be one `<title>` and one `<h1>` tag, but not everyone conforms to
    the standards.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'This program loads a web page and then prints the title and all headings to
    standard output. Try running this program against a few URLs and see whether you
    are able to get a quick idea of the contents just by looking at the headings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Crawling pages on the site that store the most common words
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This program prints out a list of all the words used on a web page along with
    the count of how many times each word appeared in the page. This will search all
    paragraph tags. If you search the whole body, it will treat all the HTML code
    as words, which clutters the data and does not really help you understand the
    content of the site. It trims the spaces, commas, periods, tabs, and newlines
    from strings. It also converts all words to lowercase in an attempt to normalize
    the data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'For each paragraph it finds, it will split the text contents apart. Each word
    is stored in a map that maps the string to an integer count. In the end, the map
    is printed out, listing each word and how many times it was seen on the page:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Printing a list of external JavaScript files in a page
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspecting the URLs of JavaScript files that are included on a page can help
    if you are trying to fingerprint an application or determine what third-party
    libraries are being loaded. This program will list the external JavaScript files
    referenced in a web page. External JavaScript files might be hosted on the same
    domain, or might be loaded from a remote site. It inspects the `src` attribute
    of all the `script` tags.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if an HTML page had the following tag:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The URL of the `src` attribute is what would be printed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that URLs in the `src` attribute may be fully qualified or relative URLs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The following program loads a URL and then looks for all the `script` tags.
    It will print the `src` attribute for each script it finds. This will only look
    for scripts that are linked externally. To print inline scripts, refer to the
    comment at the bottom of the file regarding `script.Text()`. Try running this
    against some websites you visit frequently and see how many external and third-party
    scripts they embed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This example looks for external scripts referenced by the `src` attribute, but
    some scripts are written directly in the HTML between the opening and closing
    `script` tags. These types of inline script won't have a `src` attribute referencing.
    Get inline script text using the `.Text()` function on the `goquery` object. Refer
    to the bottom of this example, where `script.Text()` is mentioned.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The reason this program does not print out the inline scripts and instead focuses
    only on the externally loaded scripts is because that is where a lot of vulnerabilities
    are introduced. Loading remote JavaScript is risky and should be done with trusted
    sources only. Even then, we don't get 100% assurance that the remote content provider
    will never be compromised and serve malicious code. Consider a large corporation
    such as Yahoo! who has acknowledged publicly that their systems have been compromised
    in the past. Yahoo! also has an ad network that hosts a **Content Delivery Network**
    (**CDN**) that serves JavaScript files to a large network of websites. This would
    be a prime target for attackers. Consider these risks when including remote JavaScript
    files in a sensitive customer portal.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Depth-first crawling
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depth-first crawling is when you prioritize links on the same domain over links
    that lead to other domains. In this program, external links are completely ignored,
    and only paths on the same domain or relative links are followed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In this example, unique paths are stored in a slice and printed all together
    at the end. Any errors encountered during the crawl are ignored. Errors are encountered
    often due to malformed links, and we don't want the whole program to exit on errors
    like that.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Instead of trying to parse URLs manually using string functions, the `url.Parse()`
    function is utilized. It does the work of splitting apart the host from the path.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'When crawling, any query strings and fragments are ignored to reduce duplicates.
    Query strings are designated with the question mark in the URL, and fragments,
    also called bookmarks, are designated with the pound or hash sign. This program
    is single-threaded and does not use goroutines:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Breadth-first crawling
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breadth-first crawling is when priority is given to finding new domains and
    spreading out as far as possible, as opposed to continuing through a single domain
    in a depth-first manner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Writing a breadth-first crawler will be left as an exercise for the reader based
    on the information provided in this chapter. It is not very different from the
    depth-first crawler in the previous section, except that it should prioritize
    URLs that point to domains that have not been seen before.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of notes to keep in mind. If you're not careful and you don't
    set a maximum limit, you could potentially end up crawling petabytes of data!
    You might choose to ignore subdomains, or you can enter a site that has infinite
    subdomains and you will never leave.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: How to protect against web scraping
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is difficult, if not impossible, to completely prevent web scraping. If you
    serve the information from the web server, there will be a way to extract the
    data programmatically somehow. There are only hurdles you can put in the way.
    It amounts to obfuscation, which you could argue is not worth the effort.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript makes it more difficult, but not impossible since Selenium can drive
    real web browsers, and frameworks such as PhantomJS can be used to execute the
    JavaScript.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Requiring authentication can help limit the amount of scraping done. Rate limiting
    can also provide some relief. Rate limiting can be done using tools such as iptables
    or done at the application level, based on the IP address or user session.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Checking the user agent provided by the client is a shallow measure, but can
    help a bit. Discard requests that come with user agents that include keywords
    such as `curl`, `wget`, `go`, `python`, `ruby`, and `perl`. Blocking or ignoring
    these requests can prevent simple bots from scraping your site, but the client
    can fake or omit their user agent so that it is easy to bypass.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take it even further, you can make the HTML ID and class names
    dynamic so that they can't be used to find specific information. Change your HTML
    structure and naming frequently to play the *cat-and-mouse* game to make it more
    work than it is worth for the scraper. This is not a real solution, and I wouldn't
    recommend it, but it is worth mentioning, as it is annoying in the eyes of the
    scraper.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: You can use JavaScript to check information about the client, such as screen
    size, before presenting data. If the screen size is 1 x 1 or 0 × 0, or something
    strange, you can assume that it is a bot and refuse to render content.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Honeypot forms are another method of detecting bot behavior. Hide form fields
    with CSS or a `hidden` attribute, and check whether values have been provided
    in those fields. If data is in these fields, assume that a bot is filling out
    all the fields and ignore the request.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use images to store information instead of text. For example,
    if you output only the image of a pie chart, it is much more difficult for someone
    to scrape the data than when you output the data as a JSON object and have JavaScript
    render the pie chart. The scraper can grab the JSON data directly. Text can be
    placed in images as well to prevent text from being scraped and to prevent keyword
    text searches, but **Optical Character Recognition** (**OCR**) can get around
    that with some extra effort.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, some of the preceding techniques can be useful.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having read this chapter, you should now understand the fundamentals of web
    scraping, such as performing an HTTP `GET` request and searching for a string
    using string matching or regular expressions to find HTML comments, emails, and
    other keywords. You should also understand how to extract the HTTP headers and
    set custom headers to set cookies and custom user agent strings. Moreover, you
    should understand the basic concepts of fingerprinting and have some idea of how
    to gather information about a web application based on the source code provided.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Having worked through this chapter, you should also understand the basics of
    using the `goquery` package to find HTML elements in the DOM in a jQuery style.
    You should feel comfortable finding links in a web page, finding documents, listing
    title and headers, finding JavaScript files, and finding the difference between
    breadth-first and depth-first crawling.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: A note about scraping public websites—be respectful. Don't produce unreasonable
    amounts of traffic to websites by sending huge batches or letting a crawler go
    uninhibited. Set reasonable rate limits and maximum page count limits on programs
    you write as to not overburden remote servers. If you are scraping for data, always
    check to see if an API is available instead. APIs are much more efficient and
    intended to be used programmatically.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of any other way to apply the tools examined in this chapter?
    Can you think of any additional features you can add to the examples provided?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the methods of host discovery and enumeration.
    We will cover things such as TCP sockets, proxies, port scanning, banner grabbing,
    and fuzzing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
