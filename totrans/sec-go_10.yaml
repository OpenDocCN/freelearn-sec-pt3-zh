- en: Web Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information gathering from the web can be useful for many situations. Websites
    can provide a wealth of information. The information can be used to help when
    performing a social engineering attack or a phishing attack. You can find names
    and emails for potential targets, or collect keywords and headers that can help
    to quickly understand the topic or business of a website. You can also potentially
    learn the location of the business, find images and documents, and analyze other
    aspects of a website using web scraping techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the target allows you to create a believable pretext. Pretexting
    is a common technique attackers use to trick an unsuspecting victim into complying
    with a request that compromises the user, their account, or their machine in some
    kind of way. For example, someone researches a company and finds out that it is
    a large company with a centralized IT support department in a specific city. They
    can call or email people at the company, pretending to be a support technician,
    and ask them to perform actions or provide their password. Information from a
    company's public website can contain many details used to set up a pretexting
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: Web crawling is another aspect of scraping, which involves following hyperlinks
    to other pages. Breadth-first crawling refers to finding as many different websites
    as you can and following them to find more sites. Depth-first crawling refers
    to crawling a single site to find all pages possible before moving on to the next
    site.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover web scraping and web crawling. We will walk you
    through examples of basic tasks such as finding links, documents, and images,
    looking for hidden files and information, and using a powerful third-party package
    named `goquery`. We will also discuss techniques for mitigating scraping of your
    own websites.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will specifically cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String matching
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting HTTP headers from a response
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cookies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting HTML comments from a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for unlisted files on a web server
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying your user agent
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fingerprinting web applications and servers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the goquery package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all links in a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all document links in a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing title and headings of a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating most common words used on a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing all external JavaScript sources of a page
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth-first crawling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Breadth-first crawling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting against web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping, as used in this book, is the process of extracting information
    from an HTML-structured page that is intended to be viewed by a human and not
    consumed programmatically. Some services provide an API that is efficient for
    programmatic use, but some websites only provide their information in HTML pages.
    These web scraping examples demonstrate various ways of extracting information
    from HTML. We'll look at basic string matching, then regular expressions, and
    then a powerful package named `goquery`, for web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Finding strings in HTTP responses with the strings package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started, let's look at making a basic HTTP request and searching for
    a string using the standard library. First, we will create `http.Client` and set
    any custom variables; for example, whether or not the client should follow redirects,
    what set of cookies it should use, or what transport to use.
  prefs: []
  type: TYPE_NORMAL
- en: The `http.Transport` type implements the network request operations to perform
    the HTTP request and get a response. By default, `http.RoundTripper` is used,
    and this executes a single HTTP request. For the majority of use cases, the default
    transport is just fine. By default, the HTTP proxy from the environment is used,
    but the proxy can also be specified in the transport. This might be useful if
    you want to use multiple proxies. This example does not use a custom `http.Transport`
    type, but I wanted to highlight how `http.Transport` is an embedded type within
    `http.Client`.
  prefs: []
  type: TYPE_NORMAL
- en: We are creating a custom `http.Client` type, but only to override the `Timeout`
    field. By default, there is no timeout and an application could hang forever.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another embedded type that can be overridden within `http.Client` is the `http.CookieJar`
    type. Two functions the `http.CookieJar` interface requires are: `SetCookies()`
    and `Cookies()`. The standard library comes with the `net/http/cookiejar` package,
    and it contains a default implementation of `CookieJar`. One use case for multiple
    cookie jars is to log in and store multiple sessions with a website. You can log
    in as many users, and store each session in a cookie jar and use each one, as
    needed. This example does not use a custom cookie jar.'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP responses contain the body as a reader interface. We can extract the data
    from the reader using any function that accepts a reader interface. This includes
    functions such as the `io.Copy()`, `io.ReadAtLeast()`, `io.ReadlAll()`, and `bufio`
    buffered readers. In this example, `ioutil.ReadAll()` is used to quickly store
    the full contents of the HTTP response into a byte-slice variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using regular expressions to find email addresses in a page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A regular expression, or regex, is actually a form of language in its own right.
    Essentially, it is a special string that expresses a text search pattern. You
    may be familiar with the asterisk (`*`) when using a shell. Commands such as `ls
    *.txt` use a simple regular expression. The asterisk in this case represents *anything*;
    so any string would match as long as it ended with `.txt`. Regular expressions
    have other symbols besides the asterisk, like the period (`.`), which matches
    any single character as opposed to the asterisk, which will match a string of
    any length. There are even more powerful expressions that can be crafted with
    the handful of symbols that are available.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions have a reputation for being slow. The implementation used
    is guaranteed to run in linear time, as opposed to exponential time, based on
    the input length. This means it will run faster than many other implementations
    of regular expressions that do not provide that guarantee, such as Perl. Russ
    Cox, one of Go's authors, published a deep comparison of the two different approaches
    in 2007, which is available at [https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html).
    This is very important for our use case of searching the contents of an HTML page.
    If the regular expression ran in exponential time, based on the input length,
    it could take quite literally years to perform a search of certain expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Learn more about regular expressions in general from [https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
    and the relevant Go documentation at [https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/).
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a regular expression that searches for email address links
    embedded in HTML. It will search for any `mailto` links and extract the email
    address. We'll use the default HTTP client and call `http.Get()` instead of creating
    a custom client to modify the timeout.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical email link looks like one of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The regular expression used is in this example is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"mailto:.*?["?]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break this down and examine each part:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"mailto:`: This whole piece is just a string literal. The first character
    is a quotation mark (`"`) and has no special meaning in the regular expression.
    It is treated like as a regular character. This means that the regex will begin
    by searching for a quotation mark character first. After the quotation mark is
    the text `mailto` with a colon (`:`). The colon has no special meaning either.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.*?`: The period (`.`) means match any character except a newline. The asterisk
    means continue matching based on the previous symbol (the period) for zero or
    more characters. Directly after the asterisk, is a question mark (`?`). This question
    mark tells the asterisk to be non-greedy. It will match the shortest string possible.
    Without it, the asterisk will continue to match as long as possible, while still
    satisfying the full regular expression. We only want the email address itself
    and not any query parameters such as `?subject`, so we are telling it to do a
    non-greedy or short match.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`["?]`: The last piece of the regular expression is the `["?]` set. The brackets
    tell the regex to match any character encapsulated by the brackets. We only have
    two characters: the quotation mark and the question mark. The question mark here
    has no special meaning and is treated as a regular character. The two characters
    inside the brackets are the two possible characters that deliminate the end of
    the email address. By default, the regex would go with whichever one came last
    and return the longest string possible because the asterisk that preceded it would
    have been greedy. However, because we added the other question mark in the previous
    section directly after the asterisk, it will perform a non-greedy search and stop
    at the first thing that matches a character inside the brackets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this technique means that we will only find emails that are explicitly
    linked using an `<a>` tag in the HTML. It will not find emails that are just written
    as plaintext in the page. Creating a regular expression to search for an email
    string based on a pattern such as `<word>@<word>.<word>` may seem simple, but
    the nuances between different regular expression implementations and the complex
    variations that emails can have make it difficult to craft a regular expression
    that catches all valid email combinations. If you do a quick search online for
    an example, you will see how many variations there are and how complex they get.
  prefs: []
  type: TYPE_NORMAL
- en: If you are creating some kind of web service it is important to verify a person's
    email account by sending them an email and having them respond or verify with
    a link in some way. I do not recommend that you ever rely solely on a regular
    expression to determine if an email is valid, and I also recommend that you be
    extremely careful about using regular expressions to perform client-side email
    validation. A user may have a weird email address that is technically valid and
    you may prevent them from signing up to your service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of email addresses that are actually valid according
    to *RFC 822* from 1982:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*.*@example.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$what^the.#!$%@example.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`!#$%^&*=()@example.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"!@#$%{}^&~*()|/="@example.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"hello@example.com"@example.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2001, *RFC 2822* replaced *RFC 822*. Out of all the preceding examples, only
    the last two containing an at (`@`) symbol are considered invalid by the newer
    *RFC 2822*. All of the other examples are still valid. Read the original RFCs
    at [https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)
    and [https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Extracting HTTP headers from an HTTP response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HTTP headers contain metadata and descriptive information about the request
    and response. You can potentially learn a lot about a server by inspecting the
    HTTP headers it serves with a response. You can learn the following things about
    the server:'
  prefs: []
  type: TYPE_NORMAL
- en: Caching system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framework or content management system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spoken language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cookies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not every web server will return all of those headers, but it is helpful to
    learn as much as you can from the headers. Popular frameworks such as WordPress
    and Drupal will return an `X-Powered-By` header telling you whether it is WordPress
    or Drupal and what version.
  prefs: []
  type: TYPE_NORMAL
- en: The session cookie can give away a lot of information too. A cookie named `PHPSESSID`
    tells you it is most likely a PHP application. Django's default session cookie
    is named `sessionid`, that of Java is `JSESSIONID`, and the session cookie of
    Ruby on Rail follows the `_APPNAME_session` pattern. You can use these clues to
    fingerprint web servers. If you only want the headers and don't need the whole
    body of a page, you can always use the HTTP `HEAD` method instead of HTTP `GET`.
    The `HEAD` method will return only headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example makes a `HEAD` request to a URL and prints out all of its headers.
    The `http.Response` type contains a map of strings to strings named `Header`,
    which contain the key-value pair for each HTTP header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Setting cookies with an HTTP client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cookies are an essential component of modern web applications. Cookies are sent
    back and forth between the client and server as HTTP headers. Cookies are just
    text key-value pairs that are stored by the browser client. They are used to store
    persistent data on the client. They can be used to store any text value, but are
    commonly used to store preferences, tokens, and session information.
  prefs: []
  type: TYPE_NORMAL
- en: Session cookies usually store a token that matches the token the server has.
    When a user logs in, the server creates a session with an identifying token tied
    to that user. The server then sends the token back to the user in the form of
    a cookie. When the client sends the session token in the form of a cookie, the
    server looks and finds a matching token in the session store, which may be a database,
    a file, or in memory. The session token requires sufficient entropy to ensure
    that it is unique and attackers cannot guess it.
  prefs: []
  type: TYPE_NORMAL
- en: If a user is on a public Wi-Fi network and visits a website that does not use
    SSL, anyone nearby can see the HTTP requests in plaintext. An attacker could steal
    the session cookie and use it in their own requests. When a cookie is sidejacked
    in this fashion, the attacker can impersonate the victim. The server will treat
    them as the already logged in user. The attacker may never learn the password
    and does not need to.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it can be useful to log out of websites occasionally and destroy
    any active sessions. Some websites allow you to manually destroy all active sessions.
    If you run a web service, I recommend that you set a reasonable expiration time
    for sessions. Bank websites do a good job of this usually enforcing a short 10-15
    minute expiration.
  prefs: []
  type: TYPE_NORMAL
- en: There is a `Set-Cookie` header that a server sends to the client when creating
    a new cookie. The client then sends the cookies back to the server using the `Cookie`
    header.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of cookie headers sent from a server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example header from a client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There are other attributes that a cookie can contain, such as the `Secure` and
    `HttpOnly` flags discussed in [Chapter 9](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml),
    *Web Applications*. Other attributes include an expiration date, a domain, and
    a path. This example is only presenting the simplest application.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, a simple request is made with a custom session cookie. The
    session cookie is what allows you to be *logged in* when making a request to a
    website. This example should serve as a reference for how to make a request with
    a cookie and not a standalone tool. First, the URL is defined just before the
    `main` function. Then, the HTTP request is created first with the HTTP `GET` method
    specified. A nil body is provided since `GET` requests generally don't require
    a body. The new request is then updated with a new header, the cookie. In this
    example, `session_id` is the name of the session cookie, but that will vary depending
    on the web application being interacted with.
  prefs: []
  type: TYPE_NORMAL
- en: Once the request is prepared, an HTTP client is created to actually make the
    request and process the response. Note that the HTTP request and the HTTP client
    are separate and independent entities. For example, you can reuse a request multiple
    times, use a request with different clients, and use multiple requests with a
    single client. This allows you to create multiple request objects with different
    session cookies if you need to manage multiple client sessions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finding HTML comments in a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HTML comments can sometimes hold amazing pieces of information. I have personally
    seen websites with the admin username and password in HTML comments. I have also
    seen an entire menu commented out, but the links still worked and could be reached
    directly. You never know what kind of information a careless developer might leave
    behind.
  prefs: []
  type: TYPE_NORMAL
- en: If you are going to leave comments in your code, it is always ideal to leave
    them in the server-side code and not in the client-facing HTML and JavaScript.
    Comment in the PHP, Ruby, Python, or whatever backend code you have. You never
    want to give the client more information than they need in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The regular expression used in this program consists of a few special sequences.
    Here is the full regular expression. It essentially says, "match anything between
    the `<!--` and `-->` strings." Let''s examine it piece by piece:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<!--(.|\n)*?-->`: The beginning and the end start with `<!--` and `-->`, which
    are the designations for opening and closing an HTML comment. Those are plain
    characters and not special characters to the regular expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.|\n)*?`: This can be broken down into two pieces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.|\n)`: The first part has a few special characters. The parentheses, `()`,
    enclose a set of options. The pipe, `|`, separates the options. The options themselves
    are the dot, `.`, and the newline character, `\n`. The dot means match any character,
    except a newline. Because an HTML comment can span multiple lines, we want to
    match any character, including a newline character. The whole piece, `(.|\n)`
    means match the dot or a newline character.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*?`: The asterisk means continue matching the previous character or expression
    zero or more times. Immediately preceding the asterisk is the set of parentheses,
    so it will continue trying to match `(.|\n)`. The question mark tells the asterisk
    to be non-greedy, or return the smallest match possible. Without the question
    mark, to designate it as non-greedy; it will match the largest thing possible,
    which means it will start at the beginning of the first comment in the page, and
    end at the ending of the very last comment in the page, including everything in
    between.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Try running this program against some websites and see what kind of HTML comments
    you find. You might be surprised at what kind of information you can uncover.
    For example, the MailChimp signup forms come with an HTML comment that actually
    gives you tips on bypassing the bot signup prevention. The MailChimp signup form
    uses a honeypot field that should not be filled out or it assumes the form was
    submitted by a bot. See what you can find.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will first fetch the URL provided, then use the regular expression
    we walked through earlier to search for HTML comments. Every match found is then
    printed out to standard output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finding unlisted files on a web server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a popular program called DirBuster, which penetration testers use for
    finding unlisted files. DirBuster is an OWASP project that comes preinstalled
    on Kali, the popular penetration testing Linux distribution. With nothing but
    the standard library, we can create a fast, concurrent, and simple clone of DirBuster
    with just a few lines. More information about DirBuster is available at [https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project).
  prefs: []
  type: TYPE_NORMAL
- en: This program is a simple clone of DirBuster that searches for unlisted files
    based on a word list. You will have to create your own word list. A small list
    of example filenames will be provided here to give you some ideas and to use as
    a starting list. Build your list of files based on your own experience and based
    on the source code. Some web applications have files with specific names that
    will allow you to fingerprint which framework is being used. Also look for backup
    files, configuration files, version control files, changelog files, private keys,
    application logs, and anything else that is not intended to be public. You can
    also find prebuilt word lists on the internet, including DirBuster's lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample list of files that you could search for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.gitignore`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.git/HEAD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id_rsa`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`debug.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`database.sql`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index-old.html`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backup.zip`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config.ini`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`settings.ini`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`settings.php.bak`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CHANGELOG.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This program will search a domain with the provided word list and report any
    files that do not return a 404 NOT FOUND response. The word list should have filenames
    separated with a newline and have one filename per line. When providing the domain
    name as a parameter, the trailing slash is optional, and the program will behave
    properly with or without the trailing slash on the domain name. The protocol must
    be specified though, so that the request knows whether to use HTTP or HTTPS.
  prefs: []
  type: TYPE_NORMAL
- en: The `url.Parse()` function is used to create a proper URL object. With the URL
    type, you can independently modify `Path` without modifying `Host` or `Scheme`.
    This provides an easy way to update the URL without resorting to manual string
    manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the file line by line, a scanner is used. By default, scanners split
    by newlines, but they can be overridden by calling `scanner.Split()` and providing
    a custom split function. We use the default behavior since the words are expected
    to be provided on separate lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Changing the user agent of a request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common technique to block scrapers and crawlers is to block certain user agents.
    Some services will blacklist certain user agents that contain keywords such as `curl`
    and `python`. You can get around most of these by simply changing your user agent
    to `firefox`.
  prefs: []
  type: TYPE_NORMAL
- en: To set the user agent, you must first create the HTTP request object. The header
    must be set before making the actual request. This means that you can't use the
    shortcut convenience functions such as `http.Get()`. We have to create the client
    and then create a request, and then use the client to `client.Do()` the request.
  prefs: []
  type: TYPE_NORMAL
- en: This example creates an HTTP request with `http.NewRequest()`, and then modifies
    the request headers to override the `User-Agent` header. You can use this to hide,
    fake, or be honest. To be a good web citizen, I recommend that you create a unique
    user agent for your crawler so that webmasters can throttle or block your bot.
    I also recommend that you include a website or email address in the user agent
    so that webmasters can request to be skipped by your scraper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fingerprinting web application technology stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fingerprinting a web application is when you try to identify the technology
    that is being used to serve a web application. Fingerprinting can be done at several
    levels. At a lower level, HTTP headers can give clues to what operating system,
    such as Windows or Linux, and what web server, such as Apache or nginx, is running.
    The headers may also give information about the programming language or framework
    being used at the application level. At a higher level, the web application can
    be fingerprinted to identify which JavaScript libraries are being used, whether
    any analytics platforms are being included, any ad networks are being displayed,
    the caching layers in use, and other information. We will first look at the HTTP
    headers, and then cover more complex methods of fingerprinting.
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting is a critical step in an attack or penetration test because it
    helps narrow down options and determine which paths to take. Identifying what
    technologies are being used also lets you search for known vulnerabilities. If
    a web application is not kept up to date, a simple fingerprinting and vulnerability
    search may be all that is needed for finding and exploiting an already-known vulnerability.
    If nothing else, it helps you learn about the target.
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting based on HTTP response headers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I recommend that you inspect the HTTP headers first since they are simple key-value
    pairs, and generally, there are only a few returned with each request. It doesn't
    take very long to go through the headers manually, so you can inspect them first
    before moving on to the application. Fingerprinting at the application level is
    more complicated and we'll talk about that in a moment. Earlier in this chapter,
    there was a section about extracting HTTP headers and printing them out for inspection
    (the *Extracting HTTP headers from an HTTP response* section). You can use that
    program to dump the headers of different web pages and see what you can find.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is simple. Look for keywords. Some headers in particular contain
    the most obvious clues, such as the `X-Powered-By`, `Server`, and `X-Generator`
    headers. The `X-Powered-By` header can contain the name of the framework or **Content
    Management System** (**CMS**) being used, such as WordPress or Drupal.
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic steps to examining the headers. First, you need to get the
    headers. Use the example provided earlier in this chapter for extracting HTTP
    headers. The second step is to do a string search to look for the keywords. You
    can use `strings.ToUpper()` and `strings.Contains()` to search directly for keywords,
    or use regular expressions. Refer to the earlier examples in this chapter that
    explain how to use regular expressions. Once you are able to search through the
    headers, you just need to be able to generate the list of keywords to search for.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many keywords you can look for. What you search for will depend on
    what you are looking for. I''ll try to cover several broad categories to give
    you ideas on what to look for. The first thing you can try to identify is the
    operating system that the host is running. Here is a sample list of keywords that
    you can find in HTTP headers to indicate the operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Linux`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Debian`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fedora`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Red Hat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CentOS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ubuntu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FreeBSD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Win32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Win64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Darwin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a list of keywords that will help you determine which web server is
    being used. This is by no means an exhaustive list, but does cover several keywords
    that will yield results if you search the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Apache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Nginx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Microsoft-IIS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tomcat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WEBrick`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lighttpd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IBM HTTP Server`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining which programming language is being used can make a big difference
    in your attack choices. Scripted languages such as PHP are vulnerable to different
    things than a Java server or an ASP.NET application. Here are a few sample keywords
    you can use to search in HTTP headers to identify which language is powering an
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Python`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ruby`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PHP`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASP.NET`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Session cookies are also big giveaways as to what framework or language is
    being used. For example, `PHPSESSID` indicates PHP and `JSESSIONID` indicates
    Java. Here are a few session cookies you can search for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PHPSESSID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JSESSIONID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sessionid`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CFID/CFTOKEN`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASP.NET_SessionId`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fingerprinting web applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fingerprinting web applications in general covers a much broader scope than
    just looking at the HTTP headers. You can do basic keyword searches in the HTTP
    headers, as just discussed, and learn a lot, but there is also a wealth of information
    in the HTML source code and the contents, or simply the existence, of other files
    on the server.
  prefs: []
  type: TYPE_NORMAL
- en: In the HTML source code, you can look for clues such as the structure of pages
    themselves and the names of classes and IDs of HTML elements. AngularJS applications
    have distinct HTML attributes, such as `ng-app`, that can be used as keywords
    for fingerprinting. Angular is also generally included with a `script` tag, the
    same way other frameworks such as jQuery are included. The `script` tags can also
    be inspected for other clues. Look for things such as Google Analytics, AdSense,
    Yahoo ads, Facebook, Disqus, Twitter, and other third-party JavaScript embedded.
  prefs: []
  type: TYPE_NORMAL
- en: Simply looking at the file extensions in URLs can tell you what language is
    being used. For example, `.php`, `.jsp`, and `.asp` indicate that PHP, Java, and
    ASP are being used, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at a program that finds HTML comments in a web page. Some frameworks
    and CMSes leave an identifiable footer or hidden HTML comment. Sometimes the marker
    is in the form of a small image.
  prefs: []
  type: TYPE_NORMAL
- en: Directory structure can also be another giveaway. It requires familiarity with
    different frameworks first. For example, Drupal stores site information in a directory
    called `/sites/default`. If you attempt to visit that URL and you get a 403 FORBIDDEN
    response and not a 404 NOT FOUND error, you likely found a Drupal-based website.
  prefs: []
  type: TYPE_NORMAL
- en: Look for files such as `wp-cron.php`. In the *Finding unlisted files on a web
    server* section, we looked at finding unlisted files with the DirBuster clone.
    Find a list of unique files that you can use to fingerprint web applications and
    add them to your word list. You can figure out which files to look for by inspecting
    the code bases for different web frameworks. For example, the source code for
    WordPress and Drupal are publicly available. Use the program discussed earlier
    in this chapter for finding unlisted files to search for files. Other unlisted
    files that you can search for are related to documentation, such as `CHANGELOG.txt`,
    `readme.txt`, `readme.md`, `readme.html`, `LICENSE.txt`, `install.txt`, or `install.php`.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to get even more detail out of a web application by fingerprinting
    the version of an application that is running. It is much easier if you have access
    to the source code. I will use WordPress as an example since is it so ubiquitous
    and the source is available on GitHub at [https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress).
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to find the differences between versions. WordPress is a good example
    because they all come with the `/wp-admin/` directory that contains all the administrative
    interfaces. Inside `/wp-admin/`, there are the `css` and `js` folders with style
    sheets and scripts in them, respectively. These files are publicly accessible
    when a site is hosted on a server. Use the `diff` command on these folders to
    identify which versions introduce new files, which versions remove files, and
    which versions modify existing files. With all that information combined, you
    can generally narrow down applications to a specific version or to at least a
    small range of versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a contrived example, let''s say version 1.0 contains only one file: `main.js`.
    Version 1.1 introduces a second file: `utility.js`. Version 1.3 removes both of
    those and replaces them with a single file: `master.js`. You can make HTTP requests
    to the web server for all three files: `main.js`, `utility.js`, and `master.js`.
    Based on which files are found with a 200 OK error and which files return a 404
    NOT FOUND error, you can determine which version is running.'
  prefs: []
  type: TYPE_NORMAL
- en: If the same files are present across multiple versions, you can inspect deeper
    into the contents of the files. Either do a byte-by-byte comparison or hash the
    files and compare the checksums. Hashing and examples of hashing are covered in [Chapter
    6](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml), *Cryptography*.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, identifying the version can be much simpler than that whole process
    just described. Sometimes there is a `CHANGELOG.txt` or `readme.html` file that
    will tell you exactly which version is running without having to do any work.
  prefs: []
  type: TYPE_NORMAL
- en: How to prevent fingerprinting of your applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As demonstrated earlier, there are multiple ways to fingerprint applications
    at many different levels of the technology stack. The first question you should
    really ask yourself is, "Do I need to prevent fingerprinting?" In general, trying
    to prevent fingerprinting is a form of obfuscation. Obfuscation is a bit controversial,
    but I think everyone does agree that obfuscation is not security in the same way
    that encoding is not encryption. It may slow down, limit information, or confuse
    an attacker temporarily, but it does not truly prevent any vulnerability from
    being exploited. Now, I'm not saying that there is no benefit at all from obfuscation,
    but it can never be relied on by itself. Obfuscation is simply a thin layer of
    concealment.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, you don't want to give away too much information about your application,
    such as debug output or configuration settings, but some information is going
    to be available no matter what when a service is available on the network. You
    will have to make a choice about how much time and effort you want to put into
    hiding information.
  prefs: []
  type: TYPE_NORMAL
- en: Some people go as far as outputting false information to mislead attackers.
    Personally, putting out fake headers is not on my checklist of things to do when
    hardening a server. One thing I recommend that you do is to remove any extra files
    as mentioned earlier. Files such as changelog files, default setting files, installation
    files, and documentation files should all be removed before deployment. Don't
    publicly serve the files that are not required for the application to work.
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscation is a topic that warrants its own chapter or even its own book. There
    are obfuscation competitions dedicated to awarding the most creative and bizarre
    forms of obfuscation. There are some tools that help you obfuscate JavaScript
    code, but on the flip side, there are also deobfuscation tools.
  prefs: []
  type: TYPE_NORMAL
- en: Using the goquery package for web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `goquery` package is not part of the standard library, but is available
    on GitHub. It is intended to work similar to jQuery—a popular JavaScript framework
    for interacting with the HTML DOM. As demonstrated in the previous sections, trying
    to search with string matching and regular expressions is both tedious and complicated.
    The `goquery` package makes it much easier to work with HTML content and search
    for specific elements. The reason I suggest this package is because it is modelled
    after the very popular jQuery framework that many people are already familiar
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the `goquery` package with the `go get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The documentation is available at [https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery).
  prefs: []
  type: TYPE_NORMAL
- en: Listing all hyperlinks in a page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the introduction to the `goquery` package, we''ll look at a common and
    simple task. We will find all hyperlinks in a page and print them out. A typical
    link looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In HTML, the `a` tag stands for **anchor** and the `href` attribute stands
    for **hyperlink reference**. It is possible to have an anchor tag with no `href`
    attribute but only a `name` attribute. These are called bookmarks, or named anchors,
    and are used to jump to a location on the same page. We will ignore these since
    they only link within the same page. The `target` attribute is just an optional
    one specifying which window or tab to open the link in. We are only interested
    in the `href` value for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Finding documents in a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documents are also points of interest. You might want to scrape a web page and
    look for documents. Word processor documents, spreadsheets, slideshow decks, CSV,
    text, and other files can contain useful information for a variety of purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The following example will search through a URL and search for documents based
    on the file extensions in the links. A global variable is defined at the top for
    convenience with the list of all extensions that should be searched for. Customize
    the list of extensions to search for your target file types. Consider extending
    the application to take a list of file extensions in from a file instead of being
    hardcoded. What other file extensions would you look for when trying to find sensitive
    information?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Listing page title and headings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Headings are the primary structural elements that define the hierarchy of a
    web page, with `<h1>` being the top level and `<h6>` being the lowest or deepest
    level of the hierarchy. The title, defined in the `<title>` tag, of an HTML page
    is what gets displayed in the browser title bar, and it is not part of the rendered
    page.
  prefs: []
  type: TYPE_NORMAL
- en: By listing the title and headings, you can quickly get an idea of what the topic
    of the page is, assuming that they properly formatted their HTML. There is only
    supposed to be one `<title>` and one `<h1>` tag, but not everyone conforms to
    the standards.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program loads a web page and then prints the title and all headings to
    standard output. Try running this program against a few URLs and see whether you
    are able to get a quick idea of the contents just by looking at the headings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Crawling pages on the site that store the most common words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This program prints out a list of all the words used on a web page along with
    the count of how many times each word appeared in the page. This will search all
    paragraph tags. If you search the whole body, it will treat all the HTML code
    as words, which clutters the data and does not really help you understand the
    content of the site. It trims the spaces, commas, periods, tabs, and newlines
    from strings. It also converts all words to lowercase in an attempt to normalize
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each paragraph it finds, it will split the text contents apart. Each word
    is stored in a map that maps the string to an integer count. In the end, the map
    is printed out, listing each word and how many times it was seen on the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Printing a list of external JavaScript files in a page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspecting the URLs of JavaScript files that are included on a page can help
    if you are trying to fingerprint an application or determine what third-party
    libraries are being loaded. This program will list the external JavaScript files
    referenced in a web page. External JavaScript files might be hosted on the same
    domain, or might be loaded from a remote site. It inspects the `src` attribute
    of all the `script` tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if an HTML page had the following tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The URL of the `src` attribute is what would be printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that URLs in the `src` attribute may be fully qualified or relative URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following program loads a URL and then looks for all the `script` tags.
    It will print the `src` attribute for each script it finds. This will only look
    for scripts that are linked externally. To print inline scripts, refer to the
    comment at the bottom of the file regarding `script.Text()`. Try running this
    against some websites you visit frequently and see how many external and third-party
    scripts they embed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This example looks for external scripts referenced by the `src` attribute, but
    some scripts are written directly in the HTML between the opening and closing
    `script` tags. These types of inline script won't have a `src` attribute referencing.
    Get inline script text using the `.Text()` function on the `goquery` object. Refer
    to the bottom of this example, where `script.Text()` is mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this program does not print out the inline scripts and instead focuses
    only on the externally loaded scripts is because that is where a lot of vulnerabilities
    are introduced. Loading remote JavaScript is risky and should be done with trusted
    sources only. Even then, we don't get 100% assurance that the remote content provider
    will never be compromised and serve malicious code. Consider a large corporation
    such as Yahoo! who has acknowledged publicly that their systems have been compromised
    in the past. Yahoo! also has an ad network that hosts a **Content Delivery Network**
    (**CDN**) that serves JavaScript files to a large network of websites. This would
    be a prime target for attackers. Consider these risks when including remote JavaScript
    files in a sensitive customer portal.
  prefs: []
  type: TYPE_NORMAL
- en: Depth-first crawling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depth-first crawling is when you prioritize links on the same domain over links
    that lead to other domains. In this program, external links are completely ignored,
    and only paths on the same domain or relative links are followed.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, unique paths are stored in a slice and printed all together
    at the end. Any errors encountered during the crawl are ignored. Errors are encountered
    often due to malformed links, and we don't want the whole program to exit on errors
    like that.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of trying to parse URLs manually using string functions, the `url.Parse()`
    function is utilized. It does the work of splitting apart the host from the path.
  prefs: []
  type: TYPE_NORMAL
- en: 'When crawling, any query strings and fragments are ignored to reduce duplicates.
    Query strings are designated with the question mark in the URL, and fragments,
    also called bookmarks, are designated with the pound or hash sign. This program
    is single-threaded and does not use goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Breadth-first crawling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breadth-first crawling is when priority is given to finding new domains and
    spreading out as far as possible, as opposed to continuing through a single domain
    in a depth-first manner.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a breadth-first crawler will be left as an exercise for the reader based
    on the information provided in this chapter. It is not very different from the
    depth-first crawler in the previous section, except that it should prioritize
    URLs that point to domains that have not been seen before.
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of notes to keep in mind. If you're not careful and you don't
    set a maximum limit, you could potentially end up crawling petabytes of data!
    You might choose to ignore subdomains, or you can enter a site that has infinite
    subdomains and you will never leave.
  prefs: []
  type: TYPE_NORMAL
- en: How to protect against web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is difficult, if not impossible, to completely prevent web scraping. If you
    serve the information from the web server, there will be a way to extract the
    data programmatically somehow. There are only hurdles you can put in the way.
    It amounts to obfuscation, which you could argue is not worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript makes it more difficult, but not impossible since Selenium can drive
    real web browsers, and frameworks such as PhantomJS can be used to execute the
    JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Requiring authentication can help limit the amount of scraping done. Rate limiting
    can also provide some relief. Rate limiting can be done using tools such as iptables
    or done at the application level, based on the IP address or user session.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the user agent provided by the client is a shallow measure, but can
    help a bit. Discard requests that come with user agents that include keywords
    such as `curl`, `wget`, `go`, `python`, `ruby`, and `perl`. Blocking or ignoring
    these requests can prevent simple bots from scraping your site, but the client
    can fake or omit their user agent so that it is easy to bypass.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take it even further, you can make the HTML ID and class names
    dynamic so that they can't be used to find specific information. Change your HTML
    structure and naming frequently to play the *cat-and-mouse* game to make it more
    work than it is worth for the scraper. This is not a real solution, and I wouldn't
    recommend it, but it is worth mentioning, as it is annoying in the eyes of the
    scraper.
  prefs: []
  type: TYPE_NORMAL
- en: You can use JavaScript to check information about the client, such as screen
    size, before presenting data. If the screen size is 1 x 1 or 0 × 0, or something
    strange, you can assume that it is a bot and refuse to render content.
  prefs: []
  type: TYPE_NORMAL
- en: Honeypot forms are another method of detecting bot behavior. Hide form fields
    with CSS or a `hidden` attribute, and check whether values have been provided
    in those fields. If data is in these fields, assume that a bot is filling out
    all the fields and ignore the request.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use images to store information instead of text. For example,
    if you output only the image of a pie chart, it is much more difficult for someone
    to scrape the data than when you output the data as a JSON object and have JavaScript
    render the pie chart. The scraper can grab the JSON data directly. Text can be
    placed in images as well to prevent text from being scraped and to prevent keyword
    text searches, but **Optical Character Recognition** (**OCR**) can get around
    that with some extra effort.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, some of the preceding techniques can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having read this chapter, you should now understand the fundamentals of web
    scraping, such as performing an HTTP `GET` request and searching for a string
    using string matching or regular expressions to find HTML comments, emails, and
    other keywords. You should also understand how to extract the HTTP headers and
    set custom headers to set cookies and custom user agent strings. Moreover, you
    should understand the basic concepts of fingerprinting and have some idea of how
    to gather information about a web application based on the source code provided.
  prefs: []
  type: TYPE_NORMAL
- en: Having worked through this chapter, you should also understand the basics of
    using the `goquery` package to find HTML elements in the DOM in a jQuery style.
    You should feel comfortable finding links in a web page, finding documents, listing
    title and headers, finding JavaScript files, and finding the difference between
    breadth-first and depth-first crawling.
  prefs: []
  type: TYPE_NORMAL
- en: A note about scraping public websites—be respectful. Don't produce unreasonable
    amounts of traffic to websites by sending huge batches or letting a crawler go
    uninhibited. Set reasonable rate limits and maximum page count limits on programs
    you write as to not overburden remote servers. If you are scraping for data, always
    check to see if an API is available instead. APIs are much more efficient and
    intended to be used programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of any other way to apply the tools examined in this chapter?
    Can you think of any additional features you can add to the examples provided?
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the methods of host discovery and enumeration.
    We will cover things such as TCP sockets, proxies, port scanning, banner grabbing,
    and fuzzing.
  prefs: []
  type: TYPE_NORMAL
