- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Exposure and Sensitive Information Leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter starts the fourth part of our book, which is about advanced API
    techniques. We will better understand the inherent problems of data exposure and
    sensitive information leakage that unpatched or badly configured API endpoints
    can suffer. We will tackle the nuances of how this can happen and ways of taking
    this in our favor as API pentesters.
  prefs: []
  type: TYPE_NORMAL
- en: Either by digesting some data masses or by taking a ride on previous pentesting
    findings, we will learn how data or sensitive information can be detected among
    other garbage or less valuable assets. This can save you time not only when conducting
    a pentest but also when planning to hit the final target of a coordinated attack.
    Some testers establish the scope of their work on exfiltrating some data from
    the endpoint, whereas others work to get it down (by abusing their network, for
    example). You will learn the techniques and understand how such problems can be
    avoided when configuring or building an API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying sensitive data exposure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing for information leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing data leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we did in previous chapters, we’ll leverage the same environment as the one
    pointed out in previous chapters, such as an Ubuntu distro. Some other new relevant
    utilities will be mentioned in the corresponding sections.
  prefs: []
  type: TYPE_NORMAL
- en: We will be especially occupied with handling vast amounts of data in this chapter.
    Hence, we will count on some data mining and curation tools that will do the hard
    work for us when analyzing huge-sized logs or other types of big data.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying sensitive data exposure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Identifying sensitive data exposure in APIs is a critical step in securing
    them. Regardless of their size, data breaches can cause severe and often irreparable
    damage to companies’ reputations. Hence, fully comprehending potential vulnerabilities
    on the API endpoints you own is paramount. The first step is defining what constitutes
    sensitive data. This goes beyond just **Personally Identifiable Information**
    (**PII**) such as names and addresses. Here’s a breakdown of different types of
    sensitive data and how APIs might expose them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PII**: This corresponds to all kinds of data or information that can be used
    to identify a person or individual. This includes government ID numbers (such
    as social security numbers in the USA or Europe, or CPF in Brazil), passport information
    (such as passport numbers, as well as issue and expiry dates), and even health
    data. APIs that return user profiles without proper access control might expose
    PII.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial data**: Credit card details, bank account numbers, and financial
    transaction history are highly sensitive. If an API endpoint needs to process
    any type of payment, even when simply redirecting data to and receiving data from
    a payment system, it must have strict security controls in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication** (**AuthN**) **credentials**: Usernames, passwords, and access
    tokens are fundamental for securing APIs. When such data leaks, access to the
    whole system behind an API endpoint can be compromised.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proprietary information:** Trade secrets, intellectual property documents,
    and internal configurations can all be considered sensitive data. APIs that interact
    with internal systems or databases could potentially leak such information if
    they are not properly secured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not always straightforward to detect when sensitive data is available to
    be extracted from an output. This may require some sophistication on the toolbelt
    we use to parse file dumps such as logs. We will now dive into a mass of logs
    and combine a few tools and patterns to discover which sensitive data or information
    is available. Depending on the log volume you have at hand, you may need to delegate
    this to an external system with more computing power to process it.
  prefs: []
  type: TYPE_NORMAL
- en: As true API endpoints with true sensitive data won’t be used during this exercise,
    we need a way to generate some log files to be analyzed. There’s a good open source
    project written in Golang called `go` command directly as a binary (including
    using `.tar.gz` packages) or run it as a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: 'These lines will not contain any type of sensitive data we are looking for.
    Therefore, let’s boost the utility with some random data that we can further search
    in queries. The code that follows does that. The loop creates log entries and
    stores them in the file pointed to by the `LOG_FILE` variable. Observe that sensitive
    data is only inserted when the iterator variable (`i`) is divisible by 100\. When
    `i` is not divisible by 100, `flog` generates a completely random line. Hence,
    we’ll have 1,000 lines with sensitive data and 9,000 lines with no sensitive data.
    This will make the output file a big mass of data with less interesting content.
    The echo command is in a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We are making use of BASH’s `$RANDOM` internal variable, which generates pseudorandom
    numbers when read. Observe that we need to have `openssl` available on the system
    to generate the random strings that correspond to fake tokens. Simply delete the
    `Auth_Token` part if you don’t want it to be included. The preceding code creates
    a file of around 1 GB in size.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we digest this data mass and only extract the interesting parts?
    There are some ways to do it. Considering that we are using a Linux system, even
    the `grep` command could fulfill this task, accompanied by a few regular expressions
    to facilitate the search. This is not the solution with the best performance though.
    We need something else.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch and more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When handling big masses of data, we require the right tool. OK, 1 GB is not
    that big nowadays, but imagining that you will have access to terabytes of log
    files, how would search them all with `grep` in a feasible timeframe? We will
    exercise one possible solution: the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack. They are three separate products that can be combined to provide one of
    the best-in-class experiences for data analysis and visualization activities.
    Also, they can run as Docker containers.'
  prefs: []
  type: TYPE_NORMAL
- en: One drawback, however, is the substantial requirement for resources (computing,
    memory, and storage). I could not run them on the lab VM (with 8 GB of RAM). Elasticsearch
    alone required more memory than was available. As a matter of fact, on the version
    I tried while writing this chapter (8.13.2), it was specifically complaining about
    the maximum map count check, which is controlled by a Linux kernel parameter.
    Even after increasing it to the number recommended by the documentation (reference
    the *Further reading* section for this), Elasticsearch didn’t work. I’ve also
    done a few tests with another system running on top of macOS, but both the container
    versions and the standalone versions presented different problems that made it
    difficult to set them up.
  prefs: []
  type: TYPE_NORMAL
- en: 'I finally decided to run this stack on Elastic’s cloud platform. They sell
    it as a **Software as a Service** (**SaaS**) with a 14-day trial period. You can
    use all the product’s features and ingest external sources. There’s a sequence
    of steps to set this platform up:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to sign up for the platform or subscribe via AWS, Google, or Microsoft’s
    cloud marketplace. Access [https://cloud.elastic.co/](https://cloud.elastic.co/)
    and click on **Sign up**. You may receive a verification email with a link to
    click. Do it and log in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The wizard will prompt you to answer a few questions about yourself, such as
    your full name, company name, and purpose for using the platform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the wizard will suggest creating a deployment. By clicking on **Edit settings**,
    you can choose the public cloud provider, region, hardware profile, and Elastic
    version. The application automatically selects a combination that’s appropriate
    to your location. Type a name for this deployment and click **Create deployment**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The deployment takes just a couple of minutes to be created and you are then
    redirected to the landing page of the platform. A small note is important here:
    you won’t receive your deployment’s credentials as expected. Because of that,
    you will need to follow an additional step that we’ll explain later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Elastic Cloud Platform’s landing page](img/B19657_figure_08.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Elastic Cloud Platform’s landing page
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to configure an input. You need to tell it how Elasticsearch
    and Kibana will receive data that you’ll further analyze. For that part, we will
    make use of Filebeat, which is both an external utility and a built-in integration.
    You can even stream logs directly to the platform. That’s very useful when you
    want to continuously send data to be analyzed. In our case, it will happen only
    once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are specific installation instructions depending on the operating system
    you are using. Ubuntu, by default, does not have the repository that the application
    can be downloaded from. For your convenience, I put a link in the *Further reading*
    section with the steps you need to follow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You won’t start Filebeat’s service right away. First, you’ll have to configure
    it to send data to Elastic’s cloud. At least on Ubuntu, the `filebeat.yml` configuration
    file is located at `/etc/filebeat`. You must only worry about two sections: **Filebeat
    inputs** and **Elastic Cloud**. Make a backup copy of this file and edit it with
    your preferred editor. Locate the **Filebeat** **inputs** section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You’ll see something like this (the comments were omitted for brevity):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You’ll have to do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace `filestream` with `log`. This is to instruct Filebeat that this is not
    a file being constantly changed, but rather a static one.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace `my-filestream-id` with something more relevant, such as `sensitive-data-log`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace `false` with `true` to effectively activate the input.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace `/var/log/*.log` with the full path of the file you generated on the
    code we used before (the one with the `flog` utility).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the **Elastic Cloud** section. You’ll see something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, you’ll have to get back to the web console and locate both parameters.
    The Cloud ID can be found using this sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the landing page in *Figure 8**.1*, click on the three horizontal lines
    located on the left to open the lateral menu and choose **Manage** **this deployment**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There’s a clipboard button you can click to facilitate copying this data. Click
    to copy and save it in a temporary place.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Where to find the Cloud ID on Elastic’s console](img/B19657_figure_08.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Where to find the Cloud ID on Elastic’s console
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Cloud Auth` parameter demands a few more steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On this screen, click on the **Actions** button and select **Reset password**.
    This will redirect you to the **Security** settings page, where you can make a
    few adjustments:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Resetting the deployment’s password](img/B19657_figure_08.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Resetting the deployment’s password
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Reset password** button. The website will ask you for confirmation.
    Simply click on **Reset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your new password will be defined. You are good to either copy it (using a similar
    clipboard button) or download a CSV file with the credentials. See *Figure 8**.4*
    for reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The new Elastic password is defined, with the option to click
    to copy it or download the CSV file](img/B19657_figure_08.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – The new Elastic password is defined, with the option to click to
    copy it or download the CSV file
  prefs: []
  type: TYPE_NORMAL
- en: Now, go back to the `filebeat.yml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uncomment the `cloud.id` and `cloud.auth` lines. Next, insert a blank space
    right after each of the colons in both lines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the data that you previously copied on the corresponding lines. For the
    `cloud.auth` line, observe that the expected format is `username:password`. The
    username portion is usually `elastic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save and close the file. There are a few commands you can use to verify whether
    the config file is in good shape and whether Filebeat can contact the cloud deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that you may need to run the commands as a superuser. This will depend
    on your operating system’s defaults. Now, you are good to either start the Filebeat
    service or run it interactively. I personally prefer the second way since you
    can watch its log output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this stage, you can return to the console to check what it is receiving.
    Assuming that everything is working, to see the lines of `dummy.log` populated,
    click on the three horizontal lines on the lateral menu again and go to **Observability**
    | **Logs**. If nothing shows up, just click **Refresh**. By default, this view
    shows the last 15 minutes of activity. If you were doing something else while
    the data was already being sent, you may not see anything at all. If that happens,
    simply change the view control to show older data, such as **Last** **1 year**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Changing the view control to display log data](img/B19657_figure_08.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Changing the view control to display log data
  prefs: []
  type: TYPE_NORMAL
- en: The change to the view control takes effect immediately. The following screenshot
    shows the type of viewing you will have when browsing the log data sent by Filebeat.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform](img/B19657_figure_08.6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that the lines start with an IP address. We can use this as an index.
    To be able to search patterns on this data, we can choose one of the options that
    follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply type the data you are looking for into this search bar. For example,
    if you type `Credit_card`, or `Auth_Token`, all lines with these patterns will
    be displayed after you press *Enter*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a data view. Some literature will use the term **Index pattern** to refer
    to this, but it was renamed some time ago to data view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a Kibana feature. To create data views, it will be easier if you type
    `Data View` in the topmost search bar. This will cause a suggestion to show up
    along with the corresponding link. Click on it. You’ll be redirected to a blank
    page with the **Create data view** button. After clicking on it, all sources will
    be displayed. Some of them were created by the deployment and there will be a
    Filebeat one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Creating a data view in Kibana](img/B19657_figure_08.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Creating a data view in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: In the `filebeat-*`. This will cause the right part of the screen to update
    and show only the Filebeat source under the `@timestamp`). Click on **Save data
    view** **to Kibana**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you do this, the previous blank page will be updated with the recently
    created data view. Now, the final step is to discover patterns. Go back to the
    topmost search bar and type `Discover`. You will now be redirected to the `message`
    keyword on KQL. We could build a query like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will end up with the filtered window displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Using KQL to look for sensitive data patterns](img/B19657_figure_08.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Using KQL to look for sensitive data patterns
  prefs: []
  type: TYPE_NORMAL
- en: This is far from being an introductory training to the ELK stack. I put other
    links in the *Further reading* section, where you can look at regular expressions
    on the platform as well as take a free training course on it. That’s cool, but
    what if you don’t want to use a browser or even leverage some cloud offering to
    do your sensitive search? We’ll cover that next.
  prefs: []
  type: TYPE_NORMAL
- en: ripgrep
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re looking for a tool with a smaller footprint than ELK for searching
    through logs for sensitive data, `rg` is a line-oriented search tool that combines
    the usability of The Silver Searcher (link in the *Further reading* section) with
    the raw speed of grep. `rg` works very efficiently by default, ignoring binary
    files, respecting your `.gitignore` files to skip hidden and ignored files, and
    using memory efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '`rg` has at least three advantages when compared to the ELK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: It is extremely fast and performs well even on large files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a sole executable file, easy to install and use without complex configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not require running services or daemons and has minimal memory and CPU
    usage compared to ELK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing it on Ubuntu is as easy as installing any application available
    via `apt` or `apt-get`. There are also versions available for macOS and Windows.
    Let’s see how it behaves with our 1 GB dummy file when looking for credit card
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In around 2.5 seconds, `rg` could find 26 lines with credit card numbers in
    a file of almost 1 GB in size! That happened while it was running on an Ubuntu
    VM with 4 vCPUs and 8 GB of RAM. By the way, Filebeat was still up, and my browser
    instances were also disputing CPU and memory with it. Let’s check how it goes
    with AuthN tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That was even more insane. Since the regular expression was simpler, it could
    find 100 lines with the pattern in less than 0.5 seconds! As is the case with
    the regular `grep`, `rg` is case-sensitive. The same switch (`-i`) can be used
    to turn this off. You can also combine regular expressions to look for multiple
    patterns at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Everything finished in less than 2 seconds. That’s a win! You can optionally
    integrate `rg` into automation scripts and redirect its output to log files. Carefully
    look at its man page to discover more about this fabulous tool. Next, we are going
    to learn how we can make some tests to detect information leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for information leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cool! So, you had access to a data mass, obtained via data exfiltration, social
    engineering, or any other pentesting technique, and you just learned how to extract
    data from such mass with a few rather nice tools. However, how can you possibly
    test an API endpoint to verify whether it is vulnerable to leaking something you’re
    looking for? That’s what we’re going to see here. It is not redundant to say that
    we are not testing real public API endpoints because we obviously do not have
    access for doing so. Consider the teachings here to be for educational and professional
    purposes only.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use our controlled lab environment to put some API routes to run and
    play with them a bit to understand to which extent they can disclose data that
    is supposed to be protected. The first thing you need to have is the data itself,
    of course. You can either pick a file with dummy data you may already have or
    run the script that follows. This will create 1,000 lines of random data, again
    making use of the `$RANDOM` BASH variable. It will contain user IDs, email addresses,
    credit card numbers, and AuthN tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The created file will be a CSV and will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now implement the API with five routes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/users`: An endpoint that exposes sensitive user information without AuthN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/login`: An endpoint that is vulnerable to SQL injection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/profile/<user_id>`: An endpoint with inadequate access control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/get_sensitive_data`: An endpoint that is vulnerable to data leakage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/cause_error`: An endpoint that triggers verbose error messages with stack
    traces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code to implement this application is available at [https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py](https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py).
    It was written in Python since that’s one of the main languages we’ve been using
    in this book and since it’s quite trivial and straightforward to understand. The
    pandas framework is used to facilitate the reading of CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, this code listens on port TCP/5000 by default. Set it to
    run and let’s play with the endpoints. As this application is vulnerable to some
    threats, you don’t necessarily have to authenticate first to be able to talk to
    the endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without having access to the code, you’d obviously have to apply the reconnaissance
    techniques that we covered in the second part of this book. However, since you
    do have access to the code, even in a sloppy analysis of it, you will discover
    how weakly this API was purposefully implemented. Going top to down, we can see
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: There’s an endpoint that sends back the whole data mass without any previous
    AuthN and AuthZ.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The login endpoint is vulnerable to SQL Injection, even in the simplest forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The route that gives information about user profiles does not check whether
    the user is authorized to access such information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The penultimate route tries to do some control by looking for an AuthZ token,
    but it’s so simple that the value could be guessed after a few attempts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there’s even an endpoint that raises an internal exception, creating
    possibilities to disclose data about the internal infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s try them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You just got all the users, organized in a JSON format to facilitate being categorized
    afterward. The login endpoint does not actually interface with a SQL database.
    Hence, we won’t be able to simulate an injection attack here, but the spirit remains.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'What about the route that shows user profiles? It does not require any previous
    AuthZ to check a profile. Let’s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You just got another API endpoint that discloses valid information without
    the correct AuthN or AuthZ. Let’s move on with the exercise and explore the one
    that tries to protect the application with an AuthZ token. In this case, we know
    that the token control is a simple Python condition that checks a trivial token
    content, but in a real-world scenario where some NoSQL or in-memory database would
    be in place, we could try a relevant injection attack to bypass the protection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The final route is just there to push a detailed error message to reinforce
    the danger of not treating exceptions and errors when they happen. To know more
    about this, check [*Chapter 6*](B19657_06.xhtml#_idTextAnchor102), where we have
    deep coverage of the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: These are some ways to interact with APIs and get access to data that should
    not be directly accessible to a regular user. Moreover, unearthing unintentional
    information disclosure in an API involves a combination of passive and active
    probing methods. You can employ tools to craft diverse inquiries to the API and
    carefully examine the replies for potential leaks. This may encompass inspecting
    hidden data embedded within the response (metadata), error messages that might
    be overly revealing, or specific pieces of information that shouldn’t be readily
    available.
  prefs: []
  type: TYPE_NORMAL
- en: In a live environment, tools such as Wireshark (or its command-line equivalent,
    `tshark`) may be useful to detect hidden fields or unprotected payloads that,
    once discovered, will most likely reveal what you are looking for. Burp Suite
    or OWASP ZAP also play a part here, and that’s especially true when the traffic
    to or from the API endpoints is encrypted with TLS. In such cases, if you are
    not able to replace the target’s TLS certificate with your own, which would allow
    you to completely see the packets’ contents, you could struggle more to dig into
    the findings. Next, we are going to understand which techniques we can use to
    reduce the chances of data leakage in the world of APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing data leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To eliminate or at least reduce the chances of suffering data leakage on your
    API or the application behind it, a multi-layered approach is possibly one of
    the best options. This involves secure coding practices, robust AuthN, and careful
    handling of sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: The first line of defense is secure API design – only create the interfaces
    you need. In other words, only expose the data your API requires to function.
    Avoid open queries that could allow unauthorized access. In GraphQL, tools such
    as query whitelisting act as bouncers, restricting data requests and preventing
    the over-fetching of sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source code best practices are a vital topic too. When interacting with databases,
    one important point to keep in mind is to use parameterized queries instead of
    simply forwarding what the user provides as input to them. Think of these as pre-prepared
    invitations to the database – they prevent attackers from manipulating the query
    and potentially stealing data (often referred to as SQL injection attacks). An
    example of Python code implementing such queries is available here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Observe the user of a parameterized placeholder (`?`) for the `user_id` field.
    This prevents the possibility that input data provided by the API endpoint’s user
    affects the final SQL database, reducing the chances of injection attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamic couple of AuthN and AuthZ must never be forgotten. APIs should
    use strong mechanisms such as OAuth 2.0 or OpenID Connect to ensure that only
    authorized users can access sensitive endpoints. **JSON Web Tokens** (**JWTs**)
    are like secure invitations – compact and protected, they allow developers to
    control who gets in. In the code block that follows, you can see an implementation
    of JWT in Python with the use of the Flask JWT Extended module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To be able to access the `/protected` API route, users must present a valid
    JWT token, which is required by the `@``jwt_required()` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Data encryption is like a crown jewel. You must apply TLS as much as possible
    in your communication. As a matter of fact, Red Hat OpenStack, which is a private
    cloud offering, uses a concept called **TLS-e** (the **e** stands for **everywhere**),
    which means that internal and public endpoints of the product have TLS enabled,
    guaranteeing traffic encryption. For data at rest, encryption algorithms such
    as AES (with strong key sizes) act as the vault door, safeguarding stored data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input validation and sanitization offer a subtle yet absolutely inevitable
    shield. Do not simply accept what comes in as valid. When designing or writing
    an API, you should always, always, always think with the mind of a criminal: every
    single line of code or implemented endpoint can be explored in a malicious way.
    Sanitizing user input helps prevent attacks such as SQL injection and **Cross-Site
    Scripting** (**XSS**) that could lead to data leakage if left unchecked. In such
    scenarios, the OWASP **Enterprise Security API** (**ESAPI**) gives a helping hand
    in enforcing security checks.'
  prefs: []
  type: TYPE_NORMAL
- en: For GraphQL APIs, preventing the over-fetching of data is crucial. Techniques
    such as query whitelisting and query cost analysis act as portion control measures,
    ensuring that users only retrieve the data they need. The Apollo GraphQL platform
    offers additional security resources and tools for managing and analyzing queries.
  prefs: []
  type: TYPE_NORMAL
- en: Correct error handling means that you shouldn’t disclose anything that’s not
    strictly necessary to display that an error has happened. Also, catching all possible
    exceptions to avoid an unmapped error can inadvertently disclose internal data
    to the public.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, logging and monitoring close our layered approach. Properly configured
    logging allows security teams to detect and respond to suspicious activity, while
    monitoring tools act as alarms, alerting administrators to potential breaches
    or unauthorized access. However, it’s important to ensure that logs don’t contain
    sensitive information. Rotate and encrypt them as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter started the fourth part of the book, covering API advanced topics.
    We learned how to identify when sensitive data is exposed. We also discussed ways
    to test for information leakage on API endpoints (or routes) and finished the
    chapter with general recommendations on why and how such problems could be prevented.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, it doesn’t matter whether an API uses a modern programming
    language, has just a few endpoints, and only does specific tasks if the data that
    this API services is not well protected. Data leakage is one of the (if not the
    number-one) most feared problems in cyber incidents when they hit companies, regardless
    of their size.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will finish part four by talking about API abuse and
    general logic tests. It’s nothing less than better understanding the business
    logic behind an API implementation and how failures on it may lead to exploitations
    on the API itself. See you there!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flog: [https://github.com/mingrammer/flog](https://github.com/mingrammer/flog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ELK stack: [https://www.elastic.co/elastic-stack](https://www.elastic.co/elastic-stack)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The maximum map count check problem: [https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html](https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Filebeat, an agent to send logs: [https://www.elastic.co/beats/filebeat](https://www.elastic.co/beats/filebeat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing Filebeat on Ubuntu: [https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt](https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KQL: [https://www.elastic.co/guide/en/kibana/current/kuery-query.html](https://www.elastic.co/guide/en/kibana/current/kuery-query.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Lucene, an open source search engine: [https://lucene.apache.org/](https://lucene.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploring regular expressions on Elasticsearch: [https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Free official Elastic training: [https://www.elastic.co/training/free](https://www.elastic.co/training/free)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rg` tool: [https://github.com/BurntSushi/ripgrep](https://github.com/BurntSushi/ripgrep)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Silver Searcher tool: [https://github.com/ggreer/the_silver_searcher](https://github.com/ggreer/the_silver_searcher)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Red Hat OpenStack TLS-e: [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OWASP ESAPI: [https://owasp.org/www-project-enterprise-security-api/](https://owasp.org/www-project-enterprise-security-api/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
